{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to AWorld\u2019s Documentation! Quickstart Install Agent Construction Workflow Construction Multi-agent System Construction Agent Training","title":"Welcome to AWorld\u2019s Documentation!"},{"location":"#welcome-to-aworlds-documentation","text":"","title":"Welcome to AWorld\u2019s Documentation!"},{"location":"#quickstart","text":"Install Agent Construction Workflow Construction Multi-agent System Construction Agent Training","title":"Quickstart"},{"location":"Quickstart/agent_construction/","text":"Building and Running Agents In AWorld's design, both Workflows and Multi-Agent Systems (MAS) are complex systems built around Agents as the core component. Using the most common llm_agent as an example, this tutorial provides detailed guidance on: How to quickly build an Agent How to customize an Agent This document is divided into two parts to explain AWorld's design philosophy. Part 1: Quick Agent Setup Declaring an Agent from aworld.agents.llm_agent import Agent # Assign a name to your agent agent = Agent(name=\"my_agent\") Configuring LLM Method 1: Using Environment Variables import os ## Set up LLM service using environment variables os.environ[\"LLM_PROVIDER\"] = \"openai\" # Choose from: openai, anthropic, azure_openai os.environ[\"LLM_MODEL_NAME\"] = \"gpt-4\" os.environ[\"LLM_API_KEY\"] = \"your-api-key\" os.environ[\"LLM_BASE_URL\"] = \"https://api.openai.com/v1\" # Optional for OpenAI Method 2: Using AgentConfig import os from aworld.agents.llm_agent import Agent from aworld.config.conf import AgentConfig agent_config = AgentConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent = Agent(name=\"my_agent\", conf=agent_config) Method 3: Using Shared ModelConfig When multiple agents use the same LLM service, you can specify a shared ModelConfig: import os from aworld.agents.llm_agent import Agent from aworld.config.conf import AgentConfig, ModelConfig # Create a shared model configuration model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) # Use the shared model config in agent configuration agent_config = AgentConfig( llm_config=model_config, ) agent = Agent(name=\"my_agent\", conf=agent_config) Configuring Prompts from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) # Define your system prompt system_prompt = \"\"\"You are a helpful AI assistant that can assist users with various tasks. You should be polite, accurate, and provide clear explanations.\"\"\" agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt ) Configuring Tools Local Tools from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig from aworld.core.tool.func_to_tool import be_tool model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) system_prompt = \"\"\"You are a helpful agent with access to various tools.\"\"\" # Define a local tool using the @be_tool decorator @be_tool(tool_name='greeting_tool', tool_desc=\"A simple greeting tool that returns a hello message\") def greeting_tool() -> str: return \"Hello, world!\" agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt, tool_names=['greeting_tool'] ) MCP (Model Context Protocol) Tools from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) system_prompt = \"\"\"You are a helpful agent with access to file system operations.\"\"\" # Configure MCP servers mcp_config = { \"mcpServers\": { \"GorillaFileSystem\": { \"type\": \"stdio\", \"command\": \"python\", \"args\": [\"examples/BFCL/mcp_tools/gorilla_file_system.py\"], }, } } agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt, mcp_servers=list(mcp_config.get(\"mcpServers\", {}).keys()), mcp_config=mcp_config ) Agent as Tool from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) system_prompt = \"\"\"You are a helpful agent that can delegate tasks to other specialized agents.\"\"\" # Create a specialized tool agent tool_agent = Agent(name=\"tool_agent\", conf=agent_config) # Create the main agent that can use the tool agent agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt, agent_names=['tool_agent'] ) Part 2: Customizing Agents Customizing Agent Input Override the init_observation() function to customize how your agent processes initial observations: async def init_observation(self, observation: Observation) -> Observation: # You can add extended information from other agents or third-party storage # For example, enrich the observation with additional context observation.metadata = {\"timestamp\": time.time(), \"source\": \"custom\"} return observation Customizing Model Input Override the async_messages_transform() function to customize how messages are transformed before being sent to the model: async def async_messages_transform(self, image_urls: List[str] = None, observation: Observation = None, message: Message = None, **kwargs) -> List[Dict[str, Any]]: \"\"\" Transform input data into the format expected by the LLM. Args: image_urls: List of images encoded using base64 observation: Observation from the environment message: Event received by the Agent \"\"\" messages = [] # Add system context if hasattr(self, 'system_prompt'): messages.append({\"role\": \"system\", \"content\": self.system_prompt}) # Add user message if message and message.content: messages.append({\"role\": \"user\", \"content\": message.content}) # Add images if present if image_urls: for img_url in image_urls: messages.append({ \"role\": \"user\", \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": img_url}}] }) return messages Customizing Model Logic Override the invoke_model() function to implement custom model logic: async def invoke_model(self, messages: List[Dict[str, str]] = [], message: Message = None, **kwargs) -> ModelResponse: \"\"\"Custom model invocation logic. You can use neural networks, rule-based systems, or any other business logic. \"\"\" # Example: Use a custom model or business logic if self.use_custom_logic: # Your custom logic here response_content = self.custom_model.predict(messages) else: # Use the default LLM response_content = await self.llm_client.chat_completion(messages) return ModelResponse( id=f\"response_{int(time.time())}\", model=self.model_name, content=response_content, tool_calls=None # Set if tool calls are present ) Customizing Model Output Create a custom ModelOutputParser class and specify it using the model_output_parser parameter: from aworld.models.model_output_parser import ModelOutputParser class CustomOutputParser(ModelOutputParser[ModelResponse, AgentResult]): async def parse(self, resp: ModelResponse, **kwargs) -> AgentResult: \"\"\"Custom parsing logic based on your model's API response format.\"\"\" # Extract relevant information from the model response content = resp.content tool_calls = resp.tool_calls # Create your custom AgentResult result = AgentResult( content=content, tool_calls=tool_calls, metadata={\"parsed_at\": time.time()} ) return result # Use the custom parser agent = Agent( name=\"my_agent\", conf=agent_config, model_output_parser=CustomOutputParser() ) Customizing Agent Response Override the async_post_run() function to customize how your agent responds: from aworld.core.message import Message class CustomMessage(Message): def __init__(self, content: str, custom_field: str = None): super().__init__(content=content) self.custom_field = custom_field async def async_post_run(self, policy_result: List[ActionModel], policy_input: Observation, message: Message = None) -> Message: \"\"\" Customize the agent's response after processing. \"\"\" # Process the policy result and create a custom response response_content = f\"Processed {len(policy_result)} actions\" custom_field = \"custom_value\" return CustomMessage( content=response_content, custom_field=custom_field ) Custom Response Parsing If the framework doesn't support your response structure, you can create a custom response parser: from aworld.runners import HandlerFactory from aworld.runners.default_handler import DefaultHandler # Define a custom handler name custom_name = \"custom_handler\" @HandlerFactory.register(name=custom_name) class CustomHandler(DefaultHandler): def is_valid_message(self, message: Message): \"\"\"Check if this handler should process the message.\"\"\" return message.category == custom_name async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: \"\"\"Custom message processing logic.\"\"\" if not self.is_valid_message(message): return # Implement your custom message processing logic here processed_message = self.process_custom_message(message) yield processed_message # Use the custom handler agent = Agent( name=\"my_agent\", conf=agent_config, event_handler_name=custom_name ) Important Note: The custom_name variable value must remain consistent across your handler registration and agent configuration.","title":"Agent Construction"},{"location":"Quickstart/agent_construction/#building-and-running-agents","text":"In AWorld's design, both Workflows and Multi-Agent Systems (MAS) are complex systems built around Agents as the core component. Using the most common llm_agent as an example, this tutorial provides detailed guidance on: How to quickly build an Agent How to customize an Agent This document is divided into two parts to explain AWorld's design philosophy.","title":"Building and Running Agents"},{"location":"Quickstart/agent_construction/#part-1-quick-agent-setup","text":"","title":"Part 1: Quick Agent Setup"},{"location":"Quickstart/agent_construction/#declaring-an-agent","text":"from aworld.agents.llm_agent import Agent # Assign a name to your agent agent = Agent(name=\"my_agent\")","title":"Declaring an Agent"},{"location":"Quickstart/agent_construction/#configuring-llm","text":"","title":"Configuring LLM"},{"location":"Quickstart/agent_construction/#method-1-using-environment-variables","text":"import os ## Set up LLM service using environment variables os.environ[\"LLM_PROVIDER\"] = \"openai\" # Choose from: openai, anthropic, azure_openai os.environ[\"LLM_MODEL_NAME\"] = \"gpt-4\" os.environ[\"LLM_API_KEY\"] = \"your-api-key\" os.environ[\"LLM_BASE_URL\"] = \"https://api.openai.com/v1\" # Optional for OpenAI","title":"Method 1: Using Environment Variables"},{"location":"Quickstart/agent_construction/#method-2-using-agentconfig","text":"import os from aworld.agents.llm_agent import Agent from aworld.config.conf import AgentConfig agent_config = AgentConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent = Agent(name=\"my_agent\", conf=agent_config)","title":"Method 2: Using AgentConfig"},{"location":"Quickstart/agent_construction/#method-3-using-shared-modelconfig","text":"When multiple agents use the same LLM service, you can specify a shared ModelConfig: import os from aworld.agents.llm_agent import Agent from aworld.config.conf import AgentConfig, ModelConfig # Create a shared model configuration model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) # Use the shared model config in agent configuration agent_config = AgentConfig( llm_config=model_config, ) agent = Agent(name=\"my_agent\", conf=agent_config)","title":"Method 3: Using Shared ModelConfig"},{"location":"Quickstart/agent_construction/#configuring-prompts","text":"from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) # Define your system prompt system_prompt = \"\"\"You are a helpful AI assistant that can assist users with various tasks. You should be polite, accurate, and provide clear explanations.\"\"\" agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt )","title":"Configuring Prompts"},{"location":"Quickstart/agent_construction/#configuring-tools","text":"","title":"Configuring Tools"},{"location":"Quickstart/agent_construction/#local-tools","text":"from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig from aworld.core.tool.func_to_tool import be_tool model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) system_prompt = \"\"\"You are a helpful agent with access to various tools.\"\"\" # Define a local tool using the @be_tool decorator @be_tool(tool_name='greeting_tool', tool_desc=\"A simple greeting tool that returns a hello message\") def greeting_tool() -> str: return \"Hello, world!\" agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt, tool_names=['greeting_tool'] )","title":"Local Tools"},{"location":"Quickstart/agent_construction/#mcp-model-context-protocol-tools","text":"from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) system_prompt = \"\"\"You are a helpful agent with access to file system operations.\"\"\" # Configure MCP servers mcp_config = { \"mcpServers\": { \"GorillaFileSystem\": { \"type\": \"stdio\", \"command\": \"python\", \"args\": [\"examples/BFCL/mcp_tools/gorilla_file_system.py\"], }, } } agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt, mcp_servers=list(mcp_config.get(\"mcpServers\", {}).keys()), mcp_config=mcp_config )","title":"MCP (Model Context Protocol) Tools"},{"location":"Quickstart/agent_construction/#agent-as-tool","text":"from aworld.agents.llm_agent import Agent import os from aworld.config.conf import AgentConfig, ModelConfig model_config = ModelConfig( llm_provider=os.getenv(\"LLM_PROVIDER\", \"openai\"), llm_model_name=os.getenv(\"LLM_MODEL_NAME\"), llm_base_url=os.getenv(\"LLM_BASE_URL\"), llm_api_key=os.getenv(\"LLM_API_KEY\"), ) agent_config = AgentConfig( llm_config=model_config, ) system_prompt = \"\"\"You are a helpful agent that can delegate tasks to other specialized agents.\"\"\" # Create a specialized tool agent tool_agent = Agent(name=\"tool_agent\", conf=agent_config) # Create the main agent that can use the tool agent agent = Agent( name=\"my_agent\", conf=agent_config, system_prompt=system_prompt, agent_names=['tool_agent'] )","title":"Agent as Tool"},{"location":"Quickstart/agent_construction/#part-2-customizing-agents","text":"","title":"Part 2: Customizing Agents"},{"location":"Quickstart/agent_construction/#customizing-agent-input","text":"Override the init_observation() function to customize how your agent processes initial observations: async def init_observation(self, observation: Observation) -> Observation: # You can add extended information from other agents or third-party storage # For example, enrich the observation with additional context observation.metadata = {\"timestamp\": time.time(), \"source\": \"custom\"} return observation","title":"Customizing Agent Input"},{"location":"Quickstart/agent_construction/#customizing-model-input","text":"Override the async_messages_transform() function to customize how messages are transformed before being sent to the model: async def async_messages_transform(self, image_urls: List[str] = None, observation: Observation = None, message: Message = None, **kwargs) -> List[Dict[str, Any]]: \"\"\" Transform input data into the format expected by the LLM. Args: image_urls: List of images encoded using base64 observation: Observation from the environment message: Event received by the Agent \"\"\" messages = [] # Add system context if hasattr(self, 'system_prompt'): messages.append({\"role\": \"system\", \"content\": self.system_prompt}) # Add user message if message and message.content: messages.append({\"role\": \"user\", \"content\": message.content}) # Add images if present if image_urls: for img_url in image_urls: messages.append({ \"role\": \"user\", \"content\": [{\"type\": \"image_url\", \"image_url\": {\"url\": img_url}}] }) return messages","title":"Customizing Model Input"},{"location":"Quickstart/agent_construction/#customizing-model-logic","text":"Override the invoke_model() function to implement custom model logic: async def invoke_model(self, messages: List[Dict[str, str]] = [], message: Message = None, **kwargs) -> ModelResponse: \"\"\"Custom model invocation logic. You can use neural networks, rule-based systems, or any other business logic. \"\"\" # Example: Use a custom model or business logic if self.use_custom_logic: # Your custom logic here response_content = self.custom_model.predict(messages) else: # Use the default LLM response_content = await self.llm_client.chat_completion(messages) return ModelResponse( id=f\"response_{int(time.time())}\", model=self.model_name, content=response_content, tool_calls=None # Set if tool calls are present )","title":"Customizing Model Logic"},{"location":"Quickstart/agent_construction/#customizing-model-output","text":"Create a custom ModelOutputParser class and specify it using the model_output_parser parameter: from aworld.models.model_output_parser import ModelOutputParser class CustomOutputParser(ModelOutputParser[ModelResponse, AgentResult]): async def parse(self, resp: ModelResponse, **kwargs) -> AgentResult: \"\"\"Custom parsing logic based on your model's API response format.\"\"\" # Extract relevant information from the model response content = resp.content tool_calls = resp.tool_calls # Create your custom AgentResult result = AgentResult( content=content, tool_calls=tool_calls, metadata={\"parsed_at\": time.time()} ) return result # Use the custom parser agent = Agent( name=\"my_agent\", conf=agent_config, model_output_parser=CustomOutputParser() )","title":"Customizing Model Output"},{"location":"Quickstart/agent_construction/#customizing-agent-response","text":"Override the async_post_run() function to customize how your agent responds: from aworld.core.message import Message class CustomMessage(Message): def __init__(self, content: str, custom_field: str = None): super().__init__(content=content) self.custom_field = custom_field async def async_post_run(self, policy_result: List[ActionModel], policy_input: Observation, message: Message = None) -> Message: \"\"\" Customize the agent's response after processing. \"\"\" # Process the policy result and create a custom response response_content = f\"Processed {len(policy_result)} actions\" custom_field = \"custom_value\" return CustomMessage( content=response_content, custom_field=custom_field )","title":"Customizing Agent Response"},{"location":"Quickstart/agent_construction/#custom-response-parsing","text":"If the framework doesn't support your response structure, you can create a custom response parser: from aworld.runners import HandlerFactory from aworld.runners.default_handler import DefaultHandler # Define a custom handler name custom_name = \"custom_handler\" @HandlerFactory.register(name=custom_name) class CustomHandler(DefaultHandler): def is_valid_message(self, message: Message): \"\"\"Check if this handler should process the message.\"\"\" return message.category == custom_name async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: \"\"\"Custom message processing logic.\"\"\" if not self.is_valid_message(message): return # Implement your custom message processing logic here processed_message = self.process_custom_message(message) yield processed_message # Use the custom handler agent = Agent( name=\"my_agent\", conf=agent_config, event_handler_name=custom_name ) Important Note: The custom_name variable value must remain consistent across your handler registration and agent configuration.","title":"Custom Response Parsing"},{"location":"Quickstart/agent_training/","text":"AWorld Train AWorld Training bridges AWorld Agents with external training frameworks (e.g., Reinforcement Learning libraries). It is framework-agnostic, enabling you to bring AWorld Agents or Swarms into your preferred training environment. The pipeline involves four key steps: Environment Setup ( env ): Set up the environment, defining the state/action spaces and interaction dynamics. Agent Construction ( agent ): Build the agent's core logic, policy, and decision-making capabilities. Framework Adaptation ( adapter ): Utilize an adapter to standardize the agent's interface, ensuring compatibility with any RL training frameworks (e.g., Verl). Training Execution ( verl ): Configure the reward function and hyperparameters, then submit the training job via a run script. Installation (Example with Verl) Follow these steps to set up your training environment. Install System-level Prerequisites : Install a compatible NVIDIA Driver . Install the CUDA Toolkit . Manually Install PyTorch : Install a PyTorch version that matches your CUDA version. You can find the command on the PyTorch website . Install Verl and Dependencies : When you install Verl (e.g., via pip install -e . ), other Python packages like transformers , deepspeed , and vllm will be installed automatically. Important : This step requires the prerequisites from steps 1 and 2 to succeed, as some packages need to be compiled against CUDA. See setup.py for a full dependency list. Setting Up the Remote Environment Follow these steps to prepare your remote server and launch the environment. System Requirements Operating System The setup is compatible with Windows, macOS, and Linux. For best performance, a Linux system is highly recommended. Note : Using a server located in regions such as Singapore or North America is also advised to minimize latency. Hardware Minimum : 4 CPU Cores and 8GB of RAM. Software Docker : Docker must be installed on your machine. Important for Mac Users : If you are using a Mac with Apple Silicon (M-series), you must enable Rosetta for x86/64 emulation. Please follow the official instructions at: Docker for Mac Installation . Login and Install the Environment Log into your server and follow these steps. a. Clone the AWorld code to a server directory. git clone https://github.com/inclusionAI/AWorld ~/AWorld b. Configure environment parameters and download the Gaia dataset. Configure parameters : Edit the ~/AWorld/env/gaia-mcp-server/mcp_servers/.env file and enter your specific configuration values. bash cd ~/AWorld/env/gaia-mcp-server/mcp_servers cp .env_template .env Download dataset : Download the gaia_dataset from Hugging Face and place it in ~/AWorld/env/gaia-mcp-server/docker/gaia_dataset . c. Launch the Gaia Environment. Run the command below to start the Gaia Environment instance in Docker. The instance will provide: An MCP service on port 8000 (endpoint: http://localhost:8000/mcp ). A VNC service on port 5901 . You can view the live interface at http://localhost:5901/vnc.html?autoconnect=true . cd ~/AWorld/env # Build the Docker image and start the container instance. This process will take approximately 5 minutes. # Upon success, the following log message will be displayed: Start mcp server success. sh run-local.sh d. Connecting and Testing the Gaia Environment The URL for the Gaia Environment's MCP service is automatically configured as an environment variable, so no manual endpoint setup is required. export MCP_SERVER_URL=http://localhost:8080/mcp When building an Agent, you use the get_agent_tool_env_and_servers function to configure parameters for making MCP requests and to provide the list of MCP Servers. If this function is called without any arguments, it will automatically use default values. gaia_env_config, gaia_env_servers = get_agent_tool_env_and_servers() print(f\"gaia_env_config: {gaia_env_config}\\ngaia_env_servers: {gaia_env_servers}\") # output # gaia_env_config: { # \"mcpServers\": { # \"aworld-mcp\": { # \"type\": \"streamable-http\", # \"url\": \"http://localhost:8080/mcp\", # \"headers\": { # \"MCP_SERVERS\": \"readweb-server,browseruse-server,documents-csv-server,documents-docx-server,documents-pptx-server,documents-pdf-server,documents-txt-server,download-server,intelligence-code-server,intelligence-think-server,intelligence-guard-server,media-audio-server,media-image-server,media-video-server,parxiv-server,terminal-server,wayback-server,wiki-server,googlesearch-server\", # }, # \"timeout\": 600, # \"sse_read_timeout\": 600, # \"client_session_timeout_seconds\": 600, # } # } # } # gaia_env_servers: ['readweb-server', 'browser-server', ...] Building a Custom Agent The AWorld framework is designed for flexibility, allowing you to integrate custom agents with external Reinforcement Learning (RL) frameworks (e.g., Verl). This is primarily handled by the adapter module. The adapter module works by providing the AWorld framework with the LLM's service URL ( llm_base_url ) and model name ( llm_model_name ), treating the LLM as a remote service. llm_base_url=self.get_llm_server_address(), llm_model_name=self.get_llm_server_model_name(), Implementing a Custom AgentLoop To train a custom agent, the primary task is to implement a CustomAgentLoop by inheriting from the AWorldAgentLoop base class. The following example file, custom_agent_loop.py , demonstrates how to create a custom loop for a single agent using the Verl framework and an environment with five available tools. from aworld.agents.llm_agent import Agent from aworld.config import AgentConfig from train.adapter.verl.aworld_agent_loop import AworldAgentLoop from train.adapter.verl.common import get_agent_tool_env_and_servers class GaiaAgentLoop(AworldAgentLoop): def build_agents(self): # Get the environment configuration and server details. # Note: The MCP server must be running (Step 1) and the # MCP_SERVER_URL/MCP_SERVER_TOKEN environment variables must be set. gaia_env_config, gaia_env_servers = get_agent_tool_env_and_servers() return Agent( conf=AgentConfig( # Get the dynamic llm server address from the server manager. # The llm server is launched within VeRL. llm_base_url=self.get_llm_server_address(), llm_model_name=self.get_llm_server_model_name(), llm_api_key=\"dummy\", ), name=\"gaia_super_agent\", system_prompt=\"<your_system_prompt>\", # MCP tool configuration for the agent, including ms-playwright,google-search,e2b-code-server,image-server and audio-server mcp_config=gaia_env_config, mcp_servers=gaia_env_servers, ) Configuration and Launch Once you have implemented your custom AgentLoop , you need to: Modify the agent.yaml configuration file to use your new custom loop. Update the run.sh launch script to point to your modified agent.yaml file. - name: gaia_agent _target_: train.examples.train_gaia_with_aworld_verl.custom_agent_loop.GaiaAgentLoop # Agent config agent_loop_config_path=${path_to_train}/examples/train_gaia_with_aworld_verl/agent.yaml Advanced Scenarios AWorld also supports more complex single-agent or multi-agent systems. Agent Construction : For details on building single-agent or multi-agent systems, please refer to the Building and Running an Agent and Building and Running a Multi-Agent System guides. MCP Tools : If your agent requires MCP tools, you must configure the corresponding mcp_config file. Instructions can be found in the Building and Running an Agent guide. Prepare for Training After the environment ( env ) and agent have been set up, the run.sh script is used to initiate the Verl training process. Prior to execution, two final configuration steps are required: Configure the Reward: Define the reward function according to the specific objectives of the task. Modify the Launch Script: Update the run.sh script to set the correct training parameters, such as configuration paths and hyperparameters. Configuring the Reward Function As an example, here is the reward function used for training the Gaia agent. The full code is located in gaia_reward_function.py . Click to view the gaia_reward_function.py implementation import re import string from aworld.logs.util import logger def normalize_number_str(number_str: str) -> float: # we replace these common units and commas to allow # conversion to float for char in [\"$\", \"%\", \",\"]: number_str = number_str.replace(char, \"\") try: return float(number_str) except ValueError: # print(f\"String {number_str} cannot be normalized to number str.\") return float(\"inf\") def split_string( s: str, char_list: list[str] = [\",\", \";\"], ) -> list[str]: pattern = f\"[{''.join(char_list)}]\" return re.split(pattern, s) def normalize_str(input_str, remove_punct=True) -> str: \"\"\" Normalize a string by: - Removing all white spaces - Optionally removing punctuation (if remove_punct is True) - Converting to lowercase Parameters: - input_str: str, the string to normalize - remove_punct: bool, whether to remove punctuation (default: True) Returns: - str, the normalized string \"\"\" # Remove all white spaces. Required e.g for seagull vs. sea gull no_spaces = re.sub(r\"\\s\", \"\", input_str) # Remove punctuation, if specified. if remove_punct: translator = str.maketrans(\"\", \"\", string.punctuation) return no_spaces.lower().translate(translator) else: return no_spaces.lower() def question_scorer( model_answer: str, ground_truth: str, ) -> bool: def is_float(element: any) -> bool: try: float(element) return True except ValueError: return False if model_answer is None: model_answer = \"None\" # if gt is a number if is_float(ground_truth): # print(f\"Evaluating {model_answer} as a number.\") normalized_answer = normalize_number_str(model_answer) return normalized_answer == float(ground_truth) # if gt is a list elif any(char in ground_truth for char in [\",\", \";\"]): # print(f\"Evaluating {model_answer} as a comma separated list.\") # question with the fish: normalization removes punct gt_elems = split_string(ground_truth) ma_elems = split_string(model_answer) # check length is the same if len(gt_elems) != len(ma_elems): # warnings.warn( # \"Answer lists have different lengths, returning False.\", UserWarning # ) return False # compare each element as float or str comparisons = [] for ma_elem, gt_elem in zip(ma_elems, gt_elems): if is_float(gt_elem): normalized_ma_elem = normalize_number_str(ma_elem) comparisons.append(normalized_ma_elem == float(gt_elem)) else: # we do not remove punct since comparisons can include punct comparisons.append( normalize_str(ma_elem, remove_punct=False) == normalize_str(gt_elem, remove_punct=False) ) return all(comparisons) # if gt is a str else: # print(f\"Evaluating {model_answer} as a string.\") return normalize_str(model_answer) == normalize_str(ground_truth) def gaia_reward_func(data_source, solution_str, ground_truth, extra_info=None): pattern = r'<answer>(.*?)</answer>' comp_match = re.search(pattern, solution_str, re.DOTALL | re.MULTILINE) if not comp_match: return 0.0 else: comp_answer = comp_match.group(1).strip() logger.info(f\"comp_answer: {comp_answer}, ground_truth: {ground_truth}\") if question_scorer(comp_answer, ground_truth): return 1.0 else: return 0.0 After implementing your custom reward function, you must update the run.sh script to point to it: reward_fn_name=gaia_reward_func reward_fn_file_path=${path_to_train}/examples/train_gaia_with_aworld_verl/metrics/gaia_reward_function.py Modifying the Launch Script Below is an example of the run.sh script for training a GaiaAgent in the AWorld environment. In this script, pay close attention to the following key configurations, which are crucial for connecting AWorld to the training framework: agent_loop_config_path (Section 3): Specifies the configuration file for your custom AgentLoop. reward_fn_file_path (Section 4.1): Defines the file path where the reward function is located. reward_fn_name (Section 4.1): Specifies the name of the reward function to use. For a detailed explanation of all parameters, please refer to the official VeRL documentation . Click to view the full run.sh script #!/usr/bin/env bash set -xeuo pipefail # ================= cluster topology ================= export GPUS_PER_NODE=${SLURM_GPUS_ON_NODE:-${GPUS_PER_NODE:-1}} # GPUs on this node NNODES=${SLURM_JOB_NUM_NODES:-${NNODES:-1}} export NNODES export RAY_NUM_NODES=$NNODES echo \"Using $NNODES nodes and $GPUS_PER_NODE GPUs per node...\" # ================= data/model/tool ================= HDFS_ROOT=${HDFS_ROOT:-$PWD} DATA_ROOT=${DATA_ROOT:-$PWD} # Prefer local model if present, otherwise fall back to HF hub path model_path=${model_path:-$DATA_ROOT/Qwen/Qwen3-4B} if [ ! -d \"$model_path\" ]; then model_path=Qwen/Qwen3-4B fi # Use the default output directory produced by create_dataset.py train_files=$DATA_ROOT/datasets/train.parquet test_files=$DATA_ROOT/datasets/test.parquet # =================== custom =================== path_to_train=\"/your/path/to/train\" reward_fn_name=gaia_reward_func reward_fn_file_path=${path_to_train}/examples/train_gaia_with_aworld_verl/metrics/gaia_reward_function.py # Agent config agent_loop_config_path=${path_to_train}/examples/train_gaia_with_aworld_verl/agent.yaml # set dummy_tool_config_path to enable auto_tool_choice dummy_tool_config_path=${path_to_train}/examples/verl/configs/dummy_tool_config.yaml # =================== wandb =================== project_name=gaia experiment_name=qwe3 default_local_dir=$DATA_ROOT/checkpoint/$experiment_name # ================= algorithm ================= adv_estimator=grpo use_kl_in_reward=false kl_coef=0.0 use_kl_loss=false kl_loss_coef=0.0 clip_ratio_low=0.2 clip_ratio_high=0.28 max_turns=8 max_prompt_length=1024 max_response_length=2048 actor_lr=1e-6 train_batch_size=1 ppo_mini_batch_size=1 n_resp_per_prompt=1 n_resp_per_prompt_val=1 # =================== logging =================== export RAY_LOGGING_LEVEL=DEBUG export HYDRA_FULL_ERROR=1 # ================= performance ================= export NCCL_IBEXT_DISABLE=1 export NCCL_NVLS_ENABLE=1 export NCCL_IB_HCA=mlx5 export UCX_NET_DEVICES=mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1 export VLLM_USE_V1=1 export VLLM_ATTENTION_BACKEND=FLASH_ATTN infer_tp=1 # vLLM tensor parallel size train_sp=1 # Ulysses sequence parallel size for actor offload=true actor_max_token_len_per_gpu=$(( (max_prompt_length + max_response_length) * 4 )) log_prob_max_token_len_per_gpu=$(( actor_max_token_len_per_gpu * 2 )) train_files=\"['$train_files']\" test_files=\"['$test_files']\" python3 -m verl.trainer.main_ppo \\ algorithm.adv_estimator=$adv_estimator \\ algorithm.use_kl_in_reward=$use_kl_in_reward \\ algorithm.kl_ctrl.kl_coef=$kl_coef \\ data.train_files=\"$train_files\" \\ data.val_files=\"$test_files\" \\ data.return_raw_chat=true \\ data.train_batch_size=$train_batch_size \\ data.max_prompt_length=$max_prompt_length \\ data.max_response_length=$max_response_length \\ data.filter_overlong_prompts=true \\ data.truncation='error' \\ actor_rollout_ref.model.path=\"$model_path\" \\ actor_rollout_ref.model.use_remove_padding=true \\ actor_rollout_ref.model.enable_gradient_checkpointing=true \\ actor_rollout_ref.actor.use_kl_loss=$use_kl_loss \\ actor_rollout_ref.actor.kl_loss_coef=$kl_loss_coef \\ actor_rollout_ref.actor.clip_ratio_low=$clip_ratio_low \\ actor_rollout_ref.actor.clip_ratio_high=$clip_ratio_high \\ actor_rollout_ref.actor.clip_ratio_c=10.0 \\ actor_rollout_ref.actor.optim.lr=$actor_lr \\ actor_rollout_ref.actor.use_dynamic_bsz=true \\ actor_rollout_ref.actor.ppo_mini_batch_size=$ppo_mini_batch_size \\ actor_rollout_ref.actor.ppo_max_token_len_per_gpu=$actor_max_token_len_per_gpu \\ actor_rollout_ref.actor.ulysses_sequence_parallel_size=$train_sp \\ actor_rollout_ref.actor.fsdp_config.param_offload=$offload \\ actor_rollout_ref.actor.fsdp_config.optimizer_offload=$offload \\ actor_rollout_ref.ref.log_prob_max_token_len_per_gpu=$log_prob_max_token_len_per_gpu \\ actor_rollout_ref.rollout.name=vllm \\ actor_rollout_ref.rollout.mode=async \\ actor_rollout_ref.rollout.tensor_model_parallel_size=$infer_tp \\ actor_rollout_ref.rollout.multi_turn.max_user_turns=$max_turns \\ actor_rollout_ref.rollout.multi_turn.max_assistant_turns=$max_turns \\ actor_rollout_ref.rollout.multi_turn.format=hermes \\ actor_rollout_ref.rollout.agent.agent_loop_config_path=$agent_loop_config_path \\ actor_rollout_ref.rollout.gpu_memory_utilization=0.75 \\ actor_rollout_ref.rollout.n=$n_resp_per_prompt \\ actor_rollout_ref.rollout.val_kwargs.top_p=0.6 \\ actor_rollout_ref.rollout.val_kwargs.temperature=1.0 \\ actor_rollout_ref.rollout.val_kwargs.n=$n_resp_per_prompt_val \\ actor_rollout_ref.rollout.multi_turn.tool_config_path=$dummy_tool_config_path \\ custom_reward_function.path=\"${reward_fn_file_path}\"\\ custom_reward_function.name=\"${reward_fn_name}\"\\ trainer.logger=console \\ trainer.project_name=$project_name \\ trainer.experiment_name=$experiment_name \\ trainer.n_gpus_per_node=\"$GPUS_PER_NODE\" \\ trainer.val_before_train=true \\ trainer.log_val_generations=50 \\ trainer.nnodes=\"$NNODES\" \\ trainer.save_freq=-1 \\ trainer.default_local_dir=\"$default_local_dir\" \\ trainer.test_freq=5 \\ trainer.total_epochs=1 \"$@\" Launching the Training After all configurations are complete, you can start the training by running: bash run.sh","title":"Agent Training"},{"location":"Quickstart/agent_training/#aworld-train","text":"AWorld Training bridges AWorld Agents with external training frameworks (e.g., Reinforcement Learning libraries). It is framework-agnostic, enabling you to bring AWorld Agents or Swarms into your preferred training environment. The pipeline involves four key steps: Environment Setup ( env ): Set up the environment, defining the state/action spaces and interaction dynamics. Agent Construction ( agent ): Build the agent's core logic, policy, and decision-making capabilities. Framework Adaptation ( adapter ): Utilize an adapter to standardize the agent's interface, ensuring compatibility with any RL training frameworks (e.g., Verl). Training Execution ( verl ): Configure the reward function and hyperparameters, then submit the training job via a run script.","title":"AWorld Train"},{"location":"Quickstart/agent_training/#installation-example-with-verl","text":"Follow these steps to set up your training environment. Install System-level Prerequisites : Install a compatible NVIDIA Driver . Install the CUDA Toolkit . Manually Install PyTorch : Install a PyTorch version that matches your CUDA version. You can find the command on the PyTorch website . Install Verl and Dependencies : When you install Verl (e.g., via pip install -e . ), other Python packages like transformers , deepspeed , and vllm will be installed automatically. Important : This step requires the prerequisites from steps 1 and 2 to succeed, as some packages need to be compiled against CUDA. See setup.py for a full dependency list.","title":"Installation (Example with Verl)"},{"location":"Quickstart/agent_training/#setting-up-the-remote-environment","text":"Follow these steps to prepare your remote server and launch the environment.","title":"Setting Up the Remote Environment"},{"location":"Quickstart/agent_training/#system-requirements","text":"","title":"System Requirements"},{"location":"Quickstart/agent_training/#operating-system","text":"The setup is compatible with Windows, macOS, and Linux. For best performance, a Linux system is highly recommended. Note : Using a server located in regions such as Singapore or North America is also advised to minimize latency.","title":"Operating System"},{"location":"Quickstart/agent_training/#hardware","text":"Minimum : 4 CPU Cores and 8GB of RAM.","title":"Hardware"},{"location":"Quickstart/agent_training/#software","text":"Docker : Docker must be installed on your machine. Important for Mac Users : If you are using a Mac with Apple Silicon (M-series), you must enable Rosetta for x86/64 emulation. Please follow the official instructions at: Docker for Mac Installation .","title":"Software"},{"location":"Quickstart/agent_training/#login-and-install-the-environment","text":"Log into your server and follow these steps. a. Clone the AWorld code to a server directory. git clone https://github.com/inclusionAI/AWorld ~/AWorld b. Configure environment parameters and download the Gaia dataset. Configure parameters : Edit the ~/AWorld/env/gaia-mcp-server/mcp_servers/.env file and enter your specific configuration values. bash cd ~/AWorld/env/gaia-mcp-server/mcp_servers cp .env_template .env Download dataset : Download the gaia_dataset from Hugging Face and place it in ~/AWorld/env/gaia-mcp-server/docker/gaia_dataset . c. Launch the Gaia Environment. Run the command below to start the Gaia Environment instance in Docker. The instance will provide: An MCP service on port 8000 (endpoint: http://localhost:8000/mcp ). A VNC service on port 5901 . You can view the live interface at http://localhost:5901/vnc.html?autoconnect=true . cd ~/AWorld/env # Build the Docker image and start the container instance. This process will take approximately 5 minutes. # Upon success, the following log message will be displayed: Start mcp server success. sh run-local.sh d. Connecting and Testing the Gaia Environment The URL for the Gaia Environment's MCP service is automatically configured as an environment variable, so no manual endpoint setup is required. export MCP_SERVER_URL=http://localhost:8080/mcp When building an Agent, you use the get_agent_tool_env_and_servers function to configure parameters for making MCP requests and to provide the list of MCP Servers. If this function is called without any arguments, it will automatically use default values. gaia_env_config, gaia_env_servers = get_agent_tool_env_and_servers() print(f\"gaia_env_config: {gaia_env_config}\\ngaia_env_servers: {gaia_env_servers}\") # output # gaia_env_config: { # \"mcpServers\": { # \"aworld-mcp\": { # \"type\": \"streamable-http\", # \"url\": \"http://localhost:8080/mcp\", # \"headers\": { # \"MCP_SERVERS\": \"readweb-server,browseruse-server,documents-csv-server,documents-docx-server,documents-pptx-server,documents-pdf-server,documents-txt-server,download-server,intelligence-code-server,intelligence-think-server,intelligence-guard-server,media-audio-server,media-image-server,media-video-server,parxiv-server,terminal-server,wayback-server,wiki-server,googlesearch-server\", # }, # \"timeout\": 600, # \"sse_read_timeout\": 600, # \"client_session_timeout_seconds\": 600, # } # } # } # gaia_env_servers: ['readweb-server', 'browser-server', ...]","title":"Login and Install the Environment"},{"location":"Quickstart/agent_training/#building-a-custom-agent","text":"The AWorld framework is designed for flexibility, allowing you to integrate custom agents with external Reinforcement Learning (RL) frameworks (e.g., Verl). This is primarily handled by the adapter module. The adapter module works by providing the AWorld framework with the LLM's service URL ( llm_base_url ) and model name ( llm_model_name ), treating the LLM as a remote service. llm_base_url=self.get_llm_server_address(), llm_model_name=self.get_llm_server_model_name(),","title":"Building a Custom Agent"},{"location":"Quickstart/agent_training/#implementing-a-custom-agentloop","text":"To train a custom agent, the primary task is to implement a CustomAgentLoop by inheriting from the AWorldAgentLoop base class. The following example file, custom_agent_loop.py , demonstrates how to create a custom loop for a single agent using the Verl framework and an environment with five available tools. from aworld.agents.llm_agent import Agent from aworld.config import AgentConfig from train.adapter.verl.aworld_agent_loop import AworldAgentLoop from train.adapter.verl.common import get_agent_tool_env_and_servers class GaiaAgentLoop(AworldAgentLoop): def build_agents(self): # Get the environment configuration and server details. # Note: The MCP server must be running (Step 1) and the # MCP_SERVER_URL/MCP_SERVER_TOKEN environment variables must be set. gaia_env_config, gaia_env_servers = get_agent_tool_env_and_servers() return Agent( conf=AgentConfig( # Get the dynamic llm server address from the server manager. # The llm server is launched within VeRL. llm_base_url=self.get_llm_server_address(), llm_model_name=self.get_llm_server_model_name(), llm_api_key=\"dummy\", ), name=\"gaia_super_agent\", system_prompt=\"<your_system_prompt>\", # MCP tool configuration for the agent, including ms-playwright,google-search,e2b-code-server,image-server and audio-server mcp_config=gaia_env_config, mcp_servers=gaia_env_servers, )","title":"Implementing a Custom AgentLoop"},{"location":"Quickstart/agent_training/#configuration-and-launch","text":"Once you have implemented your custom AgentLoop , you need to: Modify the agent.yaml configuration file to use your new custom loop. Update the run.sh launch script to point to your modified agent.yaml file. - name: gaia_agent _target_: train.examples.train_gaia_with_aworld_verl.custom_agent_loop.GaiaAgentLoop # Agent config agent_loop_config_path=${path_to_train}/examples/train_gaia_with_aworld_verl/agent.yaml","title":"Configuration and Launch"},{"location":"Quickstart/agent_training/#advanced-scenarios","text":"AWorld also supports more complex single-agent or multi-agent systems. Agent Construction : For details on building single-agent or multi-agent systems, please refer to the Building and Running an Agent and Building and Running a Multi-Agent System guides. MCP Tools : If your agent requires MCP tools, you must configure the corresponding mcp_config file. Instructions can be found in the Building and Running an Agent guide.","title":"Advanced Scenarios"},{"location":"Quickstart/agent_training/#prepare-for-training","text":"After the environment ( env ) and agent have been set up, the run.sh script is used to initiate the Verl training process. Prior to execution, two final configuration steps are required: Configure the Reward: Define the reward function according to the specific objectives of the task. Modify the Launch Script: Update the run.sh script to set the correct training parameters, such as configuration paths and hyperparameters.","title":"Prepare for Training"},{"location":"Quickstart/agent_training/#configuring-the-reward-function","text":"As an example, here is the reward function used for training the Gaia agent. The full code is located in gaia_reward_function.py . Click to view the gaia_reward_function.py implementation import re import string from aworld.logs.util import logger def normalize_number_str(number_str: str) -> float: # we replace these common units and commas to allow # conversion to float for char in [\"$\", \"%\", \",\"]: number_str = number_str.replace(char, \"\") try: return float(number_str) except ValueError: # print(f\"String {number_str} cannot be normalized to number str.\") return float(\"inf\") def split_string( s: str, char_list: list[str] = [\",\", \";\"], ) -> list[str]: pattern = f\"[{''.join(char_list)}]\" return re.split(pattern, s) def normalize_str(input_str, remove_punct=True) -> str: \"\"\" Normalize a string by: - Removing all white spaces - Optionally removing punctuation (if remove_punct is True) - Converting to lowercase Parameters: - input_str: str, the string to normalize - remove_punct: bool, whether to remove punctuation (default: True) Returns: - str, the normalized string \"\"\" # Remove all white spaces. Required e.g for seagull vs. sea gull no_spaces = re.sub(r\"\\s\", \"\", input_str) # Remove punctuation, if specified. if remove_punct: translator = str.maketrans(\"\", \"\", string.punctuation) return no_spaces.lower().translate(translator) else: return no_spaces.lower() def question_scorer( model_answer: str, ground_truth: str, ) -> bool: def is_float(element: any) -> bool: try: float(element) return True except ValueError: return False if model_answer is None: model_answer = \"None\" # if gt is a number if is_float(ground_truth): # print(f\"Evaluating {model_answer} as a number.\") normalized_answer = normalize_number_str(model_answer) return normalized_answer == float(ground_truth) # if gt is a list elif any(char in ground_truth for char in [\",\", \";\"]): # print(f\"Evaluating {model_answer} as a comma separated list.\") # question with the fish: normalization removes punct gt_elems = split_string(ground_truth) ma_elems = split_string(model_answer) # check length is the same if len(gt_elems) != len(ma_elems): # warnings.warn( # \"Answer lists have different lengths, returning False.\", UserWarning # ) return False # compare each element as float or str comparisons = [] for ma_elem, gt_elem in zip(ma_elems, gt_elems): if is_float(gt_elem): normalized_ma_elem = normalize_number_str(ma_elem) comparisons.append(normalized_ma_elem == float(gt_elem)) else: # we do not remove punct since comparisons can include punct comparisons.append( normalize_str(ma_elem, remove_punct=False) == normalize_str(gt_elem, remove_punct=False) ) return all(comparisons) # if gt is a str else: # print(f\"Evaluating {model_answer} as a string.\") return normalize_str(model_answer) == normalize_str(ground_truth) def gaia_reward_func(data_source, solution_str, ground_truth, extra_info=None): pattern = r'<answer>(.*?)</answer>' comp_match = re.search(pattern, solution_str, re.DOTALL | re.MULTILINE) if not comp_match: return 0.0 else: comp_answer = comp_match.group(1).strip() logger.info(f\"comp_answer: {comp_answer}, ground_truth: {ground_truth}\") if question_scorer(comp_answer, ground_truth): return 1.0 else: return 0.0 After implementing your custom reward function, you must update the run.sh script to point to it: reward_fn_name=gaia_reward_func reward_fn_file_path=${path_to_train}/examples/train_gaia_with_aworld_verl/metrics/gaia_reward_function.py","title":"Configuring the Reward Function"},{"location":"Quickstart/agent_training/#modifying-the-launch-script","text":"Below is an example of the run.sh script for training a GaiaAgent in the AWorld environment. In this script, pay close attention to the following key configurations, which are crucial for connecting AWorld to the training framework: agent_loop_config_path (Section 3): Specifies the configuration file for your custom AgentLoop. reward_fn_file_path (Section 4.1): Defines the file path where the reward function is located. reward_fn_name (Section 4.1): Specifies the name of the reward function to use. For a detailed explanation of all parameters, please refer to the official VeRL documentation . Click to view the full run.sh script #!/usr/bin/env bash set -xeuo pipefail # ================= cluster topology ================= export GPUS_PER_NODE=${SLURM_GPUS_ON_NODE:-${GPUS_PER_NODE:-1}} # GPUs on this node NNODES=${SLURM_JOB_NUM_NODES:-${NNODES:-1}} export NNODES export RAY_NUM_NODES=$NNODES echo \"Using $NNODES nodes and $GPUS_PER_NODE GPUs per node...\" # ================= data/model/tool ================= HDFS_ROOT=${HDFS_ROOT:-$PWD} DATA_ROOT=${DATA_ROOT:-$PWD} # Prefer local model if present, otherwise fall back to HF hub path model_path=${model_path:-$DATA_ROOT/Qwen/Qwen3-4B} if [ ! -d \"$model_path\" ]; then model_path=Qwen/Qwen3-4B fi # Use the default output directory produced by create_dataset.py train_files=$DATA_ROOT/datasets/train.parquet test_files=$DATA_ROOT/datasets/test.parquet # =================== custom =================== path_to_train=\"/your/path/to/train\" reward_fn_name=gaia_reward_func reward_fn_file_path=${path_to_train}/examples/train_gaia_with_aworld_verl/metrics/gaia_reward_function.py # Agent config agent_loop_config_path=${path_to_train}/examples/train_gaia_with_aworld_verl/agent.yaml # set dummy_tool_config_path to enable auto_tool_choice dummy_tool_config_path=${path_to_train}/examples/verl/configs/dummy_tool_config.yaml # =================== wandb =================== project_name=gaia experiment_name=qwe3 default_local_dir=$DATA_ROOT/checkpoint/$experiment_name # ================= algorithm ================= adv_estimator=grpo use_kl_in_reward=false kl_coef=0.0 use_kl_loss=false kl_loss_coef=0.0 clip_ratio_low=0.2 clip_ratio_high=0.28 max_turns=8 max_prompt_length=1024 max_response_length=2048 actor_lr=1e-6 train_batch_size=1 ppo_mini_batch_size=1 n_resp_per_prompt=1 n_resp_per_prompt_val=1 # =================== logging =================== export RAY_LOGGING_LEVEL=DEBUG export HYDRA_FULL_ERROR=1 # ================= performance ================= export NCCL_IBEXT_DISABLE=1 export NCCL_NVLS_ENABLE=1 export NCCL_IB_HCA=mlx5 export UCX_NET_DEVICES=mlx5_0:1,mlx5_1:1,mlx5_2:1,mlx5_3:1,mlx5_4:1,mlx5_5:1,mlx5_6:1,mlx5_7:1 export VLLM_USE_V1=1 export VLLM_ATTENTION_BACKEND=FLASH_ATTN infer_tp=1 # vLLM tensor parallel size train_sp=1 # Ulysses sequence parallel size for actor offload=true actor_max_token_len_per_gpu=$(( (max_prompt_length + max_response_length) * 4 )) log_prob_max_token_len_per_gpu=$(( actor_max_token_len_per_gpu * 2 )) train_files=\"['$train_files']\" test_files=\"['$test_files']\" python3 -m verl.trainer.main_ppo \\ algorithm.adv_estimator=$adv_estimator \\ algorithm.use_kl_in_reward=$use_kl_in_reward \\ algorithm.kl_ctrl.kl_coef=$kl_coef \\ data.train_files=\"$train_files\" \\ data.val_files=\"$test_files\" \\ data.return_raw_chat=true \\ data.train_batch_size=$train_batch_size \\ data.max_prompt_length=$max_prompt_length \\ data.max_response_length=$max_response_length \\ data.filter_overlong_prompts=true \\ data.truncation='error' \\ actor_rollout_ref.model.path=\"$model_path\" \\ actor_rollout_ref.model.use_remove_padding=true \\ actor_rollout_ref.model.enable_gradient_checkpointing=true \\ actor_rollout_ref.actor.use_kl_loss=$use_kl_loss \\ actor_rollout_ref.actor.kl_loss_coef=$kl_loss_coef \\ actor_rollout_ref.actor.clip_ratio_low=$clip_ratio_low \\ actor_rollout_ref.actor.clip_ratio_high=$clip_ratio_high \\ actor_rollout_ref.actor.clip_ratio_c=10.0 \\ actor_rollout_ref.actor.optim.lr=$actor_lr \\ actor_rollout_ref.actor.use_dynamic_bsz=true \\ actor_rollout_ref.actor.ppo_mini_batch_size=$ppo_mini_batch_size \\ actor_rollout_ref.actor.ppo_max_token_len_per_gpu=$actor_max_token_len_per_gpu \\ actor_rollout_ref.actor.ulysses_sequence_parallel_size=$train_sp \\ actor_rollout_ref.actor.fsdp_config.param_offload=$offload \\ actor_rollout_ref.actor.fsdp_config.optimizer_offload=$offload \\ actor_rollout_ref.ref.log_prob_max_token_len_per_gpu=$log_prob_max_token_len_per_gpu \\ actor_rollout_ref.rollout.name=vllm \\ actor_rollout_ref.rollout.mode=async \\ actor_rollout_ref.rollout.tensor_model_parallel_size=$infer_tp \\ actor_rollout_ref.rollout.multi_turn.max_user_turns=$max_turns \\ actor_rollout_ref.rollout.multi_turn.max_assistant_turns=$max_turns \\ actor_rollout_ref.rollout.multi_turn.format=hermes \\ actor_rollout_ref.rollout.agent.agent_loop_config_path=$agent_loop_config_path \\ actor_rollout_ref.rollout.gpu_memory_utilization=0.75 \\ actor_rollout_ref.rollout.n=$n_resp_per_prompt \\ actor_rollout_ref.rollout.val_kwargs.top_p=0.6 \\ actor_rollout_ref.rollout.val_kwargs.temperature=1.0 \\ actor_rollout_ref.rollout.val_kwargs.n=$n_resp_per_prompt_val \\ actor_rollout_ref.rollout.multi_turn.tool_config_path=$dummy_tool_config_path \\ custom_reward_function.path=\"${reward_fn_file_path}\"\\ custom_reward_function.name=\"${reward_fn_name}\"\\ trainer.logger=console \\ trainer.project_name=$project_name \\ trainer.experiment_name=$experiment_name \\ trainer.n_gpus_per_node=\"$GPUS_PER_NODE\" \\ trainer.val_before_train=true \\ trainer.log_val_generations=50 \\ trainer.nnodes=\"$NNODES\" \\ trainer.save_freq=-1 \\ trainer.default_local_dir=\"$default_local_dir\" \\ trainer.test_freq=5 \\ trainer.total_epochs=1 \"$@\"","title":"Modifying the Launch Script"},{"location":"Quickstart/agent_training/#launching-the-training","text":"After all configurations are complete, you can start the training by running: bash run.sh","title":"Launching the Training"},{"location":"Quickstart/install/","text":"Installation AWorld Agent Prerequisites Python 3.11+ Install git clone https://github.com/inclusionAI/AWorld && cd AWorld pip install . AWorld Env TODO AWorld Train TODO","title":"Install"},{"location":"Quickstart/install/#installation","text":"","title":"Installation"},{"location":"Quickstart/install/#aworld-agent","text":"","title":"AWorld Agent"},{"location":"Quickstart/install/#prerequisites","text":"Python 3.11+","title":"Prerequisites"},{"location":"Quickstart/install/#install","text":"git clone https://github.com/inclusionAI/AWorld && cd AWorld pip install .","title":"Install"},{"location":"Quickstart/install/#aworld-env","text":"TODO","title":"AWorld Env"},{"location":"Quickstart/install/#aworld-train","text":"TODO","title":"AWorld Train"},{"location":"Quickstart/multi-agent_system_construction/","text":"Building and Running Multi-Agent Systems (MAS) In the AWorld framework, similar to Workflow Construction, the fundamental building block for MAS is the Agent. By introducing the Swarm concept, users can easily, quickly, and efficiently build complex Multi-Agent Systems. In summary: Workflow in AWorld : Static, pre-defined execution flows MAS in AWorld : Dynamic, real-time decision-making execution flows This design ensures unified underlying capabilities (i.e., Agent, Graph-based Topology) while maintaining extensibility. MAS uses event mechanism as the communication basis, where the output from the previous agent serves as the input for the next agent. Quick MAS Construction Similar to Workflows, we can easily define communication networks between Agents through topology. The key difference is that by using build_type=GraphBuildType.HANDOFF , we allow dynamic decision-making for inter-agent calling relationships: agent1 can selectively decide to call agent2 and agent3 ; the number of calls is also dynamic (once or multiple times) agent2 can selectively decide to call agent3 ; the number of calls is also dynamic (once or multiple times) from aworld.config.conf import AgentConfig from aworld.agents.llm_agent import Agent from aworld.core.agent.swarm import Swarm, GraphBuildType from aworld.runner import Runners # Configure agents agent_conf = AgentConfig(...) agent1 = Agent(name=\"agent1\", conf=agent_conf) agent2 = Agent(name=\"agent2\", conf=agent_conf) agent3 = Agent(name=\"agent3\", conf=agent_conf) # Create swarm with dynamic handoff topology swarm = Swarm( topology=[(agent1, agent2), (agent2, agent3), (agent1, agent3)], build_type=GraphBuildType.HANDOFF ) # Run the swarm Runners.run(input=\"your question\", swarm=swarm) Specifying Entry Agent Since MAS is essentially a Graph by definition, different Agents can accept external input. We can specify which Agent receives the query using the root_agent parameter. swarm = Swarm( topology=[(agent1, agent2), (agent2, agent3), (agent1, agent3)], build_type=GraphBuildType.HANDOFF, root_agent=[agent1] ) Actions Group Parallel The agent's policy may have multiple actions on a decision, including local tools, MCP tools, and agents. AWorld will merge these actions of the same type and automatically parallelize them. The results of parallel execution will be automatically merged and returned to the agent. If you have a custom policy for mixing different types of actions (tools, agents) in parallel, GroupMessage events can be used in async_post_run() or post_run() of Agent . import uuid from aworld.core.event.base import GroupMessage, TopicType, Message def async_post_run(self, policy_result, policy_input, message) -> Message: your_actions = [tool1_action1, tool1_action2, agent1, agent2, tool2_action] group_id = f\"agent_id_{uuid.uuid1().hex}\" # The setting of other parameters can refer to the implementation of the Agent return GroupMessage(payload=your_actions, caller=..., sender=..., session_id=..., group_id=group_id, topic=TopicType.GROUP_ACTIONS, headers=...) Dynamic Routing When the async_policy() or policy() function decides which agent to call next, for special cases, Agents may need customized routing based on specific business rules. You can override the handler in the corresponding Agent: # Handler name consistency must be maintained agent = Agent(..., event_handler_name=\"your_handler_name\") from aworld.runners import HandlerFactory from aworld.runners.handler import DefaultHandler from aworld.core.event.base import Message, ToolMessage, AgentMessage from typing import AsyncGenerator @HandlerFactory.register(name=\"your_handler_name\") class YourHandler(DefaultHandler): def is_valid_message(self, message: Message) -> bool: return message.category == \"your_handler_name\" async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: if not self.is_valid_message(message): return # The type of data is generally ActionModel, but can be customized data = message.payload[0] content = data.policy_info if \"use_tool1\" in content: # not tool call, but want to use tools yield ToolMessage( payload=..., caller=..., sender=..., receiver=\"tool1\", session_id=..., headers=... ) elif \"use_agent1\" in data: # want to use agent as next step yield AgentMessage( payload=..., caller=..., sender=..., receiver=\"agent1\", session_id=..., headers=... ) else: ... You can refer to the implementation of DefaultTaskHandler in AWorld. Two Examples of Overriding Routing: ReAct and Plan-Execute ReAct @HandlerFactory.register(name='react') class ReactHandler(AgentHandler): def is_valid_message(self, message: Message): if message.category != 'react': return False return True async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: yield message Plan-Execute Compared to ReAct, agent2 and agent3 can execute in parallel simultaneously. from aworld.core.common import Observation from aworld.core.event.base import AgentMessage from aworld.logs.util import logger @HandlerFactory.register(name='plan_execute') class PlanExecuteHandler(AgentHandler): def is_valid_message(self, message: Message): if message.category != 'plan_execute': return False return True async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: logger.info(f\"PlanExecuteHandler|handle|taskid={self.task_id}|is_sub_task={message.context._task.is_sub_task}\") content = message.payload # Parse model plan plan = parse_plan(content[0].policy_info) logger.info(f\"PlanExecuteHandler|plan|{plan}\") # Execute steps output, context = execution_steps(plan.steps) # Send event message, notify the next processing agent new_plan_input = Observation(content=output) yield AgentMessage( session_id=message.session_id, payload=new_plan_input, sender=self.name(), receiver=self.swarm.communicate_agent.id(), headers={'context': context} ) For more details, refer to the examples. Combination and Recursion of MAS and Workflow Same or different types of Swarms can be deeply nested, providing multi-level Swarms with different interaction mechanisms to support complex multi-agent interactions. For example, when creating a travel itinerary planner, using a combination of Workflow + MAS, where Workflow provides deterministic processes and MAS handles multi-source information retrieval and integration. from aworld.config.conf import AgentConfig from aworld.agents.llm_agent import Agent from aworld.core.agent.swarm import Swarm, GraphBuildType # Configure agents agent_conf = AgentConfig(...) # Create five agents rewrite = Agent(name=\"rewrite\", conf=agent_conf) plan = Agent(name=\"plan\", conf=agent_conf) search = Agent(name=\"search\", conf=agent_conf) summary = Agent(name=\"summary\", conf=agent_conf) report = Agent(name=\"report\", conf=agent_conf) # Construct a MAS mas = Swarm( topology=[(plan, search), (plan, summary)], build_type=GraphBuildType.HANDOFF, root_agent=[plan] ) # Construct a combination of a workflow with the MAS team combination = Swarm( topology=[(rewrite, mas), (mas, report)], root_agent=[rewrite] )","title":"Multi-agent System Construction"},{"location":"Quickstart/multi-agent_system_construction/#building-and-running-multi-agent-systems-mas","text":"In the AWorld framework, similar to Workflow Construction, the fundamental building block for MAS is the Agent. By introducing the Swarm concept, users can easily, quickly, and efficiently build complex Multi-Agent Systems. In summary: Workflow in AWorld : Static, pre-defined execution flows MAS in AWorld : Dynamic, real-time decision-making execution flows This design ensures unified underlying capabilities (i.e., Agent, Graph-based Topology) while maintaining extensibility. MAS uses event mechanism as the communication basis, where the output from the previous agent serves as the input for the next agent.","title":"Building and Running Multi-Agent Systems (MAS)"},{"location":"Quickstart/multi-agent_system_construction/#quick-mas-construction","text":"Similar to Workflows, we can easily define communication networks between Agents through topology. The key difference is that by using build_type=GraphBuildType.HANDOFF , we allow dynamic decision-making for inter-agent calling relationships: agent1 can selectively decide to call agent2 and agent3 ; the number of calls is also dynamic (once or multiple times) agent2 can selectively decide to call agent3 ; the number of calls is also dynamic (once or multiple times) from aworld.config.conf import AgentConfig from aworld.agents.llm_agent import Agent from aworld.core.agent.swarm import Swarm, GraphBuildType from aworld.runner import Runners # Configure agents agent_conf = AgentConfig(...) agent1 = Agent(name=\"agent1\", conf=agent_conf) agent2 = Agent(name=\"agent2\", conf=agent_conf) agent3 = Agent(name=\"agent3\", conf=agent_conf) # Create swarm with dynamic handoff topology swarm = Swarm( topology=[(agent1, agent2), (agent2, agent3), (agent1, agent3)], build_type=GraphBuildType.HANDOFF ) # Run the swarm Runners.run(input=\"your question\", swarm=swarm)","title":"Quick MAS Construction"},{"location":"Quickstart/multi-agent_system_construction/#specifying-entry-agent","text":"Since MAS is essentially a Graph by definition, different Agents can accept external input. We can specify which Agent receives the query using the root_agent parameter. swarm = Swarm( topology=[(agent1, agent2), (agent2, agent3), (agent1, agent3)], build_type=GraphBuildType.HANDOFF, root_agent=[agent1] )","title":"Specifying Entry Agent"},{"location":"Quickstart/multi-agent_system_construction/#actions-group-parallel","text":"The agent's policy may have multiple actions on a decision, including local tools, MCP tools, and agents. AWorld will merge these actions of the same type and automatically parallelize them. The results of parallel execution will be automatically merged and returned to the agent. If you have a custom policy for mixing different types of actions (tools, agents) in parallel, GroupMessage events can be used in async_post_run() or post_run() of Agent . import uuid from aworld.core.event.base import GroupMessage, TopicType, Message def async_post_run(self, policy_result, policy_input, message) -> Message: your_actions = [tool1_action1, tool1_action2, agent1, agent2, tool2_action] group_id = f\"agent_id_{uuid.uuid1().hex}\" # The setting of other parameters can refer to the implementation of the Agent return GroupMessage(payload=your_actions, caller=..., sender=..., session_id=..., group_id=group_id, topic=TopicType.GROUP_ACTIONS, headers=...)","title":"Actions Group Parallel"},{"location":"Quickstart/multi-agent_system_construction/#dynamic-routing","text":"When the async_policy() or policy() function decides which agent to call next, for special cases, Agents may need customized routing based on specific business rules. You can override the handler in the corresponding Agent: # Handler name consistency must be maintained agent = Agent(..., event_handler_name=\"your_handler_name\") from aworld.runners import HandlerFactory from aworld.runners.handler import DefaultHandler from aworld.core.event.base import Message, ToolMessage, AgentMessage from typing import AsyncGenerator @HandlerFactory.register(name=\"your_handler_name\") class YourHandler(DefaultHandler): def is_valid_message(self, message: Message) -> bool: return message.category == \"your_handler_name\" async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: if not self.is_valid_message(message): return # The type of data is generally ActionModel, but can be customized data = message.payload[0] content = data.policy_info if \"use_tool1\" in content: # not tool call, but want to use tools yield ToolMessage( payload=..., caller=..., sender=..., receiver=\"tool1\", session_id=..., headers=... ) elif \"use_agent1\" in data: # want to use agent as next step yield AgentMessage( payload=..., caller=..., sender=..., receiver=\"agent1\", session_id=..., headers=... ) else: ... You can refer to the implementation of DefaultTaskHandler in AWorld.","title":"Dynamic Routing"},{"location":"Quickstart/multi-agent_system_construction/#two-examples-of-overriding-routing-react-and-plan-execute","text":"","title":"Two Examples of Overriding Routing: ReAct and Plan-Execute"},{"location":"Quickstart/multi-agent_system_construction/#react","text":"@HandlerFactory.register(name='react') class ReactHandler(AgentHandler): def is_valid_message(self, message: Message): if message.category != 'react': return False return True async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: yield message","title":"ReAct"},{"location":"Quickstart/multi-agent_system_construction/#plan-execute","text":"Compared to ReAct, agent2 and agent3 can execute in parallel simultaneously. from aworld.core.common import Observation from aworld.core.event.base import AgentMessage from aworld.logs.util import logger @HandlerFactory.register(name='plan_execute') class PlanExecuteHandler(AgentHandler): def is_valid_message(self, message: Message): if message.category != 'plan_execute': return False return True async def _do_handle(self, message: Message) -> AsyncGenerator[Message, None]: logger.info(f\"PlanExecuteHandler|handle|taskid={self.task_id}|is_sub_task={message.context._task.is_sub_task}\") content = message.payload # Parse model plan plan = parse_plan(content[0].policy_info) logger.info(f\"PlanExecuteHandler|plan|{plan}\") # Execute steps output, context = execution_steps(plan.steps) # Send event message, notify the next processing agent new_plan_input = Observation(content=output) yield AgentMessage( session_id=message.session_id, payload=new_plan_input, sender=self.name(), receiver=self.swarm.communicate_agent.id(), headers={'context': context} ) For more details, refer to the examples.","title":"Plan-Execute"},{"location":"Quickstart/multi-agent_system_construction/#combination-and-recursion-of-mas-and-workflow","text":"Same or different types of Swarms can be deeply nested, providing multi-level Swarms with different interaction mechanisms to support complex multi-agent interactions. For example, when creating a travel itinerary planner, using a combination of Workflow + MAS, where Workflow provides deterministic processes and MAS handles multi-source information retrieval and integration. from aworld.config.conf import AgentConfig from aworld.agents.llm_agent import Agent from aworld.core.agent.swarm import Swarm, GraphBuildType # Configure agents agent_conf = AgentConfig(...) # Create five agents rewrite = Agent(name=\"rewrite\", conf=agent_conf) plan = Agent(name=\"plan\", conf=agent_conf) search = Agent(name=\"search\", conf=agent_conf) summary = Agent(name=\"summary\", conf=agent_conf) report = Agent(name=\"report\", conf=agent_conf) # Construct a MAS mas = Swarm( topology=[(plan, search), (plan, summary)], build_type=GraphBuildType.HANDOFF, root_agent=[plan] ) # Construct a combination of a workflow with the MAS team combination = Swarm( topology=[(rewrite, mas), (mas, report)], root_agent=[rewrite] )","title":"Combination and Recursion of MAS and Workflow"},{"location":"Quickstart/workflow_construction/","text":"We use the classic graph syntax to describe workflows in AWorld. The following are the basic scenarios for constructing agent workflows. Agent Native Workflow Sequential \"\"\" Sequential Agent Pipeline: agent1 \u2192 agent2 \u2192 agent3 Executes agents in sequence where each agent's output becomes the next agent's input, enabling multi-step collaborative processing. \"\"\" swarm = Swarm([(agent1, agent2), (agent2, agent3)], root_agent=[agent1]) result: TaskResponse = Runners.run(input=question, swarm=swarm) Parallel \"\"\" Parallel Agent Execution with Barrier Synchronization Input \u2500\u2500\u252c\u2500\u2192 agent1 \u2500\u2500\u2510 \u2502 \u251c\u2500\u2500\u2192 agent3 (barrier wait) \u2514\u2500\u2192 agent2 \u2500\u2500\u2518 - agent1 and agent2 execute in parallel - agent3 acts as a barrier, waiting for both agents - agent3 processes combined outputs from agent1 and agent2 \"\"\" swarm = Swarm([(agent1, agent3), (agent2, agent3)], root_agent=[agent1, agent2]) result: TaskResponse = Runners.run(input=question, swarm=swarm) Parallel Multi-Path \"\"\" Parallel Multi-Path Agent Execution Input \u2500\u2500\u2192 agent1 \u2500\u2500\u252c\u2500\u2500\u2192 agent2 \u2500\u2500\u2510 \u2502 \u2502 \u2514\u2500\u2500\u2192 agent3 \u2190\u2500\u2518 (barrier wait for agent1 & agent2) - Single input enters only through agent1 - agent1 distributes to both agent2 and agent3 - agent2 processes and feeds agent3 - agent3 waits for both agent1 and agent2 completion - agent3 synthesizes outputs from both agent1 and agent2 \"\"\" swarm = Swarm([(agent1, agent2), (agent1, agent3), (agent2, agent3)], root_agent=[agent1]) result: TaskResponse = Runners.run(input=question, swarm=swarm) Task Native Workflow Task native workflow is further implemented for Isolating the agent runtimes and environments, in the distributed or other easy-to-overlap scenarios. Task native workflow is further implemented for isolating agent runtimes and environments, particularly useful in distributed or other scenarios where tool-isolation is required. task1 = Task(input=\"my question\", agent=agent1) task2 = Task(agent=agent2) task3 = Task(agent=agent3) tasks = [task1, task2, task3] result: Dict[str, TaskResponse] = Runners.run_task(tasks, RunConfig(sequence_dependent=True))","title":"Workflow Construction"},{"location":"Quickstart/workflow_construction/#agent-native-workflow","text":"","title":"Agent Native Workflow"},{"location":"Quickstart/workflow_construction/#sequential","text":"\"\"\" Sequential Agent Pipeline: agent1 \u2192 agent2 \u2192 agent3 Executes agents in sequence where each agent's output becomes the next agent's input, enabling multi-step collaborative processing. \"\"\" swarm = Swarm([(agent1, agent2), (agent2, agent3)], root_agent=[agent1]) result: TaskResponse = Runners.run(input=question, swarm=swarm)","title":"Sequential"},{"location":"Quickstart/workflow_construction/#parallel","text":"\"\"\" Parallel Agent Execution with Barrier Synchronization Input \u2500\u2500\u252c\u2500\u2192 agent1 \u2500\u2500\u2510 \u2502 \u251c\u2500\u2500\u2192 agent3 (barrier wait) \u2514\u2500\u2192 agent2 \u2500\u2500\u2518 - agent1 and agent2 execute in parallel - agent3 acts as a barrier, waiting for both agents - agent3 processes combined outputs from agent1 and agent2 \"\"\" swarm = Swarm([(agent1, agent3), (agent2, agent3)], root_agent=[agent1, agent2]) result: TaskResponse = Runners.run(input=question, swarm=swarm)","title":"Parallel"},{"location":"Quickstart/workflow_construction/#parallel-multi-path","text":"\"\"\" Parallel Multi-Path Agent Execution Input \u2500\u2500\u2192 agent1 \u2500\u2500\u252c\u2500\u2500\u2192 agent2 \u2500\u2500\u2510 \u2502 \u2502 \u2514\u2500\u2500\u2192 agent3 \u2190\u2500\u2518 (barrier wait for agent1 & agent2) - Single input enters only through agent1 - agent1 distributes to both agent2 and agent3 - agent2 processes and feeds agent3 - agent3 waits for both agent1 and agent2 completion - agent3 synthesizes outputs from both agent1 and agent2 \"\"\" swarm = Swarm([(agent1, agent2), (agent1, agent3), (agent2, agent3)], root_agent=[agent1]) result: TaskResponse = Runners.run(input=question, swarm=swarm)","title":"Parallel Multi-Path"},{"location":"Quickstart/workflow_construction/#task-native-workflow","text":"Task native workflow is further implemented for Isolating the agent runtimes and environments, in the distributed or other easy-to-overlap scenarios. Task native workflow is further implemented for isolating agent runtimes and environments, particularly useful in distributed or other scenarios where tool-isolation is required. task1 = Task(input=\"my question\", agent=agent1) task2 = Task(agent=agent2) task3 = Task(agent=agent3) tasks = [task1, task2, task3] result: Dict[str, TaskResponse] = Runners.run_task(tasks, RunConfig(sequence_dependent=True))","title":"Task Native Workflow"}]}