# SearchReasoningAgent - æœç´¢æ¨ç†æ™ºèƒ½ä½“

## æ™ºèƒ½ä½“æè¿°
SearchReasoningAgentæ˜¯ä¸€ä¸ªå…·å¤‡ç½‘ç»œæœç´¢å’ŒåŸºç¡€æ¨ç†èƒ½åŠ›çš„æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿè¿›è¡Œä¿¡æ¯æ£€ç´¢ã€é€»è¾‘åˆ†æå’ŒçŸ¥è¯†æ•´åˆã€‚

## æ ¸å¿ƒåŠŸèƒ½
1. **ç½‘ç»œæœç´¢èƒ½åŠ›** - è¿›è¡Œç½‘ç»œä¿¡æ¯æ£€ç´¢å’Œæœç´¢
2. **åŸºç¡€æ¨ç†èƒ½åŠ›** - è¿›è¡Œé€»è¾‘æ¨ç†ã€åˆ†æå’Œåˆ¤æ–­  
3. **ä¿¡æ¯æ•´åˆèƒ½åŠ›** - å°†æœç´¢ç»“æœä¸æ¨ç†ç»“åˆï¼Œæä¾›ç»¼åˆæ€§ç­”æ¡ˆ

## åº”ç”¨åœºæ™¯
- ä¿¡æ¯æŸ¥è¯¢å’Œäº‹å®éªŒè¯
- é€»è¾‘åˆ†æå’Œé—®é¢˜è§£ç­”
- çŸ¥è¯†æ•´åˆå’Œç»¼åˆåˆ†æ
- å¤æ‚æŸ¥è¯¢ä»»åŠ¡å¤„ç†

## æŠ€æœ¯å®ç°

```python
import os
import traceback
from typing import Dict, Any, List, Optional
import json

from aworld.config import AgentConfig, ModelConfig
from aworld.core.agent.base import BaseAgent
from aworld.core.agent.swarm import Swarm
from aworld.core.common import Observation, ActionModel
from aworld.core.event.base import Message
from aworld.logs.util import logger
from aworld.models.llm import acall_llm_model
from aworld_cli.core import agent
from mcp_config import mcp_config


class SearchReasoningAgent(BaseAgent[Observation, List[ActionModel]]):
    """æœç´¢æ¨ç†æ™ºèƒ½ä½“ - å…·å¤‡ç½‘ç»œæœç´¢å’ŒåŸºç¡€æ¨ç†èƒ½åŠ›çš„æ™ºèƒ½ä½“"""

    def __init__(self, name: str, conf: AgentConfig = None, desc: str = None,
                 system_prompt: str = None, tool_names: List[str] = None, **kwargs):
        super().__init__(name=name, conf=conf, desc=desc, **kwargs)
        self.system_prompt = system_prompt or self._get_default_system_prompt()
        self.model_name = conf.llm_config.llm_model_name if conf and conf.llm_config else "gpt-4"
        
        # æœç´¢å’Œæ¨ç†çŠ¶æ€ç®¡ç†
        self.search_results = []
        self.reasoning_steps = []
        self.integrated_analysis = ""

    def _get_default_system_prompt(self) -> str:
        """è·å–é»˜è®¤ç³»ç»Ÿæç¤º"""
        return """ä½ æ˜¯SearchReasoningAgentï¼Œä¸€ä¸ªä¸“ä¸šçš„æœç´¢æ¨ç†æ™ºèƒ½ä½“ã€‚ä½ å…·å¤‡ä»¥ä¸‹æ ¸å¿ƒèƒ½åŠ›ï¼š

ğŸ” **ç½‘ç»œæœç´¢èƒ½åŠ›**ï¼š
- èƒ½å¤Ÿè¿›è¡Œç²¾å‡†çš„ç½‘ç»œä¿¡æ¯æ£€ç´¢
- è¯†åˆ«å…³é”®è¯å¹¶æ„å»ºæœ‰æ•ˆçš„æœç´¢æŸ¥è¯¢
- ä»æœç´¢ç»“æœä¸­æå–æœ‰ä»·å€¼çš„ä¿¡æ¯

ğŸ§  **åŸºç¡€æ¨ç†èƒ½åŠ›**ï¼š
- è¿›è¡Œé€»è¾‘æ¨ç†ã€åˆ†æå’Œåˆ¤æ–­
- è¯†åˆ«ä¿¡æ¯é—´çš„å…³è”å’Œæ¨¡å¼
- åŸºäºè¯æ®å¾—å‡ºåˆç†ç»“è®º

ğŸ”— **ä¿¡æ¯æ•´åˆèƒ½åŠ›**ï¼š
- å°†æœç´¢ç»“æœä¸æ¨ç†åˆ†æç›¸ç»“åˆ
- æä¾›ç»¼åˆæ€§ã€ç»“æ„åŒ–çš„ç­”æ¡ˆ
- ç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§

**å·¥ä½œæµç¨‹**ï¼š
1. åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œè¯†åˆ«å…³é”®ä¿¡æ¯éœ€æ±‚
2. åˆ¶å®šæœç´¢ç­–ç•¥ï¼Œæ‰§è¡Œç½‘ç»œæœç´¢
3. å¯¹æœç´¢ç»“æœè¿›è¡Œæ¨ç†åˆ†æ
4. æ•´åˆä¿¡æ¯ï¼Œæä¾›ç»¼åˆæ€§ç­”æ¡ˆ
5. éªŒè¯ç»“è®ºçš„é€»è¾‘æ€§å’Œå‡†ç¡®æ€§

**å›ç­”åŸåˆ™**ï¼š
- åŸºäºäº‹å®å’Œè¯æ®
- é€»è¾‘æ¸…æ™°ï¼Œç»“æ„å®Œæ•´
- æ‰¿è®¤ä¸ç¡®å®šæ€§ï¼Œé¿å…è¿‡åº¦æ¨æµ‹
- æä¾›ä¿¡æ¯æ¥æºå’Œå¯ä¿¡åº¦è¯„ä¼°

è¯·æ ¹æ®ç”¨æˆ·çš„å…·ä½“éœ€æ±‚ï¼Œè¿ç”¨ä½ çš„æœç´¢å’Œæ¨ç†èƒ½åŠ›æä¾›å¸®åŠ©ã€‚"""

    async def async_policy(self, observation: Observation, info: Dict[str, Any] = {}, message: Message = None,
                           **kwargs) -> List[ActionModel]:
        """æ‰§è¡Œæœç´¢æ¨ç†çš„æ ¸å¿ƒé€»è¾‘"""
        try:
            # åˆå§‹åŒ–å·¥å…·
            try:
                await self.async_desc_transform(context=message.context)
            except Exception as e:
                logger.warning(f"{self.name()} get tools desc fail, no tool to use. error: {traceback.format_exc()}")
                self.tools = []

            # åˆ†æç”¨æˆ·æŸ¥è¯¢
            query_analysis = await self._analyze_query(observation.content)
            logger.info(f"æŸ¥è¯¢åˆ†æç»“æœ: {query_analysis}")

            # æ‰§è¡Œæœç´¢æ¨ç†æµç¨‹
            result = await self._execute_search_reasoning_workflow(observation.content, query_analysis)

            return [ActionModel(
                agent_name=self.name(),
                policy_info=result
            )]

        except Exception as e:
            logger.error(f"SearchReasoningAgent {self.name()} æ‰§è¡Œå¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            return [ActionModel(
                agent_name=self.name(),
                policy_info=f"æ‰§è¡Œå¤±è´¥: {str(e)}"
            )]

    async def _analyze_query(self, query: str) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œè¯†åˆ«æœç´¢éœ€æ±‚å’Œæ¨ç†è¦æ±‚"""
        try:
            analysis_prompt = f"""è¯·åˆ†æä»¥ä¸‹ç”¨æˆ·æŸ¥è¯¢ï¼Œè¯†åˆ«ï¼š
1. æ ¸å¿ƒé—®é¢˜å’Œä¿¡æ¯éœ€æ±‚
2. éœ€è¦æœç´¢çš„å…³é”®è¯
3. æ¨ç†åˆ†æçš„é‡ç‚¹
4. é¢„æœŸçš„ç­”æ¡ˆç±»å‹

ç”¨æˆ·æŸ¥è¯¢ï¼š{query}

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "core_question": "æ ¸å¿ƒé—®é¢˜",
    "search_keywords": ["å…³é”®è¯1", "å…³é”®è¯2"],
    "reasoning_focus": "æ¨ç†é‡ç‚¹",
    "answer_type": "ç­”æ¡ˆç±»å‹",
    "complexity": "ç®€å•/ä¸­ç­‰/å¤æ‚"
}}"""

            messages = [
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæŸ¥è¯¢åˆ†æä¸“å®¶ï¼Œèƒ½å¤Ÿå‡†ç¡®è¯†åˆ«ç”¨æˆ·çš„ä¿¡æ¯éœ€æ±‚ã€‚"},
                {"role": "user", "content": analysis_prompt}
            ]

            response = await acall_llm_model(
                self.llm,
                messages=messages,
                model=self.model_name,
                temperature=0.3
            )

            # å°è¯•è§£æJSONå“åº”
            try:
                analysis = json.loads(response.content)
            except:
                # å¦‚æœJSONè§£æå¤±è´¥ï¼Œè¿”å›åŸºç¡€åˆ†æ
                analysis = {
                    "core_question": query,
                    "search_keywords": [query],
                    "reasoning_focus": "åŸºç¡€åˆ†æ",
                    "answer_type": "ç»¼åˆå›ç­”",
                    "complexity": "ä¸­ç­‰"
                }

            return analysis

        except Exception as e:
            logger.error(f"æŸ¥è¯¢åˆ†æå¤±è´¥: {str(e)}")
            return {
                "core_question": query,
                "search_keywords": [query],
                "reasoning_focus": "åŸºç¡€åˆ†æ",
                "answer_type": "ç»¼åˆå›ç­”",
                "complexity": "ä¸­ç­‰"
            }

    async def _execute_search_reasoning_workflow(self, original_query: str, query_analysis: Dict[str, Any]) -> str:
        """æ‰§è¡Œå®Œæ•´çš„æœç´¢æ¨ç†å·¥ä½œæµç¨‹"""
        try:
            workflow_steps = []
            
            # æ­¥éª¤1ï¼šæ‰§è¡Œç½‘ç»œæœç´¢
            search_results = await self._perform_search(query_analysis.get("search_keywords", [original_query]))
            workflow_steps.append("âœ… å®Œæˆç½‘ç»œæœç´¢")
            
            # æ­¥éª¤2ï¼šæ‰§è¡Œæ¨ç†åˆ†æ
            reasoning_results = await self._perform_reasoning(original_query, search_results, query_analysis)
            workflow_steps.append("âœ… å®Œæˆæ¨ç†åˆ†æ")
            
            # æ­¥éª¤3ï¼šæ•´åˆä¿¡æ¯
            integrated_result = await self._integrate_information(original_query, search_results, reasoning_results, query_analysis)
            workflow_steps.append("âœ… å®Œæˆä¿¡æ¯æ•´åˆ")
            
            # æ„å»ºæœ€ç»ˆå›ç­”
            final_answer = await self._build_final_answer(original_query, integrated_result, workflow_steps)
            
            return final_answer

        except Exception as e:
            logger.error(f"æœç´¢æ¨ç†å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}")
            return f"å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}"

    async def _perform_search(self, keywords: List[str]) -> Dict[str, Any]:
        """æ‰§è¡Œç½‘ç»œæœç´¢"""
        try:
            search_results = {"results": [], "summary": ""}
            
            # æ„å»ºæœç´¢æ¶ˆæ¯
            search_query = " ".join(keywords) if isinstance(keywords, list) else str(keywords)
            
            messages = [
                {"role": "system", "content": "ä½ éœ€è¦ä½¿ç”¨æœç´¢å·¥å…·æ¥æŸ¥æ‰¾ç›¸å…³ä¿¡æ¯ã€‚"},
                {"role": "user", "content": f"è¯·æœç´¢å…³äºä»¥ä¸‹å†…å®¹çš„ä¿¡æ¯ï¼š{search_query}"}
            ]

            # ä½¿ç”¨å·¥å…·è¿›è¡Œæœç´¢
            tools = self.tools if self.tools else None
            
            response = await acall_llm_model(
                self.llm,
                messages=messages,
                model=self.model_name,
                temperature=0.5,
                tools=tools
            )

            search_results["summary"] = response.content or "æœç´¢å®Œæˆ"
            search_results["query"] = search_query
            
            logger.info(f"æœç´¢å®Œæˆ: {search_query}")
            return search_results

        except Exception as e:
            logger.error(f"æœç´¢æ‰§è¡Œå¤±è´¥: {str(e)}")
            return {"results": [], "summary": f"æœç´¢å¤±è´¥: {str(e)}", "query": str(keywords)}

    async def _perform_reasoning(self, original_query: str, search_results: Dict[str, Any], query_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œæ¨ç†åˆ†æ"""
        try:
            reasoning_prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœï¼Œå¯¹ç”¨æˆ·æŸ¥è¯¢è¿›è¡Œæ·±å…¥çš„æ¨ç†åˆ†æï¼š

åŸå§‹æŸ¥è¯¢ï¼š{original_query}
æŸ¥è¯¢åˆ†æï¼š{query_analysis}
æœç´¢ç»“æœï¼š{search_results.get('summary', 'æ— æœç´¢ç»“æœ')}

è¯·è¿›è¡Œä»¥ä¸‹æ¨ç†åˆ†æï¼š
1. ä¿¡æ¯å¯ä¿¡åº¦è¯„ä¼°
2. é€»è¾‘å…³ç³»åˆ†æ
3. å› æœå…³ç³»æ¨ç†
4. ç»“è®ºæ¨å¯¼
5. ä¸ç¡®å®šæ€§è¯†åˆ«

è¯·ä½¿ç”¨æ¨ç†å·¥å…·è¿›è¡Œæ·±å…¥åˆ†æã€‚"""

            messages = [
                {"role": "system", "content": "ä½ éœ€è¦ä½¿ç”¨æ¨ç†å·¥å…·è¿›è¡Œé€»è¾‘åˆ†æå’Œæ¨ç†ã€‚"},
                {"role": "user", "content": reasoning_prompt}
            ]

            # ä½¿ç”¨å·¥å…·è¿›è¡Œæ¨ç†
            tools = self.tools if self.tools else None
            
            response = await acall_llm_model(
                self.llm,
                messages=messages,
                model=self.model_name,
                temperature=0.3,
                tools=tools
            )

            reasoning_results = {
                "analysis": response.content or "æ¨ç†åˆ†æå®Œæˆ",
                "confidence": "ä¸­ç­‰",
                "key_insights": []
            }
            
            logger.info("æ¨ç†åˆ†æå®Œæˆ")
            return reasoning_results

        except Exception as e:
            logger.error(f"æ¨ç†åˆ†æå¤±è´¥: {str(e)}")
            return {"analysis": f"æ¨ç†åˆ†æå¤±è´¥: {str(e)}", "confidence": "ä½", "key_insights": []}

    async def _integrate_information(self, original_query: str, search_results: Dict[str, Any], 
                                   reasoning_results: Dict[str, Any], query_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """æ•´åˆæœç´¢ç»“æœå’Œæ¨ç†åˆ†æ"""
        try:
            integration_prompt = f"""è¯·æ•´åˆä»¥ä¸‹ä¿¡æ¯ï¼Œä¸ºç”¨æˆ·æŸ¥è¯¢æä¾›ç»¼åˆæ€§ç­”æ¡ˆï¼š

ç”¨æˆ·æŸ¥è¯¢ï¼š{original_query}
æŸ¥è¯¢åˆ†æï¼š{query_analysis}
æœç´¢ç»“æœï¼š{search_results.get('summary', 'æ— æœç´¢ç»“æœ')}
æ¨ç†åˆ†æï¼š{reasoning_results.get('analysis', 'æ— æ¨ç†ç»“æœ')}

æ•´åˆè¦æ±‚ï¼š
1. ç¡®ä¿ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œä¸€è‡´æ€§
2. æä¾›ç»“æ„åŒ–çš„ç»¼åˆç­”æ¡ˆ
3. æ ‡æ˜ä¿¡æ¯æ¥æºå’Œå¯ä¿¡åº¦
4. è¯†åˆ«å¹¶è¯´æ˜ä¸ç¡®å®šæ€§
5. æä¾›å®ç”¨çš„å»ºè®®æˆ–ç»“è®º"""

            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": integration_prompt}
            ]

            response = await acall_llm_model(
                self.llm,
                messages=messages,
                model=self.model_name,
                temperature=0.4
            )

            integrated_result = {
                "integrated_answer": response.content or "ä¿¡æ¯æ•´åˆå®Œæˆ",
                "confidence_level": reasoning_results.get("confidence", "ä¸­ç­‰"),
                "sources": [search_results.get("query", "æœç´¢ç»“æœ")],
                "limitations": []
            }
            
            logger.info("ä¿¡æ¯æ•´åˆå®Œæˆ")
            return integrated_result

        except Exception as e:
            logger.error(f"ä¿¡æ¯æ•´åˆå¤±è´¥: {str(e)}")
            return {"integrated_answer": f"ä¿¡æ¯æ•´åˆå¤±è´¥: {str(e)}", "confidence_level": "ä½", "sources": [], "limitations": []}

    async def _build_final_answer(self, original_query: str, integrated_result: Dict[str, Any], workflow_steps: List[str]) -> str:
        """æ„å»ºæœ€ç»ˆç­”æ¡ˆ"""
        try:
            final_answer = f"""# ğŸ” SearchReasoningAgent åˆ†ææŠ¥å‘Š

## ğŸ“‹ æŸ¥è¯¢å†…å®¹
{original_query}

## ğŸ”„ å¤„ç†æµç¨‹
{chr(10).join(workflow_steps)}

## ğŸ“Š ç»¼åˆåˆ†æç»“æœ
{integrated_result.get('integrated_answer', 'æ— åˆ†æç»“æœ')}

## ğŸ“ˆ å¯ä¿¡åº¦è¯„ä¼°
**ç½®ä¿¡åº¦**: {integrated_result.get('confidence_level', 'æœªçŸ¥')}

## ğŸ“š ä¿¡æ¯æ¥æº
{chr(10).join([f"- {source}" for source in integrated_result.get('sources', ['æ— æ¥æºä¿¡æ¯'])])}

## âš ï¸ æ³¨æ„äº‹é¡¹
- æœ¬åˆ†æåŸºäºå½“å‰å¯è·å–çš„ä¿¡æ¯
- å»ºè®®ç»“åˆå¤šä¸ªä¿¡æ¯æºè¿›è¡ŒéªŒè¯
- å¦‚æœ‰ç–‘é—®ï¼Œè¯·è¿›ä¸€æ­¥æ ¸å®ç›¸å…³ä¿¡æ¯

---
*ç”±SearchReasoningAgentæä¾› - ç½‘ç»œæœç´¢ + åŸºç¡€æ¨ç†*"""

            return final_answer

        except Exception as e:
            logger.error(f"æ„å»ºæœ€ç»ˆç­”æ¡ˆå¤±è´¥: {str(e)}")
            return f"æ„å»ºæœ€ç»ˆç­”æ¡ˆå¤±è´¥: {str(e)}"


@agent(
    name="search_reasoning_agent",
    desc="å…·å¤‡ç½‘ç»œæœç´¢å’ŒåŸºç¡€æ¨ç†èƒ½åŠ›çš„æ™ºèƒ½ä½“ï¼Œèƒ½å¤Ÿè¿›è¡Œä¿¡æ¯æ£€ç´¢ã€é€»è¾‘åˆ†æå’ŒçŸ¥è¯†æ•´åˆ"
)
def build_search_reasoning_swarm():
    """æ„å»ºæœç´¢æ¨ç†æ™ºèƒ½ä½“ç¾¤"""
    # åˆ›å»ºAgenté…ç½®
    agent_config = AgentConfig(
        llm_config=ModelConfig(
            llm_model_name=os.environ.get("LLM_MODEL_NAME", "gpt-4"),
            llm_provider=os.environ.get("LLM_PROVIDER", "openai"),
            llm_api_key=os.environ.get("LLM_API_KEY"),
            llm_base_url=os.environ.get("LLM_BASE_URL", "https://api.openai.com/v1"),
            llm_temperature=float(os.environ.get("LLM_TEMPERATURE", "0.7"))
        )
    )

    # ä»mcp_configä¸­æå–æ‰€æœ‰æœåŠ¡å™¨åç§°
    mcp_servers = list(mcp_config.get("mcpServers", {}).keys())

    # åˆ›å»ºSearchReasoningAgentå®ä¾‹
    search_reasoning_agent = SearchReasoningAgent(
        name="search_reasoning_agent",
        desc="å…·å¤‡ç½‘ç»œæœç´¢å’ŒåŸºç¡€æ¨ç†èƒ½åŠ›çš„æ™ºèƒ½ä½“ï¼Œä¸“é—¨ç”¨äºä¿¡æ¯æ£€ç´¢ã€é€»è¾‘åˆ†æå’ŒçŸ¥è¯†æ•´åˆ",
        conf=agent_config,
        system_prompt=None,  # ä½¿ç”¨é»˜è®¤ç³»ç»Ÿæç¤º
        mcp_servers=mcp_servers,
        mcp_config=mcp_config
    )

    # è¿”å›åŒ…å«è¯¥Agentçš„Swarm
    return Swarm(search_reasoning_agent)
```

## MCPé…ç½®æ–‡ä»¶

```python
# mcp_config.py - MCPæœåŠ¡å™¨é…ç½®

mcp_config = {
    "mcpServers": {
        "search": {
            "command": "python",
            "args": [
                "-m",
                "examples.gaia.mcp_collections.tools.search"
            ],
            "env": {
                "GOOGLE_API_KEY": "${GOOGLE_API_KEY}",
                "GOOGLE_CSE_ID": "${GOOGLE_CSE_ID}"
            },
            "client_session_timeout_seconds": 9999.0
        },
        "reasoning": {
            "command": "python",
            "args": [
                "-m",
                "examples.gaia.mcp_collections.intelligence.think"
            ],
            "env": {},
            "client_session_timeout_seconds": 9999.0
        },
        "terminal": {
            "command": "python",
            "args": [
                "-m",
                "examples.gaia.mcp_collections.tools.terminal"
            ],
            "env": {},
            "client_session_timeout_seconds": 9999.0
        },
        "browser": {
            "command": "python",
            "args": [
                "-m",
                "examples.gaia.mcp_collections.tools.browser"
            ],
            "env": {
                "LLM_MODEL_NAME": "${LLM_MODEL_NAME}",
                "LLM_API_KEY": "${LLM_API_KEY}",
                "LLM_BASE_URL": "${LLM_BASE_URL}"
            },
            "client_session_timeout_seconds": 9999.0
        }
    }
}
```

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨
```python
# åˆ›å»ºæ™ºèƒ½ä½“å®ä¾‹
swarm = build_search_reasoning_swarm()
agent = swarm.agents[0]

# ç¤ºä¾‹æŸ¥è¯¢1ï¼šäº‹å®éªŒè¯
query1 = "è¯·éªŒè¯å¹¶åˆ†æï¼šäººå·¥æ™ºèƒ½æ˜¯å¦çœŸçš„ä¼šåœ¨2030å¹´è¶…è¶Šäººç±»æ™ºèƒ½ï¼Ÿ"

# ç¤ºä¾‹æŸ¥è¯¢2ï¼šä¿¡æ¯æ•´åˆ
query2 = "æ¯”è¾ƒåˆ†æä¸åŒå›½å®¶çš„æ–°èƒ½æºæ”¿ç­–ï¼Œå¹¶æ¨ç†å…¶å¯¹å…¨çƒæ°”å€™å˜åŒ–çš„å½±å“"

# ç¤ºä¾‹æŸ¥è¯¢3ï¼šé€»è¾‘åˆ†æ
query3 = "åˆ†æå½“å‰ç»æµå½¢åŠ¿ä¸‹ï¼ŒæŠ•èµ„ç§‘æŠ€è‚¡æ˜¯å¦æ˜¯æ˜æ™ºçš„é€‰æ‹©ï¼Ÿ"
```

### é«˜çº§åŠŸèƒ½
- **å¤šè½®å¯¹è¯æ”¯æŒ**ï¼šèƒ½å¤ŸåŸºäºä¸Šä¸‹æ–‡è¿›è¡Œè¿ç»­æ¨ç†
- **ä¿¡æ¯æºè¿½è¸ª**ï¼šè®°å½•å’Œè¯„ä¼°ä¿¡æ¯æ¥æºçš„å¯ä¿¡åº¦
- **ä¸ç¡®å®šæ€§ç®¡ç†**ï¼šæ˜ç¡®æ ‡è¯†æ¨ç†ä¸­çš„ä¸ç¡®å®šå› ç´ 
- **ç»“æ„åŒ–è¾“å‡º**ï¼šæä¾›æ ¼å¼åŒ–çš„åˆ†ææŠ¥å‘Š

## æŠ€æœ¯ç‰¹ç‚¹

### ğŸ” æœç´¢åŠŸèƒ½æ¨¡å—
- æ™ºèƒ½å…³é”®è¯æå–å’ŒæŸ¥è¯¢æ„å»º
- å¤šæºä¿¡æ¯æ£€ç´¢å’Œç»“æœç­›é€‰
- ä¿¡æ¯è´¨é‡è¯„ä¼°å’Œæ’åº

### ğŸ§  æ¨ç†é€»è¾‘æ¨¡å—  
- é€»è¾‘æ¨ç†å’Œå› æœåˆ†æ
- æ¨¡å¼è¯†åˆ«å’Œå…³è”åˆ†æ
- ä¸ç¡®å®šæ€§é‡åŒ–å’Œé£é™©è¯„ä¼°

### ğŸ”— ä¿¡æ¯æ•´åˆæ¨¡å—
- å¤šæºä¿¡æ¯èåˆå’Œä¸€è‡´æ€§æ£€æŸ¥
- ç»“æ„åŒ–çŸ¥è¯†è¡¨ç¤º
- ç»¼åˆæ€§ç»“è®ºç”Ÿæˆ

### ğŸ›¡ï¸ é”™è¯¯å¤„ç†å’Œå¼‚å¸¸ç®¡ç†
- å®Œå–„çš„å¼‚å¸¸æ•è·å’Œå¤„ç†æœºåˆ¶
- ä¼˜é›…çš„é™çº§ç­–ç•¥
- è¯¦ç»†çš„æ—¥å¿—è®°å½•å’Œé”™è¯¯è¿½è¸ª

## ç¯å¢ƒå˜é‡é…ç½®

```bash
# LLMé…ç½®
export LLM_MODEL_NAME="gpt-4"
export LLM_PROVIDER="openai"
export LLM_API_KEY="your_openai_api_key"
export LLM_BASE_URL="https://api.openai.com/v1"
export LLM_TEMPERATURE="0.7"

# æœç´¢APIé…ç½®
export GOOGLE_API_KEY="your_google_api_key"
export GOOGLE_CSE_ID="your_custom_search_engine_id"
```

è¿™ä¸ªSearchReasoningAgentæ™ºèƒ½ä½“å…·å¤‡å®Œæ•´çš„æœç´¢æ¨ç†èƒ½åŠ›ï¼Œèƒ½å¤Ÿå¤„ç†å¤æ‚çš„ä¿¡æ¯æŸ¥è¯¢ä»»åŠ¡ï¼Œæä¾›å‡†ç¡®ã€ç»“æ„åŒ–çš„åˆ†æç»“æœã€‚