# AWorld CAST Framework

**C**ode **AS**is**T**ant - A unified code assistant framework based on Tree-sitter, designed specifically for agent code understanding and optimization. Adopts a three-tier hierarchical indexing architecture to provide LLMs with precise code understanding and modification capabilities.

## üèóÔ∏è Layered Architecture Design

### Overall Architecture Overview

```
                    AWorld CAST Framework
                        (10,000+ lines)
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                üéØ ACast Core Framework                  ‚îÇ
    ‚îÇ                  (core.py - 1,637 lines)                ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ             ‚îÇ             ‚îÇ             ‚îÇ
   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   ‚îÇüåêSearch‚îÇ  ‚îÇüìäAnalyzer ‚îÇ  ‚îÇüîßCoder    ‚îÇ ‚îÇüõ†Ô∏èTools  ‚îÇ
   ‚îÇ  Engine  ‚îÇ  ‚îÇ  Layer  ‚îÇ  ‚îÇ  Layer   ‚îÇ ‚îÇ  Layer ‚îÇ
   ‚îÇsearchers ‚îÇ  ‚îÇanalyzers‚îÇ  ‚îÇ coders/  ‚îÇ ‚îÇ tools/ ‚îÇ
   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îò
        ‚îÇ            ‚îÇ             ‚îÇ            ‚îÇ
        ‚îÇ      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ
        ‚îÇ      ‚îÇüîçParser  ‚îÇ  ‚îÇüìÑData   ‚îÇ      ‚îÇ
        ‚îÇ      ‚îÇ  Layer  ‚îÇ  ‚îÇ Models  ‚îÇ      ‚îÇ
        ‚îÇ      ‚îÇparsers/ ‚îÇ  ‚îÇmodels.py‚îÇ      ‚îÇ
        ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ
        ‚îÇ                                     ‚îÇ
        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                    ‚îÇ
               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚îÇ  üóÑÔ∏è Three-Tier Hierarchical ‚îÇ
               ‚îÇ      Index Architecture      ‚îÇ
               ‚îÇ  L1-Logic | L2-Skeleton | L3-Implementation ‚îÇ
               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üì¶ Core Module Details

### üéØ Framework Entry Layer
| Module | Lines | Description | Design Pattern |
|--------|-------|-------------|----------------|
| `core.py` | 1,637 | ACast main framework entry, unified management of parsers and analyzers | Facade Pattern, Factory Pattern |
| `models.py` | 431 | Data models: Symbol, CodeNode, RepositoryMap, etc. | Builder Pattern, Serialization Design |
| `analyzer.py` | 381 | Abstract analyzer interface, PageRank importance calculation | Template Method Pattern |
| `parser_utils.py` | - | Parser factory functions, automatic language detection | Factory Method Pattern |

### üîç Parser Layer (`ast_parsers/`)

Adopts a unified parsing architecture based on **Tree-sitter**:

```
BaseParser (Abstract Base Class)
‚îú‚îÄ‚îÄ PythonParser     Supports .py, .pyi, .pyx
‚îú‚îÄ‚îÄ HtmlParser       Supports .html, .htm
‚îî‚îÄ‚îÄ [Extensible]    JavaScript, Go, Rust...
```

**Core Features**:
- **Unified Interface**: All parsers follow the same abstract interface
- **High-Precision Parsing**: Based on Tree-sitter syntax parsing engine
- **Symbol Extraction**: Automatically identifies functions, classes, variables, imports, etc.
- **Reference Analysis**: Tracks call relationships and dependencies

### üìä Analyzer Layer (`analyzers/`)

**Main Components**:
- **RepositoryAnalyzer**: Main analyzer that coordinates the entire analysis process
- **analyzer.py**: Abstract analyzer interface supporting PageRank importance calculation
- **repository_analyzer.py**: Multi-level code context recall

**Analysis Algorithms**:
- **PageRank Weighting**: Symbol importance calculation based on call relationships
- **Multi-dimensional Matching**: Four-dimensional relevance scoring (content, signature, documentation, name)
- **Incremental Caching**: SQLite persistence, supports cross-session usage
- **Smart Filtering**: Automatically excludes cache files, build artifacts, etc.

### üîß Coder Layer (`coders/`)

Inspired by the **aider** project's coder architecture, supports multiple code modification strategies:

```
BaseCoder (Abstract Base Class)
‚îú‚îÄ‚îÄ SearchReplaceCoder   Exact match search and replace operations
‚îú‚îÄ‚îÄ DmpCoder             Patch application based on difflib
‚îî‚îÄ‚îÄ OpCoder             JSON operation deployment via patch conversion
```

**Design Principles**:
- **Single Responsibility**: Each coder handles specific operation types
- **Consistency**: All operations return standardized CoderResult objects
- **Extensibility**: Easy to add new coder types

### üåê Search Engine Layer (`searchers/`)

Unified search interface integrating multiple search strategies:

```
SearchEngine (Search Engine Core)
‚îú‚îÄ‚îÄ GrepSearcher        Content search based on Ripgrep
‚îú‚îÄ‚îÄ GlobSearcher         File pattern matching search
‚îú‚îÄ‚îÄ ReadSearcher         File read search
‚îî‚îÄ‚îÄ RipgrepManager      Cross-platform Ripgrep binary management
```

**Special Features**:
- **Tool Registration**: Supports dynamic registration of new search tools
- **Combined Search**: Supports combined use of multiple search tools
- **High Performance**: Integrates Ripgrep for ultra-fast text search

### üõ†Ô∏è Tools Layer (`tools/`)

Serves as the integration bridge between the CAST framework and the AWorld ecosystem:

| Tool | Description | Integration Interface |
|------|-------------|----------------------|
| `cast_analysis_tool.py` | Code analysis and structure extraction | ANALYZE_REPOSITORY, SEARCH_AST |
| `cast_coder_tool.py` | Code modification and deployment tools | GENERATE_SNAPSHOT, DEPLOY_PATCHES, DEPLOY_OPS, SEARCH_REPLACE |
| `cast_search_tool.py` | Search functionality tools | GREP_SEARCH, GLOB_SEARCH, READ_FILE |

## üóÑÔ∏è Three-Tier Hierarchical Index Architecture

The CAST framework implements a **"Dynamic Panoramic - Static Skeleton - On-demand Source"** three-tier architecture, designed to enable efficient, progressive code understanding and optimization. This architecture transforms code from static text into a dynamic, navigable graph structure.

### Design Philosophy: From Static Text to Dynamic Graph

Traditional approaches treat code as either:
- **Static Text**: Direct context injection, leading to token overflow and attention dispersion
- **Fragmented Segments**: RAG-based retrieval, losing structural logic relationships

CAST adopts a **graph-based approach** where code is represented as:
- **Static Structure**: AST (Abstract Syntax Tree) extraction using Tree-sitter
- **Dynamic Relationships**: Call Graph construction with PageRank importance calculation
- **Execution Awareness**: Trajectory-based dynamic pruning and heatmap generation

### L1 - Panoramic Logic Layer (LogicLayer)

**Function**: Provides a global, high-level view of code architecture, enabling rapid understanding of agent composition and execution patterns.

**Core Components**:

```python
class LogicLayer:
    project_structure: Dict[str, Any]     # Project directory structure
    key_symbols: List[Symbol]             # Key symbol table (classes, functions)
    call_graph: Dict[str, List[str]]      # Call relationship graph
    dependency_graph: Dict[Path, Set[Path]] # Dependency relationship graph
    execution_heatmap: Dict[str, int]     # Execution frequency heatmap
    module_descriptions: Dict[Path, str]  # Module functional descriptions
    trajectory_mapping: Dict[str, Any]    # Execution path annotations
```

**Key Features**:

1. **Project Structure Visualization**: Displays the complete directory tree, showing file organization and module relationships
2. **Key Symbol Table**: Lists core classes (e.g., `AgentCore`, `MemoryManager`, `ToolExecutor`) and their primary methods
3. **Dynamic Execution Heatmap**: Leverages execution trajectory to highlight files and functions that were actually executed during task performance
4. **Call Relationship Graph**: Shows how modules interact (e.g., `AgentCore.run()` calls `ToolExecutor.execute()`)
5. **Trajectory-Based Pruning**: Automatically hides or collapses files that were not executed, dramatically reducing context size

**Example Output Format**:

```text
[Project Structure & Execution Path]
The agent consists of 5 files.

> main.py (Entry Point, EXECUTED)
  - class Agent:
    - run() (CALLED 1 time) ‚Üí calls planner.py/plan()
    
> planner.py (Logic Core, EXECUTED)
  - class Planner:
    - plan() (CALLED 1 time) ‚Üí calls tools.py/search()
    
> tools.py (Tool Library, EXECUTED)
  - search() (CALLED 2 times)
  
> memory.py (NOT EXECUTED - Pruned from context to save space)
> config.py (Configuration, Read-only)
```

**Use Cases**:
- Rapid architecture understanding for large codebases
- Identification of core modules and execution-critical components
- Analysis of inter-module dependencies and call patterns
- Problem localization through execution path analysis

### L2 - Interface Skeleton Layer (SkeletonLayer)

**Function**: Provides interface-level overview without implementation details, enabling precise code location and API understanding.

**Core Components**:

```python
class SkeletonLayer:
    file_skeletons: Dict[Path, str]           # File skeleton code (signatures only)
    symbol_signatures: Dict[str, str]         # Symbol signature mapping
    line_mappings: Dict[Path, Dict[int, int]] # Line number mapping (original ‚Üí skeleton)
    docstring_index: Dict[str, str]           # Docstring preservation
```

**Technical Implementation**:

1. **AST-Based Extraction**: Uses Tree-sitter or Python's `ast` module to parse source files
2. **Signature Preservation**: Retains all `import` statements, `class` definitions, and `def` declarations
3. **Docstring Retention**: Preserves all docstrings (critical for understanding design intent)
4. **Implementation Removal**: Replaces function bodies with `pass` or `...`, or retains only the first 5 lines
5. **Line Number Annotation**: Marks each line with its original line number (e.g., `(Line 105)`)

**Example Output Format**:

```python
# File: planner.py
(Line 1) import openai
(Line 3) class Planner:
(Line 4)     """Handles task decomposition."""
(Line 5)     def __init__(self, model_name):
(Line 6)         ...
(Line 8)     def plan(self, task_description: str) -> list:
(Line 9)         """
(Line 10)        Generates a list of steps based on description.
(Line 11)        """
(Line 12)        ... # Implementation hidden
```

**Key Features**:
- **Complete API Overview**: All function signatures, type annotations, and parameters visible
- **Design Intent Preservation**: Docstrings provide context for each component's purpose
- **Precise Location Mapping**: Line numbers enable exact code navigation
- **Minimal Token Consumption**: Typically 5-10% of original code size

**Use Cases**:
- Interface-level code understanding without implementation noise
- Rapid identification of function signatures and parameter types
- Design pattern and architecture analysis
- Precise problem localization before detailed code inspection

### L3 - Source Implementation Layer (ImplementationLayer)

**Function**: Provides complete source code implementation for precise code location and surgical modification.

**Core Components**:

```python
class ImplementationLayer:
    code_nodes: Dict[Path, CodeNode]  # Complete code nodes with full implementation
    symbol_references: Dict[str, List[Location]]  # Symbol reference locations
    modification_history: List[Patch]  # Code modification tracking
```

**Access Pattern**:

The Optimizer Agent accesses L3 **on-demand** through tool calls:

```python
# Agent identifies problem in L1/L2, then requests specific code
read_file(file="planner.py", start_line=12, end_line=50)
```

**Key Features**:
- **Complete Source Code**: Full implementation details for targeted code sections
- **Precise Symbol Location**: Exact file paths and line numbers for all symbols
- **Reference Relationship Analysis**: Tracks where symbols are defined, called, and modified
- **Modification Support**: Enables surgical code replacement at specific locations

**Use Cases**:
- Precise code modification after problem identification
- Implementation detail inspection for specific functions
- Code refactoring and optimization
- Bug fixing with minimal context overhead

### Trajectory-Based Optimization

A key innovation of the CAST framework is its integration of **execution trajectory** data to optimize context presentation:

#### Trajectory Mapping

Instead of treating trajectory as raw text logs, CAST:

1. **Parses Trajectory**: Extracts stack traces, executed function calls, and error locations
2. **Maps to Code Structure**: Associates trajectory events with specific code locations in L1/L2
3. **Visual Annotation**: Marks problematic areas directly on the code structure (e.g., `[ERROR: Timeout]` next to `tools.py`)
4. **Result**: The Optimizer Agent can immediately see problem locations without reading thousands of log lines

#### Dynamic Pruning

When an agent has 100 files but only 3 were executed:

1. **Execution Analysis**: Identifies which files/functions were actually called
2. **Context Pruning**: Hides or collapses unused files in L1/L2
3. **Token Reduction**: Achieves 70-90% reduction in context length
4. **Focus Enhancement**: Optimizer Agent concentrates on the "crime scene" code paths

### Workflow Integration

The three-tier architecture enables a **progressive refinement workflow**:

```
1. Preprocessing Phase:
   ‚îî‚îÄ> Scan codebase ‚Üí Generate L1 (panoramic map) + L2 (skeleton)

2. Trajectory Integration:
   ‚îî‚îÄ> Parse execution logs ‚Üí Map to code structure ‚Üí Annotate L1

3. Analysis Phase:
   ‚îî‚îÄ> Optimizer Agent reads L1 + L2 + Reward ‚Üí Identifies problem areas

4. Investigation Phase:
   ‚îî‚îÄ> Agent uses L2 signatures to locate specific functions/classes

5. Modification Phase:
   ‚îî‚îÄ> Agent calls read_file() to retrieve L3 code ‚Üí Generates fixes

6. Deployment Phase:
   ‚îî‚îÄ> Agent applies code patches ‚Üí Verifies changes
```

This workflow ensures that the Optimizer Agent only consumes detailed code (L3) for the specific areas requiring modification, dramatically improving efficiency while maintaining comprehensive understanding.

## üöÄ Quick Start

TODO...

## üîß Extension Development

### Adding a New Language Parser

```python
from aworld.experimental.cast.ast_parsers.base_parser import BaseParser

class JavaScriptParser(BaseParser):
    def __init__(self):
        super().__init__("javascript", {".js", ".jsx", ".ts", ".tsx"})

    def _get_default_query(self):
        return '''
        (function_declaration name: (identifier) @name) @definition.function
        (class_declaration name: (identifier) @name) @definition.class
        (method_definition key: (property_identifier) @name) @definition.method
        '''

    def _extract_symbols(self, captures):
        # Implement JavaScript-specific symbol extraction logic
        return symbols

# Register with framework
framework = ACast()
framework.register_parser("javascript", JavaScriptParser())
```

### Adding a New Searcher

```python
from aworld.experimental.cast.searchers.base_searcher import BaseSearcher

class DatabaseSearcher(BaseSearcher):
    def __init__(self, db_config):
        super().__init__(name="database")
        self.db_config = db_config

    def search(self, query, options=None):
        # Implement database search logic
        return search_results

# Register with search engine
from aworld.experimental.cast.searchers.search_engine import SearchEngine
search_engine = SearchEngine()
search_engine.register_tool(DatabaseSearcher(db_config))
```

## üéØ Use Cases

### ü§ñ Agent Self-Optimization
- **Code Analysis** ‚Üí **Problem Location** ‚Üí **Automatic Patching** ‚Üí **Verification & Deployment**
- Supports multi-round iterative optimization, continuously improving code quality

### üìñ Code Understanding
- **Architecture Analysis**: Quickly understand overall project structure
- **Documentation Generation**: Automatically generate API documentation and architecture diagrams
- **Onboarding**: Help new developers quickly familiarize themselves with code

### üîç Code Quality Analysis
- **Code Review**: Automatically identify potential issues and improvement suggestions
- **Refactoring Suggestions**: Provide refactoring solutions based on dependency analysis
- **Technical Debt**: Quantitatively assess code quality and maintenance costs

## üõ°Ô∏è Performance & Reliability

### Performance Optimization
- **Incremental Caching**: SQLite persistence, avoiding redundant analysis
- **Parallel Processing**: Multi-process parallel analysis of large codebases
- **Memory Optimization**: Layered loading, on-demand code content loading
- **Index Acceleration**: Intelligent sorting based on PageRank

### Reliability Assurance
- **Comprehensive Error Handling**: Gracefully handles parsing errors and exceptions
- **Detailed Logging**: Supports debugging and issue troubleshooting
- **Type Safety**: Complete type annotations and runtime checks
- **Unit Testing**: Comprehensive test coverage for core functionality

## üìà Technical Metrics

| Metric | Value | Description |
|--------|-------|-------------|
| Total Lines of Code | 10,000+ | Includes all modules and tools |
| Supported Languages | 2+ | Python, HTML (extensible) |
| Parsing Accuracy | >99% | Based on Tree-sitter engine |
| Analysis Speed | ~1000 files/s | Depends on hardware configuration |
| Cache Hit Rate | >95% | Incremental analysis scenarios |

## üìã Design Background & Problem Statement

### The Agent Self-Optimization Challenge

In modern AI development, there is an emerging need for **meta-optimization**: having one agent (the "Optimizer Agent") analyze and improve another agent (the "Target Agent"). This optimization process requires three critical inputs:

1. **Agent Codebase**: The complete source code implementation of the target agent, including model API calls, context parsing, tool implementations, configurations, and agent-specific logic
2. **Execution Trajectory**: Detailed logs of the agent's behavior during task execution, including function calls, state changes, and execution paths
3. **Task Reward**: Performance metrics and evaluation results that indicate the quality of agent performance

### Core Technical Challenge

The fundamental challenge lies in **efficient, concise, complete, and lossless representation** of the target agent's codebase to the Optimizer Agent's LLM. The core problem emerges when:

- **Scale Problem**: Target agents often consist of multiple files (e.g., 5+ Python files), each containing hundreds or thousands of lines of code
- **Context Limitation**: Excessive input length negatively impacts LLM analysis effectiveness and attention distribution
- **Trajectory Verbosity**: Execution logs can be extremely lengthy, further compounding the context length problem
- **Structural Complexity**: Agent codebases contain intricate relationships between components (modules, classes, functions) that must be preserved for accurate optimization

### The Critical Question

**How can we create a hierarchical index structure that enables:**

1. **Logical Overview**: High-level textual representation of agent composition and functionality, allowing the Optimizer Agent to understand the overall architecture at a glance
2. **Precise Localization**: Mapping logical components to specific file locations and line numbers, enabling targeted code navigation
3. **On-demand Detail Access**: Selective retrieval of source code only when needed, rather than loading the entire codebase upfront

The Optimizer Agent's LLM should initially consume only the high-level structure (L1) and interface information (L2). Through analysis of execution trajectories and identified issues, it can then precisely locate and retrieve specific code sections (L3) for targeted modifications‚Äîsignificantly reducing input token consumption while maintaining comprehensive understanding.

### Solution Requirements

The ideal architecture should enable a **progressive refinement workflow**:

1. **Initial Analysis Phase**: The Optimizer Agent analyzes the high-level structure (L1) combined with execution trajectory to identify potential problem areas
2. **Interface Investigation Phase**: Through interface signatures (L2), the agent locates specific functions, classes, and modules that require attention
3. **Surgical Modification Phase**: The agent retrieves specific implementation details (L3) only for the identified problem areas, performing precise code modifications

This **progressive refinement approach** dramatically reduces input token consumption (typically 70-90% reduction) while maintaining comprehensive code understanding‚Äîenabling precise surgical modifications rather than blind exploration of the entire codebase.

---

## üî¨ Industry SOTA Comparative Analysis

### Current Challenges in Agent Code Understanding

Traditional approaches for LLMs to understand and optimize code face critical limitations:

**Direct Context Injection**:
- **Limitations**: Token overflow, attention dispersion, inability to process large-scale projects
- **Impact**: Poor optimization quality for codebases exceeding 1000 lines

**RAG-based Code Segmentation**:
- **Critical Issue**: Loss of **structural logic relationships** between code components
- **Impact**: Cannot understand essential causality chains like "Function A calls Function B, which modifies global state C"
- **Result**: Fragmented code understanding leads to suboptimal optimization decisions

### Industry Leading Solutions Analysis

**Current SOTA Representatives**:

| Solution | Core Technology | Architecture Approach | Limitations |
|----------|----------------|----------------------|-------------|
| **Aider Repository Map** | Tree-sitter AST + PageRank | Symbol extraction ‚Üí Importance ranking ‚Üí Context selection | Limited dynamic execution awareness |
| **SWE-agent FileViewer** | Call Graph + Static Analysis | Multi-file dependency tracking ‚Üí Structured context | No trajectory-based optimization |
| **Code RAG Systems** | Vector embeddings + Retrieval | Semantic similarity ‚Üí Context retrieval | Loses structural relationships |

### CAST Framework Innovation

**Revolutionary Approach**: **"Dynamic Panoramic - Static Skeleton - On-demand Source"** three-tier architecture

**Core Design Philosophy**:

The CAST framework transforms code understanding from a **static text problem** into a **dynamic graph navigation problem**. By leveraging AST extraction, Call Graph construction, and execution trajectory analysis, CAST enables LLMs to progressively refine their understanding‚Äîstarting with high-level architecture and drilling down to specific implementation details only when necessary.

**Key Technical Innovations**:

1. **Static Structure + Dynamic Execution Integration**:
   - **AST Extraction**: Uses Tree-sitter to generate precise syntax trees for multiple languages
   - **Call Graph Construction**: Builds relationship graphs using NetworkX, tracking function calls, class inheritance, and module dependencies
   - **PageRank Analysis**: Calculates symbol importance based on graph centrality, not just call frequency
   - **Unique Advantage**: Integrates execution trajectory for dynamic pruning and heatmap generation

2. **Trajectory-Driven Context Optimization**:
   - **Trajectory Parsing**: Extracts executed functions, error locations, and execution paths from logs
   - **Dynamic Mapping**: Associates trajectory events with specific code locations in the hierarchical structure
   - **Intelligent Pruning**: Automatically hides unused files/functions, focusing attention on executed code paths
   - **Visual Annotation**: Marks problematic areas directly on code structure (e.g., error indicators, execution frequency)
   - **Result**: 70-90% reduction in context length while maintaining optimization accuracy

3. **Progressive Refinement Architecture**:
   - **L1 (Panoramic)**: Provides macro-level understanding with execution heatmaps
   - **L2 (Skeleton)**: Offers interface-level details with precise location mapping
   - **L3 (Implementation)**: Delivers complete source code on-demand for surgical modifications
   - **Innovation**: Combines static importance (PageRank) with dynamic execution frequency for optimal context selection

**Architectural Superiority**:

```
Traditional RAG:
  Code ‚Üí Fragments ‚Üí Vector Embeddings ‚Üí Semantic Search ‚Üí Context
  ‚ùå Loses structural relationships
  ‚ùå No execution awareness

Aider/SWE-agent:
  Code ‚Üí AST ‚Üí Static Symbol Map ‚Üí PageRank Ranking ‚Üí Context
  ‚úÖ Preserves structure
  ‚ùå No dynamic execution awareness

CAST Framework:
  Code ‚Üí AST + Call Graph + Trajectory ‚Üí Dynamic Pruning ‚Üí Progressive Refinement
  ‚úÖ Preserves structure
  ‚úÖ Execution-aware optimization
  ‚úÖ 70-90% context reduction
  ‚úÖ Surgical precision modifications
```

**Technical Implementation Details**:

- **Tree-sitter Integration**: Multi-language AST parsing with incremental update support
- **NetworkX Graph Analysis**: PageRank algorithm for importance calculation, shortest path analysis for dependency tracking
- **Trajectory Parser**: Stack trace extraction, function call tracking, error location mapping
- **Incremental Caching**: SQLite-based persistence for cross-session analysis reuse
- **Parallel Processing**: Multi-process analysis for large codebases

## üîó Technology Stack

### Core Technologies

- **Tree-sitter**: High-precision, incremental syntax parsing engine supporting multiple programming languages
- **NetworkX**: Graph analysis library for PageRank calculation, call graph construction, and dependency analysis
- **Ripgrep**: Ultra-fast text search engine for content-based code search
- **SQLite**: Lightweight embedded database for incremental analysis caching and cross-session persistence
- **Python 3.8+**: Modern Python with type annotations, dataclasses, and async/await support

### Recommended Tools and Libraries

Based on industry best practices and research into SOTA solutions:

- **PyCG (Python Call Graph)**: For generating static call graphs from Python codebases
- **Aider Repository Map**: Reference implementation for PageRank-based symbol importance ranking
- **SWE-agent FileViewer**: Inspiration for structured multi-file dependency tracking

### Implementation Workflow

1. **Preprocessing Stage**: 
   - Tree-sitter parses codebase ‚Üí Generates AST for all files
   - NetworkX constructs call graph ‚Üí Calculates PageRank importance
   - Generates L1 (panoramic map) and L2 (skeleton layer)

2. **Trajectory Integration Stage**:
   - Parses execution trajectory ‚Üí Extracts function calls and error locations
   - Maps trajectory events to code structure ‚Üí Generates execution heatmap
   - Annotates L1 with execution information ‚Üí Prunes unused code paths

3. **Optimization Stage**:
   - Optimizer Agent receives L1 + L2 + Reward
   - Analyzes structure and identifies problem areas
   - Requests L3 code on-demand for targeted modifications
   - Applies code patches and verifies changes

---

*Based on Tree-sitter and layered architecture, enabling agents to understand and optimize code more precisely.*
