"""
AWorld AST Framework - æ ¸å¿ƒæ¥å£
===============================

å®šä¹‰ASTåˆ†ææ¡†æ¶çš„æ ¸å¿ƒæŠ½è±¡æ¥å£å’Œä¸»è¦ç»„ä»¶ã€‚
"""

import json
import logging
import math
import tempfile
import traceback
from difflib import SequenceMatcher
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple, TYPE_CHECKING

from .analyzer import CodeAnalyzer
from .models import (
    Symbol, CodeNode, RepositoryMap
)
from ...logs.util import logger

# å¯¼å…¥BaseParserï¼ˆä¸ä¼šé€ æˆå¾ªç¯ä¾èµ–ï¼Œå› ä¸ºparsers.base_parserä¸å¯¼å…¥coreï¼‰
if TYPE_CHECKING:
    from .parsers.base_parser import BaseParser
else:
    from .parsers.base_parser import BaseParser


class ValidationError(Exception):
    """ä¸Šä¸‹æ–‡éªŒè¯å¤±è´¥å¼‚å¸¸"""
    pass


class RepositoryAnalyzer:
    """ä»“åº“åˆ†æå™¨ä¸»ç±»"""

    def __init__(self, code_analyzer: CodeAnalyzer):
        self.code_analyzer = code_analyzer
        logger = logging.getLogger(f"{__name__}.RepositoryAnalyzer")

    def analyze(self,
                root_path: Path,
                file_patterns: Optional[List[str]] = None,
                ignore_patterns: Optional[List[str]] = None) -> RepositoryMap:
        """
        æ‰§è¡Œå®Œæ•´çš„ä»“åº“åˆ†æ

        Args:
            root_path: ä»“åº“æ ¹ç›®å½•
            file_patterns: æ–‡ä»¶åŒ…å«æ¨¡å¼
            ignore_patterns: æ–‡ä»¶å¿½ç•¥æ¨¡å¼

        Returns:
            å®Œæ•´çš„ä»“åº“æ˜ å°„
        """
        logger.info(f"å¼€å§‹åˆ†æä»“åº“: {root_path}")

        # æ‰§è¡Œä»£ç åˆ†æ
        repo_map = self.code_analyzer.analyze_repository(
            root_path, file_patterns, ignore_patterns
        )

        logger.info("ä»“åº“åˆ†æå®Œæˆ")
        return repo_map

    def recall(self,
               repo_map: RepositoryMap,
               user_query: str = "",
               max_tokens: int = 8000,
               context_layers: Optional[List[str]] = None) -> str:
        """
        å¤šå±‚æ¬¡ä»£ç ä¸Šä¸‹æ–‡å¬å›
        å‚è€ƒ aider çš„åˆ†å±‚ä¸Šä¸‹æ–‡ç®¡ç†è®¾è®¡

        Args:
            repo_map: ä»“åº“æ˜ å°„
            user_query: ç”¨æˆ·æŸ¥è¯¢
            max_tokens: æœ€å¤§tokenæ•°é‡
            context_layers: æŒ‡å®šè¦åŒ…å«çš„ä¸Šä¸‹æ–‡å±‚æ¬¡

        Returns:
            æ ¼å¼åŒ–çš„åˆ†å±‚ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        logger.info(f"ğŸš€ å¼€å§‹å¤šå±‚æ¬¡ä»£ç ä¸Šä¸‹æ–‡å¬å›")
        logger.info(f"ğŸ“ ç”¨æˆ·æŸ¥è¯¢: '{user_query}'")
        logger.info(f"ğŸ¯ æœ€å¤§tokenæ•°: {max_tokens}")
        logger.info(f"ğŸ“‹ æŒ‡å®šå±‚æ¬¡: {context_layers}")

        # æå–ç”¨æˆ·æŸ¥è¯¢ä¸­çš„å…³é”®è¯
        user_mentions = [user_query]
        logger.info(f"ğŸ” æå–çš„æŸ¥è¯¢å…³é”®è¯: {user_mentions}")

        # æ„å»ºåˆ†å±‚ä¸Šä¸‹æ–‡
        logger.info(f"ğŸ—ï¸ å¼€å§‹æ„å»ºåˆ†å±‚ä¸Šä¸‹æ–‡...")
        logger.info('repo_map: ', repo_map)
        layered_context = self._build_layered_context(
            repo_map, user_mentions, context_layers
        )
        logger.info(f"ğŸ“Š åˆ†å±‚ä¸Šä¸‹æ–‡æ„å»ºç»“æœ: {len(layered_context)} ä¸ªå±‚æ¬¡")
        return layered_context
        # ä¼˜åŒ–tokené¢„ç®—
        # logger.info(f"âš–ï¸ å¼€å§‹ä¼˜åŒ–tokené¢„ç®—...")
        # final_context = self._optimize_token_budget(layered_context, max_tokens)
        # logger.info(f"âœ… ä¸Šä¸‹æ–‡å¬å›å®Œæˆï¼Œæœ€ç»ˆé•¿åº¦: {len(final_context)} å­—ç¬¦")

        # return final_context

    def _build_layered_context(self,
                               repo_map: RepositoryMap,
                               user_mentions: List[str],
                               context_layers: List[str]) -> Dict[str, str]:
        """æ„å»ºåˆ†å±‚ä¸Šä¸‹æ–‡å†…å®¹"""

        logger.info(f"ğŸ—ï¸ å¼€å§‹æ„å»ºåˆ†å±‚ä¸Šä¸‹æ–‡")
        logger.info(f"ğŸ“‹ è¯·æ±‚çš„å±‚æ¬¡: {context_layers}")
        logger.info(f"ğŸ” ç”¨æˆ·æŸ¥è¯¢: {user_mentions}")

        layer_generators = {
            "skeleton": self._generate_skeleton_context,
            "implementation": self._generate_implementation_context,
        }

        layered_content = {}
        for layer_idx, layer_name in enumerate(context_layers):
            logger.info(f"ğŸ”„ å¤„ç†å±‚æ¬¡ [{layer_idx+1}/{len(context_layers)}]: {layer_name}")

            if layer_name in layer_generators:
                try:
                    logger.info(f"  âš™ï¸ è°ƒç”¨ç”Ÿæˆå™¨: {layer_generators[layer_name].__name__}")
                    content = layer_generators[layer_name](repo_map, user_mentions)
                    logger.info(f"  ğŸ“ ç”Ÿæˆå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")

                    # æ˜¾ç¤ºå†…å®¹é¢„è§ˆï¼ˆå‰200å­—ç¬¦ï¼‰
                    preview = content[:200].replace('\n', '\\n')
                    logger.debug(f"  ğŸ‘€ å†…å®¹é¢„è§ˆ: {preview}...")

                    if content.strip():  # åªæ·»åŠ éç©ºå†…å®¹
                        layered_content[layer_name] = content
                        logger.info(f"  âœ… å±‚æ¬¡ {layer_name} å†…å®¹å·²æ·»åŠ ")
                    else:
                        logger.warning(f"  âš ï¸ å±‚æ¬¡ {layer_name} ç”Ÿæˆçš„å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡")

                except Exception as e:
                    logger.error(f"  âŒ ç”Ÿæˆ{layer_name}å±‚å†…å®¹å¤±è´¥: {e}")
                    import traceback
                    logger.debug(f"  ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            else:
                logger.warning(f"  âš ï¸ æœªçŸ¥çš„å±‚æ¬¡ç±»å‹: {layer_name}")
                logger.info(f"  ğŸ“ å¯ç”¨çš„å±‚æ¬¡ç±»å‹: {list(layer_generators.keys())}")

        logger.info(f"ğŸ åˆ†å±‚ä¸Šä¸‹æ–‡æ„å»ºå®Œæˆ")
        logger.info(f"ğŸ“Š æˆåŠŸç”Ÿæˆçš„å±‚æ¬¡: {list(layered_content.keys())}")
        total_length = sum(len(content) for content in layered_content.values())
        logger.info(f"ğŸ“ æ€»å†…å®¹é•¿åº¦: {total_length} å­—ç¬¦")

        return layered_content

    def _generate_system_context(self, repo_map: RepositoryMap, user_mentions: List[str]) -> str:
        """ç”Ÿæˆç³»ç»Ÿæ¦‚è§ˆå±‚ä¸Šä¸‹æ–‡"""
        lines = ["# ç³»ç»Ÿæ¦‚è§ˆ"]

        # é¡¹ç›®åŸºæœ¬ä¿¡æ¯
        lines.append(f"\n## é¡¹ç›®ç»“æ„")
        lines.append(f"æ€»æ–‡ä»¶æ•°: {len(repo_map.code_nodes)}")

        # è®¡ç®—æ€»ä»£ç è¡Œæ•°å’Œç¬¦å·æ•°
        total_lines = 0
        total_symbols = sum(len(node.symbols) for node in repo_map.code_nodes.values())

        # å®‰å…¨è·å–ä»£ç è¡Œæ•°
        for file_path, node in repo_map.code_nodes.items():
            if hasattr(node, 'line_count'):
                total_lines += node.line_count
            else:
                # å¤‡ç”¨æ–¹æ¡ˆï¼šå°è¯•è¯»å–æ–‡ä»¶è®¡ç®—è¡Œæ•°
                try:
                    content = file_path.read_text(encoding='utf-8', errors='ignore')
                    total_lines += len(content.split('\n'))
                except:
                    total_lines += len(node.symbols) * 3  # ä¼°ç®—

        lines.append(f"ä»£ç è¡Œæ•°: {total_lines}")
        lines.append(f"ç¬¦å·æ€»æ•°: {total_symbols}")

        # æŠ€æœ¯æ ˆä¿¡æ¯
        file_types = {}
        for file_path in repo_map.code_nodes.keys():
            ext = file_path.suffix.lower()
            file_types[ext] = file_types.get(ext, 0) + 1

        if file_types:
            lines.append("\n### æ–‡ä»¶ç±»å‹åˆ†å¸ƒ")
            for ext, count in sorted(file_types.items(), key=lambda x: x[1], reverse=True)[:5]:
                lines.append(f"- {ext or 'æ— æ‰©å±•å'}: {count}ä¸ªæ–‡ä»¶")

        # æåŠçš„å…³é”®ç»„ä»¶
        if user_mentions:
            lines.append(f"\n### æŸ¥è¯¢å…³æ³¨ç‚¹")
            lines.append(f"æåŠæ ‡è¯†ç¬¦: {', '.join(user_mentions[:10])}")

        return '\n'.join(lines) + '\n'

    def _generate_structure_context(self, repo_map: RepositoryMap, user_mentions: List[str]) -> str:
        """ç”Ÿæˆé¡¹ç›®ç»“æ„å±‚ä¸Šä¸‹æ–‡"""
        lines = ["# é¡¹ç›®ç»“æ„"]

        # ä¸»è¦ç›®å½•å’Œæ–‡ä»¶
        lines.append("\n## æ ¸å¿ƒæ–‡ä»¶")
        sorted_files = sorted(repo_map.code_nodes.keys(), key=lambda x: len(repo_map.code_nodes[x].symbols),
                              reverse=True)

        for i, file_path in enumerate(sorted_files[:10], 1):
            node = repo_map.code_nodes[file_path]
            lines.append(f"{i:2d}. {file_path.name} - {len(node.symbols)}ä¸ªç¬¦å·")

        # æ¨¡å—å…³ç³»
        if hasattr(repo_map.logic_layer, 'import_graph') and repo_map.logic_layer.import_graph:
            lines.append("\n## ä¸»è¦æ¨¡å—ä¾èµ–")
            for i, (module, imports) in enumerate(list(repo_map.logic_layer.import_graph.items())[:8], 1):
                imports_str = ', '.join(list(imports)[:3])
                lines.append(f"{i:2d}. {module} â†’ {imports_str}")

        return '\n'.join(lines) + '\n'

    def _generate_symbols_context(self, repo_map: RepositoryMap, user_mentions: List[str]) -> str:
        """ç”Ÿæˆé‡è¦ç¬¦å·å±‚ä¸Šä¸‹æ–‡"""
        lines = ["# é‡è¦ç¬¦å·"]

        # è·å–ç›¸å…³ç¬¦å·
        relevant_symbols = self.get_relevant_symbols(repo_map, user_mentions, max_symbols=15)

        if relevant_symbols:
            lines.append("\n## ç›¸å…³ç¬¦å·")
            for i, (symbol, score) in enumerate(relevant_symbols[:15], 1):
                lines.append(
                    f"{i:2d}. {symbol.name} ({symbol.symbol_type.value}) - {symbol.file_path.name}:{symbol.line_number}")
                if symbol.signature:
                    lines.append(f"    {symbol.signature}")
        else:
            lines.append("\n## æ‰€æœ‰ç¬¦å· (æŒ‰æ–‡ä»¶åˆ†ç»„)")
            for file_path, node in list(repo_map.code_nodes.items())[:5]:
                if node.symbols:
                    lines.append(f"\n### {file_path.name}")
                    for symbol in node.symbols[:5]:
                        lines.append(f"- {symbol.name} ({symbol.symbol_type.value})")

        return '\n'.join(lines) + '\n'

    def _generate_skeleton_context(self, repo_map: RepositoryMap, user_mentions: List[str]) -> str:
        """ç”Ÿæˆä»£ç éª¨æ¶å±‚ä¸Šä¸‹æ–‡"""
        lines = ["# ä»£ç éª¨æ¶"]

        # ä¸ºæœ€ç›¸å…³çš„æ–‡ä»¶ç”Ÿæˆéª¨æ¶
        relevant_files = []

        # åŸºäºç”¨æˆ·æåŠç­›é€‰ç›¸å…³æ–‡ä»¶ï¼ˆä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼‰
        import re
        if user_mentions:
            for file_path, node in repo_map.code_nodes.items():
                score = 0
                for mention in user_mentions:
                    try:
                        if re.search(mention, file_path.name, re.IGNORECASE):
                            score += 10
                        for symbol in node.symbols:
                            if re.search(mention, symbol.name, re.IGNORECASE):
                                score += 5
                    except re.error:
                        # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼æ— æ•ˆï¼Œå›é€€åˆ°ç®€å•å­—ç¬¦ä¸²åŒ¹é…
                        mention_lower = mention.lower()
                        if mention_lower in file_path.name.lower():
                            score += 10
                        for symbol in node.symbols:
                            if mention_lower in symbol.name.lower():
                                score += 5
                if score > 0:
                    relevant_files.append((file_path, score))

        if not relevant_files:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®ç›¸å…³çš„æ–‡ä»¶ï¼Œé€‰æ‹©ç¬¦å·æœ€å¤šçš„å‰3ä¸ªæ–‡ä»¶
            relevant_files = [(fp, len(node.symbols)) for fp, node in repo_map.code_nodes.items()]

        relevant_files.sort(key=lambda x: x[1], reverse=True)

        for i, (file_path, score) in enumerate(relevant_files[:3], 1):
            lines.append(f"\n## {file_path.name}")

            # ç®€å•çš„éª¨æ¶ç”Ÿæˆ
            node = repo_map.code_nodes[file_path]
            for symbol in node.symbols[:10]:
                lines.append(f"  {symbol.symbol_type.value} {symbol.name}")
                if symbol.signature:
                    lines.append(f"    {symbol.signature}")

        return '\n'.join(lines) + '\n'

    def _generate_implementation_context(self, repo_map: RepositoryMap, user_mentions: List[str]) -> str:
        """
        ç”Ÿæˆè¯¦ç»†å®ç°å±‚ä¸Šä¸‹æ–‡ï¼ŒåŸºäºæ­£åˆ™è¡¨è¾¾å¼ä»ç¬¦å·contentä¸­åŒ¹é…

        Args:
            repo_map: ä»“åº“æ˜ å°„
            user_mentions: ç”¨æˆ·æåŠçš„æ­£åˆ™è¡¨è¾¾å¼åˆ—è¡¨

        Returns:
            æ ¼å¼åŒ–çš„å®ç°å±‚ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        import re
        lines = ["# å…³é”®å®ç°"]

        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•æ—¥å¿—
        logger.info(f"ğŸ” å¼€å§‹ç”Ÿæˆå®ç°å±‚ä¸Šä¸‹æ–‡")
        logger.info(f"ğŸ“ ç”¨æˆ·æŸ¥è¯¢æ¨¡å¼: {user_mentions}")

        # ä»implementation_layerçš„code_nodesä¸­æœç´¢åŒ¹é…çš„ç¬¦å·
        matches = []

        # æ£€æŸ¥ repo_map ç»“æ„
        logger.info(f"ğŸ“Š ä»“åº“æ˜ å°„ç»“æ„æ£€æŸ¥:")
        logger.info(f"  - repo_map æ˜¯å¦å­˜åœ¨: {repo_map is not None}")
        logger.info(f"  - implementation_layer æ˜¯å¦å­˜åœ¨: {hasattr(repo_map, 'implementation_layer') and repo_map.implementation_layer is not None}")

        if not repo_map.implementation_layer or not repo_map.implementation_layer.code_nodes:
            logger.warning(f"âš ï¸ å®ç°å±‚æ•°æ®ç¼ºå¤±:")
            logger.warning(f"  - implementation_layer: {repo_map.implementation_layer}")
            if repo_map.implementation_layer:
                logger.warning(f"  - code_nodes: {repo_map.implementation_layer.code_nodes}")
            lines.append("\næœªæ‰¾åˆ°å®ç°å±‚ä»£ç èŠ‚ç‚¹")
            return '\n'.join(lines) + '\n'

        # ç»Ÿè®¡å®ç°å±‚æ•°æ®
        total_files = len(repo_map.implementation_layer.code_nodes)
        total_symbols = sum(len(node.symbols) for node in repo_map.implementation_layer.code_nodes.values())
        symbols_with_content = 0

        logger.info(f"ğŸ“ˆ å®ç°å±‚ç»Ÿè®¡:")
        logger.info(f"  - æ€»æ–‡ä»¶æ•°: {total_files}")
        logger.info(f"  - æ€»ç¬¦å·æ•°: {total_symbols}")

        # éå†æ‰€æœ‰code_nodesä¸­çš„ç¬¦å·
        for file_idx, (file_path, code_node) in enumerate(repo_map.implementation_layer.code_nodes.items()):
            logger.info(f"ğŸ” å¤„ç†æ–‡ä»¶ [{file_idx+1}/{total_files}]: {file_path}")
            logger.info(f"  - ç¬¦å·æ•°é‡: {len(code_node.symbols)}")

            for symbol_idx, symbol in enumerate(code_node.symbols):
                logger.info(f'symbol: {symbol}')
                if not symbol.content:  # è·³è¿‡æ²¡æœ‰contentçš„ç¬¦å·
                    logger.info(f"  - è·³è¿‡ç¬¦å· [{symbol_idx+1}] {symbol.name}: æ— å†…å®¹")
                    continue

                symbols_with_content += 1
                logger.info(f"  - æ£€æŸ¥ç¬¦å· [{symbol_idx+1}] {symbol.name} ({symbol.symbol_type.value})")
                logger.info(f"    å†…å®¹é•¿åº¦: {len(symbol.content)} å­—ç¬¦")

                symbol_score = 0.0
                match_details = []

                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åœ¨symbol.contentä¸­æœç´¢
                for pattern_idx, mention_pattern in enumerate(user_mentions):
                    logger.info(f"    ğŸ” åº”ç”¨æ¨¡å¼ [{pattern_idx+1}]: '{mention_pattern}'")

                    try:
                        # åœ¨ç¬¦å·å†…å®¹ä¸­æœç´¢
                        content_matches = re.finditer(mention_pattern, symbol.content, re.IGNORECASE | re.MULTILINE)
                        content_match_count = len(list(content_matches))
                        logger.info(f"      å†…å®¹åŒ¹é…æ¬¡æ•°: {content_match_count}")

                        if content_match_count > 0:
                            symbol_score += content_match_count * 15.0  # å†…å®¹åŒ¹é…ç»™é«˜åˆ†
                            match_details.append(f"å†…å®¹åŒ¹é…: {content_match_count}æ¬¡")
                            logger.info(f"      âœ… å†…å®¹åŒ¹é… +{content_match_count * 15.0} åˆ†")

                        # åœ¨ç¬¦å·ç­¾åä¸­æœç´¢ï¼ˆå¦‚æœæœ‰ï¼‰
                        if symbol.signature:
                            signature_match = re.search(mention_pattern, symbol.signature, re.IGNORECASE)
                            logger.info(f"      ç­¾ååŒ¹é…: {signature_match is not None}")
                            if signature_match:
                                symbol_score += 12.0
                                match_details.append("ç­¾ååŒ¹é…")
                                logger.info(f"      âœ… ç­¾ååŒ¹é… +12.0 åˆ†")

                        # åœ¨æ–‡æ¡£å­—ç¬¦ä¸²ä¸­æœç´¢ï¼ˆå¦‚æœæœ‰ï¼‰
                        if symbol.docstring:
                            docstring_match = re.search(mention_pattern, symbol.docstring, re.IGNORECASE)
                            logger.info(f"      æ–‡æ¡£åŒ¹é…: {docstring_match is not None}")
                            if docstring_match:
                                symbol_score += 8.0
                                match_details.append("æ–‡æ¡£åŒ¹é…")
                                logger.info(f"      âœ… æ–‡æ¡£åŒ¹é… +8.0 åˆ†")

                        # åœ¨ç¬¦å·åç§°ä¸­æœç´¢
                        name_match = re.search(mention_pattern, symbol.name, re.IGNORECASE)
                        logger.info(f"      åç§°åŒ¹é…: {name_match is not None}")
                        if name_match:
                            symbol_score += 5.0
                            match_details.append("åç§°åŒ¹é…")
                            logger.info(f"      âœ… åç§°åŒ¹é… +5.0 åˆ†")

                    except re.error as e:
                        # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼æ— æ•ˆï¼Œè®°å½•é”™è¯¯å¹¶è·³è¿‡
                        logger.warning(f"âŒ æ— æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼ '{mention_pattern}': {e}")
                        continue

                # å¦‚æœæœ‰åŒ¹é…ï¼Œæ·»åŠ åˆ°ç»“æœä¸­
                if symbol_score > 0:
                    matches.append((symbol, symbol_score, match_details))
                    logger.info(f"ğŸ¯ æ‰¾åˆ°åŒ¹é…ç¬¦å·: {symbol.name} (åˆ†æ•°: {symbol_score:.1f}, è¯¦æƒ…: {match_details})")

        # ç»Ÿè®¡æœ€ç»ˆç»“æœ
        logger.info(f"ğŸ“Š æœç´¢å®Œæˆç»Ÿè®¡:")
        logger.info(f"  - æœ‰å†…å®¹çš„ç¬¦å·æ•°: {symbols_with_content}")
        logger.info(f"  - åŒ¹é…çš„ç¬¦å·æ•°: {len(matches)}")

        # æŒ‰ç›¸å…³æ€§åˆ†æ•°æ’åº
        matches.sort(key=lambda x: x[1], reverse=True)
        logger.info(f"ğŸ† æ’åºåçš„å‰5ä¸ªåŒ¹é…:")
        for i, (symbol, score, details) in enumerate(matches[:5]):
            logger.info(f"  {i+1}. {symbol.name} - {score:.1f}åˆ† ({', '.join(details)})")

        # ç”Ÿæˆä¸Šä¸‹æ–‡å†…å®¹
        if matches:
            logger.info(f"ğŸ“ ç”Ÿæˆä¸Šä¸‹æ–‡å†…å®¹ï¼Œæ˜¾ç¤ºå‰8ä¸ªåŒ¹é…ç»“æœ")
            for symbol, score, match_details in matches[:8]:  # æœ€å¤šæ˜¾ç¤º8ä¸ªåŒ¹é…ç»“æœ
                lines.append(f"\n## {symbol.name} ({symbol.symbol_type.value})")
                lines.append(f"æ–‡ä»¶: {symbol.file_path}")
                lines.append(f"è¡Œå·: {symbol.line_number}-{symbol.end_line}")
                lines.append(f"åŒ¹é…åˆ†æ•°: {score:.1f}")
                lines.append(f"åŒ¹é…è¯¦æƒ…: {', '.join(match_details)}")

                if symbol.signature:
                    lines.append(f"\nç­¾å:")
                    lines.append(symbol.signature)

                if symbol.docstring:
                    lines.append(f"\næ–‡æ¡£:")
                    lines.append(symbol.docstring)

                # æ˜¾ç¤ºç¬¦å·çš„å®Œæ•´å†…å®¹ï¼ˆå¸¦è¡Œå·ï¼‰
                source_code = self._get_symbol_source_code(repo_map, symbol)
                if source_code:
                    lines.append(f"\næºç :")
                    lines.append("```")
                    lines.append(source_code)
                    lines.append("```")
        else:
            logger.warning(f"âš ï¸ æœªæ‰¾åˆ°ä»»ä½•åŒ¹é…çš„å®ç°ä»£ç ")
            lines.append("\næœªæ‰¾åˆ°ä¸æŸ¥è¯¢æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…çš„å®ç°ä»£ç ")

        result = '\n'.join(lines) + '\n'
        logger.info(f"âœ… å®ç°å±‚ä¸Šä¸‹æ–‡ç”Ÿæˆå®Œæˆï¼Œæ€»é•¿åº¦: {len(result)} å­—ç¬¦")
        return result

    def _get_symbol_source_code(self, repo_map: RepositoryMap, symbol: Symbol) -> Optional[str]:
        """è·å–ç¬¦å·çš„æºç å†…å®¹ï¼Œæ¯ä¸€è¡Œå‰é¢æ·»åŠ è¡Œå·"""
        try:
            # ä¼˜å…ˆä½¿ç”¨ Symbol å¯¹è±¡ä¸­çš„ content å­—æ®µ
            if hasattr(symbol, 'content') and symbol.content:
                # å¦‚æœ Symbol å·²åŒ…å«ä»£ç å†…å®¹ï¼Œç›´æ¥ä½¿ç”¨å¹¶æ·»åŠ è¡Œå·
                content_lines = symbol.content.split('\n')
                numbered_lines = []
                for i, line in enumerate(content_lines):
                    line_number = symbol.line_number + i  # ä»ç¬¦å·èµ·å§‹è¡Œå·å¼€å§‹è®¡ç®—
                    numbered_lines.append(f"{line_number}â†’{line}")

                return '\n'.join(numbered_lines)

            # å›é€€æ–¹æ¡ˆï¼šç›´æ¥ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–ï¼ˆæ–°çš„ImplementationLayerä¸å†ç¼“å­˜file_contentsï¼‰
            file_path = symbol.file_path
            if file_path.exists():
                with open(file_path, 'r', encoding='utf-8') as f:
                    file_content = f.read()

                # æå–ç¬¦å·å¯¹åº”çš„æºç è¡Œ
                lines = file_content.split('\n')
                start_idx = max(0, symbol.line_number - 1)  # è½¬æ¢ä¸º0-basedç´¢å¼•
                end_idx = min(len(lines), symbol.end_line) if symbol.end_line > 0 else start_idx + 1

                # ä¸ºæ¯ä¸€è¡Œæ·»åŠ è¡Œå·å‰ç¼€
                numbered_lines = []
                for i, line in enumerate(lines[start_idx:end_idx]):
                    line_number = start_idx + i + 1  # è½¬æ¢å›1-basedè¡Œå·
                    # ç›´æ¥æ‹¼æ¥ï¼Œä¸æ·»åŠ é¢å¤–çš„æ ¼å¼åŒ–ç©ºé—´
                    numbered_lines.append(f"{line_number}â†’{line}")

                source_code = '\n'.join(numbered_lines)
                return source_code.strip() if source_code else None
            else:
                return None

        except Exception as e:
            logger.warning(f"è·å–ç¬¦å·æºç å¤±è´¥ {symbol.name}: {e}")
            return None

    def _generate_references_context(self, repo_map: RepositoryMap, user_mentions: List[str]) -> str:
        """ç”Ÿæˆå¼•ç”¨å…³ç³»å±‚ä¸Šä¸‹æ–‡"""
        lines = ["# è°ƒç”¨å…³ç³»"]

        if repo_map.logic_layer.call_graph:
            lines.append("\n## ä¸»è¦è°ƒç”¨å…³ç³»")
            # æ˜¾ç¤ºå‰10ä¸ªæœ€é‡è¦çš„è°ƒç”¨å…³ç³»
            for i, (caller, callees) in enumerate(list(repo_map.logic_layer.call_graph.items())[:10], 1):
                callees_str = ', '.join(list(callees)[:5])  # é™åˆ¶æ˜¾ç¤ºçš„è°ƒç”¨ç›®æ ‡
                lines.append(f"{i:2d}. {caller} â†’ {callees_str}")

        return '\n'.join(lines) + '\n'

    def _generate_trajectory_context(self, repo_map: RepositoryMap, user_mentions: List[str]) -> str:
        """ç”Ÿæˆæ‰§è¡Œè½¨è¿¹å±‚ä¸Šä¸‹æ–‡ï¼ˆå·²ç¦ç”¨ï¼‰"""
        return ""  # è½¨è¿¹åŠŸèƒ½å·²ç§»é™¤

    def _optimize_token_budget(self, layered_content: Dict[str, str], max_tokens: int) -> str:
        """
        åŠ¨æ€ä¼˜åŒ–tokené¢„ç®—åˆ†é…
        å‚è€ƒ aider çš„äºŒåˆ†æœç´¢ç®—æ³•
        """
        # å±‚æ¬¡ä¼˜å…ˆçº§ (æ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜)
        layer_priorities = {
            "system": 1,
            "structure": 2,
            "symbols": 3,
            "skeleton": 4,
            "references": 5,
            "implementation": 6,
            "trajectory": 7
        }

        # æŒ‰ä¼˜å…ˆçº§æ’åºå±‚æ¬¡
        sorted_layers = sorted(
            layered_content.items(),
            key=lambda x: layer_priorities.get(x[0], 999)
        )

        # ä¼°ç®—tokenæ•°é‡çš„ç®€å•æ–¹æ³• (1 token â‰ˆ 4 characters)
        def estimate_tokens(text: str) -> int:
            return len(text) // 4

        # ç´¯ç§¯æ„å»ºä¸Šä¸‹æ–‡ï¼Œç¡®ä¿ä¸è¶…è¿‡tokené™åˆ¶
        final_sections = []
        current_tokens = 0

        for layer_name, content in sorted_layers:
            content_tokens = estimate_tokens(content)

            if current_tokens + content_tokens <= max_tokens:
                final_sections.append(content)
                current_tokens += content_tokens
            else:
                # å¦‚æœæ˜¯é«˜ä¼˜å…ˆçº§å±‚æ¬¡ï¼Œå°è¯•æˆªæ–­è€Œä¸æ˜¯å®Œå…¨ä¸¢å¼ƒ
                if layer_priorities.get(layer_name, 999) <= 3:
                    remaining_tokens = max_tokens - current_tokens
                    if remaining_tokens > 100:  # è‡³å°‘ä¿ç•™100ä¸ªtokençš„å†…å®¹
                        truncated_content = content[:remaining_tokens * 4]
                        final_sections.append(truncated_content + "\n...(å†…å®¹è¢«æˆªæ–­)")
                        break
                else:
                    break

        return '\n'.join(final_sections)

    def get_relevant_symbols(self,
                             repo_map: RepositoryMap,
                             user_mentions: List[str],
                             max_symbols: int = 50) -> List[Tuple[Symbol, float]]:
        """
        æ ¹æ®ç”¨æˆ·æåŠçš„æ ‡è¯†ç¬¦ï¼Œè·å–ç›¸å…³çš„ç¬¦å·å¹¶æŒ‰ç›¸å…³æ€§æ’åº
        å‚è€ƒ aider çš„ç¬¦å·æ’åç®—æ³•
        """
        if not user_mentions:
            return []

        symbol_scores = []

        # éå†æ‰€æœ‰æ–‡ä»¶ä¸­çš„ç¬¦å·
        for file_path, code_node in repo_map.code_nodes.items():
            for symbol in code_node.symbols:
                score = self._calculate_symbol_relevance(symbol, user_mentions, file_path, repo_map)
                if score > 0:
                    symbol_scores.append((symbol, score))

        # æŒ‰å¾—åˆ†æ’åº
        ranked_symbols = sorted(symbol_scores, key=lambda x: x[1], reverse=True)
        print('ranked_symbols: ', ranked_symbols)
        return ranked_symbols[:max_symbols]

    def _calculate_symbol_relevance(self,
                                    symbol: Symbol,
                                    user_mentions: List[str],
                                    file_path: Path,
                                    repo_map: RepositoryMap) -> float:
        """
        è®¡ç®—ç¬¦å·ä¸ç”¨æˆ·æŸ¥è¯¢çš„ç›¸å…³æ€§å¾—åˆ†
        å‚è€ƒ aider çš„è¯„åˆ†ç®—æ³•
        """
        score = 0.0
        symbol_name = symbol.name.lower()

        # 1. ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼šquery ä½œä¸ºæ­£åˆ™åŒ¹é…å­—ç¬¦ä¸²
        import re
        for mention in user_mentions:
            try:
                if re.search(mention, symbol_name, re.IGNORECASE):
                    score += 10.0  # æ­£åˆ™åŒ¹é…
            except re.error:
                # å¦‚æœæ­£åˆ™è¡¨è¾¾å¼æ— æ•ˆï¼Œå›é€€åˆ°ç®€å•å­—ç¬¦ä¸²åŒ¹é…
                if mention.lower() in symbol_name:
                    score += 5.0

        # 2. å‘½åé£æ ¼åŠ æƒï¼šç»“æ„åŒ–å‘½åé€šå¸¸æ›´é‡è¦
        if self._has_structured_naming(symbol.name):
            score *= 1.5

        # 3. ç¬¦å·ç±»å‹åŠ æƒï¼šæŸäº›ç±»å‹çš„ç¬¦å·æ›´é‡è¦
        type_multipliers = {
            'CLASS': 2.0,  # ç±»é€šå¸¸æ˜¯é‡è¦çš„å…¥å£ç‚¹
            'FUNCTION': 1.5,  # å‡½æ•°æ˜¯ä¸»è¦é€»è¾‘
            'METHOD': 1.2,  # æ–¹æ³•
            'CONSTANT': 1.1,  # å¸¸é‡
            'VARIABLE': 1.0  # å˜é‡
        }
        score *= type_multipliers.get(symbol.symbol_type.value, 1.0)

        # 4. æ–‡ä»¶é‡è¦æ€§ï¼šPageRank åˆ†æ•°é«˜çš„æ–‡ä»¶ä¸­çš„ç¬¦å·æ›´é‡è¦
        if hasattr(repo_map, 'pagerank_scores') and repo_map.pagerank_scores:
            file_score = repo_map.pagerank_scores.get(file_path, 0.0)
            score *= (1.0 + file_score * 2)  # PageRank ä½œä¸ºä¹˜æ•°

        # 5. ç¬¦å·é•¿åº¦ï¼šè¿‡çŸ­çš„ç¬¦å·é™æƒ
        if len(symbol.name) < 3:
            score *= 0.5
        elif len(symbol.name) >= 8 and self._has_structured_naming(symbol.name):
            score *= 1.2  # é•¿çš„ç»“æ„åŒ–å‘½ååŠ æƒ

        # 6. ç§æœ‰ç¬¦å·é™æƒ
        if symbol.name.startswith('_') and not symbol.name.startswith('__'):
            score *= 0.3
        elif symbol.name.startswith('__'):
            score *= 0.1

        return score

    def _is_partial_match(self, symbol_name: str, mention: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºéƒ¨åˆ†åŒ¹é…"""
        # åŒ…å«å…³ç³»
        if mention in symbol_name or symbol_name in mention:
            return True

        # é©¼å³°å‘½åæ‹†åˆ†åŒ¹é…
        if self._camel_case_contains(symbol_name, mention):
            return True

        # ä¸‹åˆ’çº¿åˆ†å‰²åŒ¹é…
        if self._snake_case_contains(symbol_name, mention):
            return True

        return False

    def _camel_case_contains(self, symbol_name: str, mention: str) -> bool:
        """æ£€æŸ¥é©¼å³°å‘½åæ˜¯å¦åŒ…å«æåŠçš„è¯"""
        import re
        # å°†é©¼å³°å‘½åæ‹†åˆ†æˆå•è¯
        words = re.findall(r'[A-Z]?[a-z]+|[A-Z]+(?=[A-Z]|$)', symbol_name)
        words_lower = [w.lower() for w in words]
        return mention in words_lower

    def _snake_case_contains(self, symbol_name: str, mention: str) -> bool:
        """æ£€æŸ¥ä¸‹åˆ’çº¿å‘½åæ˜¯å¦åŒ…å«æåŠçš„è¯"""
        if '_' not in symbol_name:
            return False
        words = symbol_name.split('_')
        words_lower = [w.lower() for w in words if w]
        return mention in words_lower

    def _has_structured_naming(self, name: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºç»“æ„åŒ–å‘½åï¼ˆé©¼å³°ã€ä¸‹åˆ’çº¿ç­‰ï¼‰"""
        import re

        # camelCase æˆ– PascalCase
        if re.search(r'[a-z][A-Z]', name):
            return True

        # snake_case
        if '_' in name and any(c.isalpha() for c in name):
            return True

        # CONSTANT_CASE
        if name.isupper() and '_' in name:
            return True

        # åŒ…å«æ•°å­—çš„å‘½å
        if re.search(r'\w+\d+', name):
            return True

        return False

    def _extract_mentions(self, query: str) -> List[str]:
        """
        ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–åŒ¹é…å­—ç¬¦ä¸²
        query ä½œä¸ºæ­£åˆ™åŒ¹é…å­—ç¬¦ä¸²ç›´æ¥è¿”å›
        """
        return [query]


class ACast:
    """ASTæ¡†æ¶ä¸»å…¥å£ç±»"""

    def __init__(self, auto_register_parsers: bool = True, tmp_path: str = "~/.aworld/acast"):
        self.parsers: Dict[str, BaseParser] = {}
        self.analyzer: Optional[RepositoryAnalyzer] = None
        self.tmp_path = tmp_path

        if auto_register_parsers:
            self._auto_register_all_parsers()

        self.analyzer = self.create_analyzer()

    """
    code parser
    """

    def _auto_register_all_parsers(self) -> None:
        """è‡ªåŠ¨æ³¨å†Œæ‰€æœ‰å¯ç”¨çš„è§£æå™¨"""
        try:
            from .parser_utils import get_supported_languages, create_parser

            supported_languages = get_supported_languages()
            logger.info(f"æ­£åœ¨è‡ªåŠ¨æ³¨å†Œè§£æå™¨ï¼Œæ”¯æŒçš„è¯­è¨€: {', '.join(supported_languages)}")

            for lang in supported_languages:
                try:
                    parser = create_parser(lang)
                    if parser:
                        self.parsers[lang] = parser
                        logger.debug(f"âœ… è‡ªåŠ¨æ³¨å†Œè§£æå™¨: {lang}")
                except Exception as e:
                    logger.warning(f"âŒ æ— æ³•æ³¨å†Œ{lang}è§£æå™¨: {e}")

            logger.info(f"è§£æå™¨è‡ªåŠ¨æ³¨å†Œå®Œæˆï¼Œå…±æ³¨å†Œ {len(self.parsers)} ä¸ªè§£æå™¨")

        except Exception as e:
            logger.error(f"è‡ªåŠ¨æ³¨å†Œè§£æå™¨å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸æ‰‹åŠ¨æ³¨å†Œ

    def register_parser(self, language: str, parser: BaseParser) -> None:
        """æ³¨å†Œè¯­è¨€è§£æå™¨"""
        self.parsers[language] = parser
        logger.info(f"æ³¨å†Œè§£æå™¨: {language}")

    def list_supported_languages(self) -> List[str]:
        """åˆ—å‡ºæ”¯æŒçš„ç¼–ç¨‹è¯­è¨€"""
        return list(self.parsers.keys())

    def get_parser_info(self, language: str) -> Dict[str, Any]:
        """è·å–è§£æå™¨ä¿¡æ¯"""
        if language not in self.parsers:
            return {}

        parser = self.parsers[language]
        return {
            'language': parser.language,
            'file_extensions': list(parser.file_extensions),
            'comment_patterns': parser.comment_patterns,
        }

    def parse(self, file_path: Path) -> Optional[CodeNode]:
        """
        è§£ææ–‡ä»¶çš„ä¾¿æ·æ–¹æ³•

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            è§£æç»“æœCodeNodeï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        parser = self.get_parser(file_path)
        if parser:
            return parser.parse_file(file_path)
        return None

    def get_parser(self, file_path: Path) -> Optional[BaseParser]:
        """æ ¹æ®æ–‡ä»¶è·¯å¾„è·å–é€‚å½“çš„è§£æå™¨"""
        for parser in self.parsers.values():
            if parser.can_parse(file_path):
                return parser
        return None

    """
    repository analyzer
    """

    def create_analyzer(self, code_analyzer_class: type = None) -> RepositoryAnalyzer:
        """åˆ›å»ºä»“åº“åˆ†æå™¨"""
        if code_analyzer_class is None:
            from .analyzer import DefaultCodeAnalyzer
            code_analyzer_class = DefaultCodeAnalyzer

        code_analyzer = code_analyzer_class(self.parsers)
        self.analyzer = RepositoryAnalyzer(code_analyzer)
        return self.analyzer

    def analyze(self, *args, **kwargs) -> RepositoryMap:
        """
        åˆ†æä»“åº“çš„ä¾¿æ·æ–¹æ³•ï¼Œè‡ªåŠ¨è®°å½•åˆ†æç»“æœåˆ°tmp_pathç›®å½•
        
        Args:
            *args: ä¼ é€’ç»™analyzer.analyzeçš„ä½ç½®å‚æ•°
            **kwargs: ä¼ é€’ç»™analyzer.analyzeçš„å…³é”®å­—å‚æ•°ï¼ŒåŒ…æ‹¬ï¼š
                - root_path: ä»“åº“æ ¹ç›®å½•ï¼ˆç”¨äºç”Ÿæˆæ–‡ä»¶åï¼‰
                - auto_record: æ˜¯å¦è‡ªåŠ¨è®°å½•ï¼ˆé»˜è®¤Trueï¼‰
                - record_name: è®°å½•æ–‡ä»¶åï¼ˆå¯é€‰ï¼Œé»˜è®¤åŸºäºroot_pathå’Œæ—¶é—´æˆ³ç”Ÿæˆï¼‰
        
        Returns:
            å®Œæ•´çš„ä»“åº“æ˜ å°„
        """
        if not self.analyzer:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ create_analyzer() åˆ›å»ºåˆ†æå™¨")

        # æå–è®°å½•ç›¸å…³å‚æ•°ï¼ˆåœ¨è°ƒç”¨analyzeä¹‹å‰æå–root_pathï¼‰
        auto_record = kwargs.pop('auto_record', True)
        record_name = kwargs.pop('record_name', None)

        # æ‰§è¡Œåˆ†æ
        repo_map = self.analyzer.analyze(*args, **kwargs)

        # è‡ªåŠ¨è®°å½•åˆ†æç»“æœ
        if auto_record:
            try:
                # ç”Ÿæˆæ–‡ä»¶å
                name = record_name
                # è®°å½•åˆ†æç»“æœ
                self.record_analyze_result(name, repo_map)
            except Exception as e:
                # è®°å½•å¤±è´¥ä¸å½±å“åˆ†æç»“æœè¿”å›
                logger.warning(f"è‡ªåŠ¨è®°å½•åˆ†æç»“æœå¤±è´¥: {e}")

        return repo_map

    def record_analyze_result(self, name, repo_map):
        """
        å°†repo_mapè®°å½•åˆ°tmp_pathç›®å½•ä¸‹ï¼Œæ ‡è®°æ–‡ä»¶ä¸ºname
        
        Args:
            name: æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
            repo_map: è¦ä¿å­˜çš„ä»“åº“æ˜ å°„å¯¹è±¡
        
        Returns:
            ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        tmp_dir = Path(self.tmp_path)
        tmp_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜ä¸ºJSONæ–‡ä»¶
        file_path = tmp_dir / f"{name}.json"
        try:
            # ä½¿ç”¨RepositoryMapçš„to_dictæ–¹æ³•åºåˆ—åŒ–
            json_data = repo_map.to_dict()
            with open(file_path, 'w', encoding='utf-8') as f:
                json.dump(json_data, f, ensure_ascii=False, indent=2)
            logger.info(f"åˆ†æç»“æœå·²ä¿å­˜åˆ°: {file_path}")
            return file_path
        except Exception as e:
            logger.error(f"ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")
            raise

    def load_analyze_result(self, name: str) -> Optional[RepositoryMap]:
        """
        ä»tmp_pathç›®å½•åŠ è½½å·²ä¿å­˜çš„åˆ†æç»“æœ
        
        Args:
            name: æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        
        Returns:
            ä»“åº“æ˜ å°„å¯¹è±¡ï¼Œå¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–åŠ è½½å¤±è´¥åˆ™è¿”å›None
        """
        tmp_dir = Path(self.tmp_path)
        file_path = tmp_dir / f"{name}.json"

        if not file_path.exists():
            logger.warning(f"åˆ†ææŠ¥å‘Šä¸å­˜åœ¨: {file_path}")
            return None

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                json_data = json.load(f)
            # ä½¿ç”¨RepositoryMapçš„from_dictæ–¹æ³•ååºåˆ—åŒ–
            repo_map = RepositoryMap.from_dict(json_data)
            # print('repo_map', repo_map)
            logger.info(f"æˆåŠŸåŠ è½½åˆ†ææŠ¥å‘Š: {file_path}")
            return repo_map
        except Exception as e:
            logger.error(f"åŠ è½½åˆ†ææŠ¥å‘Šå¤±è´¥: {e}")
            return None

    def recall(self,
               repo_map: Optional[RepositoryMap] = None,
               user_query: str = "",
               max_tokens: int = 8000,
               context_layers: Optional[List[str]] = None,
               record_name: Optional[str] = None) -> str:
        """
        è·å–ä¼˜åŒ–çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ç»™LLM
        ä¼˜å…ˆä»record_analyze_resultè®°å½•çš„åˆ†ææŠ¥å‘Šä¸­å¬å›

        Args:
            repo_map: ä»“åº“æ˜ å°„ï¼ˆå¯é€‰ï¼Œå¦‚æœæä¾›äº†record_nameåˆ™ä¼˜å…ˆä½¿ç”¨è®°å½•çš„åˆ†ææŠ¥å‘Šï¼‰
            user_query: ç”¨æˆ·æŸ¥è¯¢
            max_tokens: æœ€å¤§tokenæ•°é‡
            context_layers: æŒ‡å®šè¦åŒ…å«çš„ä¸Šä¸‹æ–‡å±‚æ¬¡
            record_name: å·²ä¿å­˜çš„åˆ†ææŠ¥å‘Šåç§°ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰

        Returns:
            æ ¼å¼åŒ–çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if not self.analyzer:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ create_analyzer() åˆ›å»ºåˆ†æå™¨")

        # ä¼˜å…ˆä»è®°å½•çš„åˆ†ææŠ¥å‘Šä¸­åŠ è½½
        if record_name:
            loaded_repo_map = self.load_analyze_result(record_name)
            if loaded_repo_map is not None:
                repo_map = loaded_repo_map
            elif repo_map is None:
                logger.warning(f"æ— æ³•åŠ è½½åˆ†ææŠ¥å‘Š '{record_name}'ï¼Œä¸”æœªæä¾›repo_mapå‚æ•°")

        # å¦‚æœä»ç„¶æ²¡æœ‰repo_mapï¼ŒæŠ›å‡ºé”™è¯¯
        if repo_map is None:
            raise ValueError("å¿…é¡»æä¾›repo_mapæˆ–æœ‰æ•ˆçš„record_name")

        # å¦‚æœæŒ‡å®šäº†é«˜çº§å‚æ•°ï¼Œä½¿ç”¨analyzerçš„é«˜çº§recallæ–¹æ³•
        if context_layers is not None:
            return self.analyzer.recall(
                repo_map=repo_map,
                user_query=user_query,
                max_tokens=max_tokens,
                context_layers=context_layers,
            )
        else:
            # ä½¿ç”¨é»˜è®¤çš„ç®€å•recallæ–¹æ³•
            return self.analyzer.recall(repo_map, user_query, max_tokens)

    def generate_snapshot(self, target_dir: Path, version: str = "v0") -> Path:
        """
        ç”Ÿæˆç›®æ ‡ç›®å½•çš„å‹ç¼©å¿«ç…§

        Args:
            target_dir: ç›®æ ‡ç›®å½•è·¯å¾„ï¼Œéœ€è¦åˆ›å»ºå¿«ç…§çš„ç›®å½•
            version: ç‰ˆæœ¬å·å­—ç¬¦ä¸²ï¼Œé»˜è®¤ä¸º"v0"

        Returns:
            ä¿å­˜çš„å¿«ç…§æ–‡ä»¶è·¯å¾„
        """
        import tarfile

        target_dir = Path(target_dir)
        if not target_dir.exists():
            raise ValueError(f"ç›®æ ‡ç›®å½•ä¸å­˜åœ¨: {target_dir}")

        # ä¿å­˜å¿«ç…§åˆ°tmp_pathç›®å½•
        tmp_dir = Path(self.tmp_path)
        tmp_dir.mkdir(parents=True, exist_ok=True)

        # ç”Ÿæˆå¿«ç…§æ–‡ä»¶åï¼š{pathæœ«å°¾ä¸€æ®µ}_{version}.tar.gz
        path_suffix = target_dir.name or "default"
        snapshot_filename = f"{path_suffix}_{version}.tar.gz"
        snapshot_path = tmp_dir / snapshot_filename

        # åˆ›å»ºå‹ç¼©å¿«ç…§
        with tarfile.open(snapshot_path, "w:gz") as tar:
            tar.add(target_dir, arcname=target_dir.name,
                    filter=lambda tarinfo: None if '__pycache__' in tarinfo.name or '.pyc' in tarinfo.name else tarinfo)

        logger.info(f"Generated snapshot saved to: {snapshot_path}")

        return snapshot_path

    def create_enhanced_copy(self,
                             source_dir: Path,
                             patch_content: str,
                             version: str = "v0",
                             strict_validation: bool = True,
                             max_context_mismatches: int = 0) -> Path:
        """
        åŸåœ°æ›´æ–°æºä»£ç ç›®å½•å¹¶åº”ç”¨patchï¼Œå¢å¼ºéªŒè¯æœºåˆ¶

        Args:
            source_dir: æºä»£ç ç›®å½•ï¼ˆå°†åœ¨æ­¤ç›®å½•åŸåœ°æ›´æ–°ï¼‰
            patch_content: patchæ–‡ä»¶å†…å®¹
            version: ç‰ˆæœ¬å·ï¼ˆå¦‚ "v0", "v1"ï¼‰ï¼Œç”¨äºå‘½åpatchæ–‡ä»¶
            strict_validation: æ˜¯å¦å¯ç”¨ä¸¥æ ¼éªŒè¯æ¨¡å¼ï¼ˆé»˜è®¤Trueï¼‰
            max_context_mismatches: å…è®¸çš„æœ€å¤§ä¸Šä¸‹æ–‡ä¸åŒ¹é…æ¬¡æ•°ï¼ˆé»˜è®¤0ï¼‰

        Returns:
            æ›´æ–°åçš„ç›®å½•è·¯å¾„ï¼ˆä¸source_dirç›¸åŒï¼‰

        Raises:
            ValidationError: å½“ä¸Šä¸‹æ–‡éªŒè¯å¤±è´¥ä¸”è¶…è¿‡å…è®¸çš„ä¸åŒ¹é…æ¬¡æ•°æ—¶
        """
        source_dir = Path(source_dir)
        if not source_dir.exists():
            raise ValueError(f"æºç›®å½•ä¸å­˜åœ¨: {source_dir}")

        try:
            # ä¿å­˜patchæ–‡ä»¶ï¼š{pathæœ«å°¾ä¸€æ®µ}_{version}
            path_suffix = Path(source_dir).name or "default"
            patch_file = source_dir / f"{path_suffix}_{version}.patch"
            patch_file.write_text(patch_content, encoding='utf-8')

            # è§£æå¹¶åº”ç”¨patchï¼ˆåŸåœ°æ›´æ–°ï¼‰ï¼Œå¢å¼ºéªŒè¯
            self._apply_patches_with_validation(source_dir, patch_content, strict_validation, max_context_mismatches)

            return source_dir

        except Exception as e:
            raise RuntimeError(f"åº”ç”¨patchå¤±è´¥: {e}")

    def _apply_patches_with_validation(self, target_dir: Path, patch_content: str, strict_validation: bool = True,
                                       max_context_mismatches: int = 0):
        """
        ä½¿ç”¨difflibç”Ÿæˆå’Œpatch_ngåº“åº”ç”¨çš„æœ€ä¼˜è¡¥ä¸å¤„ç†æ–¹æ³•

        åŸºäº/Users/hgc/hgc_repo/basic/text2agent/difflib_apply_run.pyçš„å‚è€ƒå®ç°ï¼Œé‡‡ç”¨ä»¥ä¸‹æŠ€æœ¯æ ˆï¼š
        - difflib: Pythonæ ‡å‡†åº“ï¼Œç”¨äºç”Ÿæˆç»Ÿä¸€diffæ ¼å¼
        - patch_ng: ä¸“ä¸šè¡¥ä¸åº“ï¼Œç”¨äºè§£æå’Œåº”ç”¨è¡¥ä¸

        Args:
            target_dir: ç›®æ ‡ç›®å½•
            patch_content: unified diffæ ¼å¼çš„è¡¥ä¸å†…å®¹
            strict_validation: æ˜¯å¦å¯ç”¨ä¸¥æ ¼éªŒè¯æ¨¡å¼
            max_context_mismatches: å…è®¸çš„æœ€å¤§ä¸Šä¸‹æ–‡ä¸åŒ¹é…æ¬¡æ•°
        """
        try:
            import patch_ng
        except ImportError:
            logger.error("patch_ngåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install patch-ng")
            raise RuntimeError("éœ€è¦å®‰è£…patch_ngåº“ï¼špip install patch-ng")

        logger.info("ğŸš€ å¼€å§‹ä½¿ç”¨ç»è¿‡éªŒè¯çš„difflib+patch_ngæ–¹æ¡ˆåº”ç”¨è¡¥ä¸")
        # logging.basicConfig(level=logging.DEBUG, format='%(levelname)s: %(message)s')
        # å°†patchå†…å®¹å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œpatch_ngéœ€è¦ä»æ–‡ä»¶è¯»å–

        try:
            with tempfile.NamedTemporaryFile(mode='w', suffix='.patch', delete=False, encoding='utf-8') as temp_patch_file:
                temp_patch_file.write(patch_content)
                temp_patch_path = temp_patch_file.name
            logger.info(f"patch_content: {patch_content}")

            logger.info(f"ğŸ“‹ è¡¥ä¸å·²å†™å…¥ä¸´æ—¶æ–‡ä»¶: {temp_patch_path}")

            # éªŒè¯åº”ç”¨ç»“æœï¼ˆå¦‚æœå¯ç”¨ä¸¥æ ¼éªŒè¯ï¼‰
            if strict_validation:
                self._validate_patch_ng_result(target_dir, patch_content)

            # ä½¿ç”¨patch_ngåŠ è½½å’Œåº”ç”¨è¡¥ä¸ï¼ˆå‚è€ƒå®ç°çš„æ–¹å¼ï¼‰
            pset = patch_ng.fromfile(temp_patch_path)

            if not pset:
                raise RuntimeError("patch_ngæ— æ³•è§£æè¡¥ä¸å†…å®¹")

            logger.info(f"ğŸ“‹ patch_ngè§£æåˆ°è¡¥ä¸æ–‡ä»¶")

            # åº”ç”¨è¡¥ä¸åˆ°ç›®æ ‡ç›®å½•
            # patch_ng.apply(root=str(target_dir)) æ–¹å¼å‚è€ƒå®ç°
            apply_result = pset.apply(root=str(target_dir))

            if apply_result:
                logger.info("âœ… patch_ngè¡¥ä¸åº”ç”¨æˆåŠŸï¼")

                logger.info("ğŸ“Š å¤„ç†ç»“æœ: è¡¥ä¸åº”ç”¨å®Œæˆï¼Œæ‰€æœ‰æ–‡ä»¶æˆåŠŸå¤„ç†")
            else:
                error_msg = "patch_ngè¡¥ä¸åº”ç”¨å¤±è´¥ï¼Œå¯èƒ½æ˜¯ä¸Šä¸‹æ–‡ä¸åŒ¹é…æˆ–æ–‡ä»¶ä¸å­˜åœ¨"
                logger.error(f"âŒ {error_msg}")

                if strict_validation:
                    raise RuntimeError(error_msg)
                else:
                    logger.warning("âš ï¸ éä¸¥æ ¼æ¨¡å¼ä¸‹ç»§ç»­æ‰§è¡Œ")

        except Exception as e:
            logger.error(f"âŒ è¡¥ä¸åº”ç”¨è¿‡ç¨‹å¤±è´¥: {e} {traceback.format_exc()}")
            if strict_validation:
                raise

    def _validate_patch_ng_result(self, target_dir: Path, patch_content: str):
        """
        éªŒè¯patch_ngåº”ç”¨ç»“æœçš„æ­£ç¡®æ€§

        Args:
            target_dir: ç›®æ ‡ç›®å½•
            patch_content: åŸå§‹è¡¥ä¸å†…å®¹
        """
        try:
            logger.debug("ğŸ” å¼€å§‹éªŒè¯patch_ngåº”ç”¨ç»“æœ...")

            # ç®€å•çš„éªŒè¯ï¼šæ£€æŸ¥è¡¥ä¸æ˜¯å¦åŒ…å«é¢„æœŸçš„å˜æ›´æ ‡è®°
            lines = patch_content.split('\n')
            added_lines = [line[1:] for line in lines if line.startswith('+') and not line.startswith('+++')]
            removed_lines = [line[1:] for line in lines if line.startswith('-') and not line.startswith('---')]

            logger.debug(f"é¢„æœŸæ·»åŠ  {len(added_lines)} è¡Œ, åˆ é™¤ {len(removed_lines)} è¡Œ")

            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„éªŒè¯é€»è¾‘
            # ä¾‹å¦‚ï¼šéªŒè¯ç‰¹å®šçš„æ–‡ä»¶å†…å®¹æ˜¯å¦åŒ…å«é¢„æœŸçš„å˜æ›´

            logger.debug("âœ… patch_ngåº”ç”¨ç»“æœéªŒè¯é€šè¿‡")

        except Exception as e:
            logger.warning(f"âš ï¸ patch_ngç»“æœéªŒè¯å¤±è´¥: {e}")

    def _apply_patches(self, target_dir: Path, patch_content: str):
        """ä½¿ç”¨ç»è¿‡éªŒè¯çš„difflib+patch_ngæ–¹æ¡ˆåº”ç”¨è¡¥ä¸ï¼ˆå…¼å®¹æ€§åŒ…è£…å™¨ï¼‰"""
        # è°ƒç”¨ç»è¿‡å®Œæ•´æµ‹è¯•éªŒè¯çš„æ–¹æ³•ï¼Œä¿æŒå‘åå…¼å®¹
        self._apply_patches_with_validation(
            target_dir, patch_content,
            strict_validation=False,  # éä¸¥æ ¼æ¨¡å¼ä»¥ä¿æŒå…¼å®¹æ€§
            max_context_mismatches=999  # å…è®¸æ›´å¤šä¸åŒ¹é…ä»¥ä¿æŒå…¼å®¹æ€§
        )

    def json_operations_to_patch(self, operations_json: str, source_dir: Path) -> str:
        """
        å°†JSONæ ¼å¼çš„æ“ä½œæŒ‡ä»¤è½¬æ¢ä¸ºunified diffæ ¼å¼çš„patchå†…å®¹

        Args:
            operations_json: JSONæ ¼å¼çš„æ“ä½œæŒ‡ä»¤å­—ç¬¦ä¸²ï¼Œæ”¯æŒä»¥ä¸‹æ“ä½œç±»å‹ï¼š
                - insert: åœ¨æŒ‡å®šè¡Œåæ’å…¥ä»£ç 
                - replace: æ›¿æ¢æŒ‡å®šè¡ŒèŒƒå›´çš„ä»£ç 
                - delete: åˆ é™¤æŒ‡å®šè¡ŒèŒƒå›´çš„ä»£ç 
            source_dir: æºä»£ç ç›®å½•è·¯å¾„

        Returns:
            ç»Ÿä¸€diffæ ¼å¼çš„patchå†…å®¹

        Example:
            æ“ä½œJSONæ ¼å¼ï¼š
            {
                "operations": [
                    {
                        "type": "insert",
                        "file_path": "example.py",
                        "after_line": 10,
                        "content": ["æ–°å¢è¡Œ1", "æ–°å¢è¡Œ2"]
                    },
                    {
                        "type": "replace",
                        "file_path": "example.py",
                        "start_line": 15,
                        "end_line": 20,
                        "content": ["æ›¿æ¢å†…å®¹"]
                    },
                    {
                        "type": "delete",
                        "file_path": "example.py",
                        "start_line": 25,
                        "end_line": 30
                    }
                ]
            }
        """
        import difflib

        try:
            operations_data = json.loads(operations_json)
        except json.JSONDecodeError as e:
            raise ValueError(f"æ— æ•ˆçš„JSONæ ¼å¼: {e}")

        if "operations" not in operations_data:
            raise ValueError("JSONä¸­ç¼ºå°‘'operations'å­—æ®µ")

        operations = operations_data["operations"]

        # æŒ‰æ–‡ä»¶è·¯å¾„åˆ†ç»„æ“ä½œï¼Œç¡®ä¿æ¯ä¸ªæ–‡ä»¶çš„æ“ä½œæŒ‰è¡Œå·æ’åº
        file_operations = {}
        for op in operations:
            if "file_path" not in op or "type" not in op:
                raise ValueError("æ“ä½œç¼ºå°‘å¿…è¦å­—æ®µ 'file_path' æˆ– 'type'")

            file_path = op["file_path"]
            if file_path not in file_operations:
                file_operations[file_path] = []
            file_operations[file_path].append(op)

        # å¯¹æ¯ä¸ªæ–‡ä»¶çš„æ“ä½œæŒ‰è¡Œå·æ’åºï¼ˆä»åå¾€å‰ï¼Œé¿å…è¡Œå·åç§»ï¼‰
        for file_path in file_operations:
            file_operations[file_path].sort(key=self._get_operation_sort_key, reverse=True)

        all_diffs = []

        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for file_path, ops in file_operations.items():
            full_file_path = source_dir / file_path

            if not full_file_path.exists():
                logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡: {full_file_path}")
                continue

            try:
                # è¯»å–åŸå§‹æ–‡ä»¶å†…å®¹
                with open(full_file_path, 'r', encoding='utf-8') as f:
                    original_lines = f.readlines()

                # åº”ç”¨æ‰€æœ‰æ“ä½œåˆ°å†…å®¹å‰¯æœ¬
                modified_lines = original_lines.copy()

                for op in ops:
                    modified_lines = self._apply_single_operation(modified_lines, op)

                # ç”Ÿæˆunified diff
                original_content = ''.join(original_lines)
                modified_content = ''.join(modified_lines)

                diff = difflib.unified_diff(
                    original_content.splitlines(keepends=True),
                    modified_content.splitlines(keepends=True),
                    fromfile=f"a/{file_path}",
                    tofile=f"b/{file_path}",
                    lineterm='\n'
                )

                diff_content = ''.join(diff)
                if diff_content.strip():  # åªæ·»åŠ éç©ºçš„diff
                    all_diffs.append(diff_content)

            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                raise

        if not all_diffs:
            return ""  # æ²¡æœ‰ä»»ä½•å˜åŒ–

        return '\n'.join(all_diffs)

    def _get_operation_sort_key(self, op: dict) -> int:
        """è·å–æ“ä½œçš„æ’åºé”®ï¼Œç”¨äºæŒ‰è¡Œå·æ’åº"""
        if op["type"] == "insert":
            return op.get("after_line", 0)
        elif op["type"] in ["replace", "delete"]:
            return op.get("start_line", 0)
        else:
            return 0

    def _apply_single_operation(self, lines: List[str], op: dict) -> List[str]:
        """
        å¯¹è¡Œåˆ—è¡¨åº”ç”¨å•ä¸ªæ“ä½œ

        Args:
            lines: æ–‡ä»¶è¡Œåˆ—è¡¨ï¼ˆæ¯è¡ŒåŒ…å«æ¢è¡Œç¬¦ï¼‰
            op: å•ä¸ªæ“ä½œå­—å…¸

        Returns:
            åº”ç”¨æ“ä½œåçš„è¡Œåˆ—è¡¨
        """
        op_type = op["type"]

        if op_type == "insert":
            after_line = op.get("after_line", 0)
            content = op.get("content", [])

            if after_line < 0 or after_line > len(lines):
                raise ValueError(f"æ’å…¥ä½ç½®æ— æ•ˆ: after_line={after_line}, æ–‡ä»¶å…±{len(lines)}è¡Œ")

            # ç¡®ä¿å†…å®¹è¡Œéƒ½ä»¥æ¢è¡Œç¬¦ç»“å°¾
            insert_lines = [line if line.endswith('\n') else line + '\n' for line in content]

            # åœ¨æŒ‡å®šè¡Œåæ’å…¥
            return lines[:after_line] + insert_lines + lines[after_line:]

        elif op_type == "replace":
            start_line = op.get("start_line", 1)
            end_line = op.get("end_line", start_line)
            content = op.get("content", [])

            if start_line < 1 or end_line < start_line or start_line > len(lines):
                raise ValueError(f"æ›¿æ¢èŒƒå›´æ— æ•ˆ: start_line={start_line}, end_line={end_line}, æ–‡ä»¶å…±{len(lines)}è¡Œ")

            # è½¬æ¢ä¸º0-basedç´¢å¼•
            start_idx = start_line - 1
            end_idx = min(end_line, len(lines))

            # ç¡®ä¿å†…å®¹è¡Œéƒ½ä»¥æ¢è¡Œç¬¦ç»“å°¾
            replace_lines = [line if line.endswith('\n') else line + '\n' for line in content]

            # æ›¿æ¢æŒ‡å®šèŒƒå›´
            return lines[:start_idx] + replace_lines + lines[end_idx:]

        elif op_type == "delete":
            start_line = op.get("start_line", 1)
            end_line = op.get("end_line", start_line)

            if start_line < 1 or end_line < start_line or start_line > len(lines):
                raise ValueError(f"åˆ é™¤èŒƒå›´æ— æ•ˆ: start_line={start_line}, end_line={end_line}, æ–‡ä»¶å…±{len(lines)}è¡Œ")

            # è½¬æ¢ä¸º0-basedç´¢å¼•
            start_idx = start_line - 1
            end_idx = min(end_line, len(lines))

            # åˆ é™¤æŒ‡å®šèŒƒå›´
            return lines[:start_idx] + lines[end_idx:]

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {op_type}")

    def deploy_operations(self,
                         operations_json: str,
                         source_dir: Path,
                         version: str = "v0",
                         strict_validation: bool = True,
                         max_context_mismatches: int = 0) -> Path:
        """
        æ ¹æ®JSONæ“ä½œæŒ‡ä»¤éƒ¨ç½²ä»£ç å˜æ›´

        è¿™ä¸ªæ–¹æ³•ç»“åˆäº†json_operations_to_patchå’Œcreate_enhanced_copyçš„åŠŸèƒ½ï¼Œ
        æä¾›äº†ä¸€ä¸ªä¾¿æ·çš„æ¥å£æ¥ç›´æ¥ä»JSONæ“ä½œéƒ¨ç½²åˆ°æºä»£ç ç›®å½•ã€‚

        Args:
            operations_json: JSONæ ¼å¼çš„æ“ä½œæŒ‡ä»¤
            source_dir: æºä»£ç ç›®å½•
            version: ç‰ˆæœ¬å·
            strict_validation: æ˜¯å¦å¯ç”¨ä¸¥æ ¼éªŒè¯
            max_context_mismatches: å…è®¸çš„æœ€å¤§ä¸Šä¸‹æ–‡ä¸åŒ¹é…æ¬¡æ•°

        Returns:
            æ›´æ–°åçš„ç›®å½•è·¯å¾„
        """
        logger.info("ğŸš€ å¼€å§‹æ ¹æ®JSONæ“ä½œæŒ‡ä»¤éƒ¨ç½²ä»£ç å˜æ›´")

        # è½¬æ¢JSONæ“ä½œä¸ºpatchæ ¼å¼
        patch_content = self.json_operations_to_patch(operations_json, source_dir)

        if not patch_content.strip():
            logger.info("ğŸ“‹ æ²¡æœ‰æ£€æµ‹åˆ°ä»»ä½•ä»£ç å˜æ›´ï¼Œè·³è¿‡éƒ¨ç½²")
            return source_dir

        logger.info(f"ğŸ“ å·²ç”Ÿæˆpatchå†…å®¹ï¼Œé•¿åº¦: {len(patch_content)} å­—ç¬¦")

        # ä½¿ç”¨ç°æœ‰çš„create_enhanced_copyæ–¹æ³•åº”ç”¨patch
        return self.create_enhanced_copy(
            source_dir=source_dir,
            patch_content=patch_content,
            version=version,
            strict_validation=strict_validation,
            max_context_mismatches=max_context_mismatches
        )

    def search_replace_in_file(self,
                              file_path: Path,
                              search_text: str,
                              replace_text: str,
                              fuzzy_match: bool = True,
                              similarity_threshold: float = 0.8) -> Dict[str, Any]:
        """
        åŸºäºaiderç®—æ³•åœ¨æ–‡ä»¶ä¸­æ‰§è¡Œæœç´¢æ›¿æ¢æ“ä½œ

        Args:
            file_path: ç›®æ ‡æ–‡ä»¶è·¯å¾„
            search_text: è¦æœç´¢çš„ä»£ç æ®µ
            replace_text: æ›¿æ¢åçš„ä»£ç æ®µ
            fuzzy_match: æ˜¯å¦å¯ç”¨æ¨¡ç³ŠåŒ¹é…
            similarity_threshold: æ¨¡ç³ŠåŒ¹é…çš„ç›¸ä¼¼åº¦é˜ˆå€¼(0.0-1.0)

        Returns:
            åŒ…å«æ“ä½œç»“æœçš„å­—å…¸ï¼š
            {
                "success": bool,
                "modified": bool,
                "original_content": str,
                "new_content": str,
                "match_info": dict,
                "error": str
            }
        """
        result = {
            "success": False,
            "modified": False,
            "original_content": "",
            "new_content": "",
            "match_info": {},
            "error": ""
        }

        try:
            if not file_path.exists():
                result["error"] = f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"
                return result

            # è¯»å–æ–‡ä»¶å†…å®¹
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()

            result["original_content"] = content

            # æ‰§è¡Œæœç´¢æ›¿æ¢
            new_content = self._fuzzy_search_replace(
                content, search_text, replace_text,
                fuzzy_match, similarity_threshold
            )

            if new_content:
                result["new_content"] = new_content
                result["modified"] = True
                result["success"] = True

                # å†™å…¥æ–‡ä»¶
                with open(file_path, 'w', encoding='utf-8') as f:
                    f.write(new_content)

                logger.info(f"âœ… æœç´¢æ›¿æ¢æˆåŠŸ: {file_path}")
            else:
                result["error"] = "æœªæ‰¾åˆ°åŒ¹é…çš„ä»£ç æ®µè¿›è¡Œæ›¿æ¢"
                logger.warning(f"âš ï¸ æœªæ‰¾åˆ°åŒ¹é…é¡¹: {file_path}")

            # ä¿®æ”¹åé‡å»ºç´¢å¼•
            logger.info(f"rebuild analyze|start|{file_path}")
            self.analyze(root_path=file_path.parent,
                         ignore_patterns=['__pycache__', '*.pyc', '.git'],
                         record_name=Path(file_path).name)
            logger.info(f"rebuild analyze|end|{file_path}")
        except Exception as e:
            result["error"] = f"æœç´¢æ›¿æ¢å¤±è´¥: {str(e)}"
            logger.error(f"âŒ æœç´¢æ›¿æ¢é”™è¯¯: {e}")

        return result



    def _fuzzy_search_replace(self,
                             content: str,
                             search_text: str,
                             replace_text: str,
                             fuzzy_match: bool = False,  # é»˜è®¤ç¦ç”¨æ¨¡ç³ŠåŒ¹é…
                             similarity_threshold: float = 1.0) -> Optional[str]:
        """
        ç²¾ç¡®æœç´¢æ›¿æ¢ç®—æ³• - ä»…æ”¯æŒç²¾ç¡®åŒ¹é…

        é‡‡ç”¨ä»¥ä¸‹ç­–ç•¥ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰ï¼š
        1. ç²¾ç¡®åŒ¹é…ï¼ˆæ¨èï¼‰
        2. å¦‚æœå¯ç”¨fuzzy_matchï¼Œåˆ™è¿›è¡Œç©ºç™½å­—ç¬¦çµæ´»åŒ¹é…ï¼ˆä¸æ¨èï¼‰

        Args:
            content: æ–‡ä»¶å†…å®¹
            search_text: æœç´¢æ–‡æœ¬
            replace_text: æ›¿æ¢æ–‡æœ¬
            fuzzy_match: æ˜¯å¦å¯ç”¨ç©ºç™½å­—ç¬¦çµæ´»åŒ¹é…ï¼ˆé»˜è®¤Falseï¼‰
            similarity_threshold: è¢«å¿½ç•¥ï¼ˆä¸ºä¿æŒæ¥å£å…¼å®¹æ€§ï¼‰

        Returns:
            æ›¿æ¢åçš„å†…å®¹ï¼Œå¦‚æœæœªæ‰¾åˆ°åŒ¹é…åˆ™è¿”å›None
        """
        if not search_text.strip():
            return None

        # å‡†å¤‡å†…å®¹å’Œæœç´¢æ–‡æœ¬
        content, content_lines = self._prep_text(content)
        search_text, search_lines = self._prep_text(search_text)
        replace_text, replace_lines = self._prep_text(replace_text)

        # ç­–ç•¥1: ç²¾ç¡®åŒ¹é…ï¼ˆä¸»è¦ç­–ç•¥ï¼‰
        result = self._perfect_replace(content_lines, search_lines, replace_lines)
        if result:
            logger.info("âœ… ä½¿ç”¨ç²¾ç¡®åŒ¹é…ç­–ç•¥")
            return result

        if fuzzy_match:
            # ç­–ç•¥2: ç©ºç™½å­—ç¬¦çµæ´»åŒ¹é…ï¼ˆä»…åœ¨æ˜ç¡®å¯ç”¨æ—¶ä½¿ç”¨ï¼‰
            result = self._whitespace_flexible_replace(content_lines, search_lines, replace_lines)
            if result:
                logger.warning("âš ï¸ ä½¿ç”¨ç©ºç™½å­—ç¬¦çµæ´»åŒ¹é…ç­–ç•¥ï¼ˆä¸æ¨èï¼Œå»ºè®®ä½¿ç”¨ç²¾ç¡®åŒ¹é…ï¼‰")
                return result

        # ä¸å†æ”¯æŒæ¨¡ç³Šç›¸ä¼¼åº¦åŒ¹é…
        logger.warning("âŒ æœªæ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œæœç´¢æ›¿æ¢å¤±è´¥")
        return None

    def _prep_text(self, text: str) -> Tuple[str, List[str]]:
        """å‡†å¤‡æ–‡æœ¬ï¼Œç¡®ä¿ä»¥æ¢è¡Œç¬¦ç»“å°¾å¹¶åˆ†å‰²æˆè¡Œ"""
        if text and not text.endswith("\n"):
            text += "\n"
        lines = text.splitlines(keepends=True)
        return text, lines

    def _perfect_replace(self, content_lines: List[str], search_lines: List[str], replace_lines: List[str]) -> Optional[str]:
        """ç²¾ç¡®åŒ¹é…æ›¿æ¢ - åŸºäºaiderçš„perfect_replaceç®—æ³•"""
        search_tuple = tuple(search_lines)
        search_len = len(search_lines)

        for i in range(len(content_lines) - search_len + 1):
            content_tuple = tuple(content_lines[i:i + search_len])
            if search_tuple == content_tuple:
                # æ‰¾åˆ°ç²¾ç¡®åŒ¹é…ï¼Œæ‰§è¡Œæ›¿æ¢
                result_lines = content_lines[:i] + replace_lines + content_lines[i + search_len:]
                return "".join(result_lines)

        return None

    def _whitespace_flexible_replace(self, content_lines: List[str], search_lines: List[str], replace_lines: List[str]) -> Optional[str]:
        """ç©ºç™½å­—ç¬¦çµæ´»åŒ¹é… - åŸºäºaiderçš„whitespace matchingç®—æ³•"""
        # è®¡ç®—æœ€å°å…¬å…±ç¼©è¿›
        leading_spaces = []
        for line in search_lines + replace_lines:
            if line.strip():  # åªè€ƒè™‘éç©ºè¡Œ
                leading_spaces.append(len(line) - len(line.lstrip()))

        if not leading_spaces:
            return None

        # ç§»é™¤å…¬å…±ç¼©è¿›
        min_indent = min(leading_spaces) if leading_spaces else 0
        if min_indent > 0:
            normalized_search = [line[min_indent:] if line.strip() else line for line in search_lines]
            normalized_replace = [line[min_indent:] if line.strip() else line for line in replace_lines]
        else:
            normalized_search = search_lines
            normalized_replace = replace_lines

        # å¯»æ‰¾åŒ¹é…ï¼ˆå¿½ç•¥ç¼©è¿›ï¼‰
        for i in range(len(content_lines) - len(normalized_search) + 1):
            match_indent = self._check_indent_match(
                content_lines[i:i + len(normalized_search)],
                normalized_search
            )

            if match_indent is not None:
                # åº”ç”¨ç›¸åŒçš„ç¼©è¿›åˆ°æ›¿æ¢æ–‡æœ¬
                adjusted_replace = [
                    match_indent + line if line.strip() else line
                    for line in normalized_replace
                ]
                result_lines = content_lines[:i] + adjusted_replace + content_lines[i + len(normalized_search):]
                return "".join(result_lines)

        return None

    def _check_indent_match(self, content_section: List[str], search_section: List[str]) -> Optional[str]:
        """æ£€æŸ¥å†…å®¹ç‰‡æ®µæ˜¯å¦ä¸æœç´¢ç‰‡æ®µåŒ¹é…ï¼ˆå¿½ç•¥ç¼©è¿›ï¼‰"""
        if len(content_section) != len(search_section):
            return None

        # æ£€æŸ¥å»é™¤ç¼©è¿›åçš„å†…å®¹æ˜¯å¦åŒ¹é…
        for content_line, search_line in zip(content_section, search_section):
            if content_line.lstrip() != search_line.lstrip():
                return None

        # è®¡ç®—ç»Ÿä¸€çš„ç¼©è¿›å‰ç¼€
        indents = set()
        for content_line, search_line in zip(content_section, search_section):
            if content_line.strip():  # åªè€ƒè™‘éç©ºè¡Œ
                content_indent = content_line[:len(content_line) - len(content_line.lstrip())]
                search_indent = search_line[:len(search_line) - len(search_line.lstrip())]
                indent_diff = content_indent[len(search_indent):] if len(content_indent) >= len(search_indent) else ""
                indents.add(indent_diff)

        if len(indents) == 1:
            return indents.pop()
        return None

    def _similarity_replace(self,
                           content_lines: List[str],
                           search_text: str,
                           search_lines: List[str],
                           replace_lines: List[str],
                           threshold: float) -> Optional[str]:
        """åŸºäºç›¸ä¼¼åº¦çš„æ¨¡ç³ŠåŒ¹é… - åŸºäºaiderçš„similarity matchingç®—æ³•"""
        max_similarity = 0.0
        best_match_start = -1
        best_match_end = -1

        # æœç´¢èŒƒå›´ï¼šå…è®¸10%çš„é•¿åº¦å˜åŒ–
        search_len = len(search_lines)
        min_len = math.floor(search_len * 0.9)
        max_len = math.ceil(search_len * 1.1)

        for length in range(min_len, max_len + 1):
            for i in range(len(content_lines) - length + 1):
                chunk_lines = content_lines[i:i + length]
                chunk_text = "".join(chunk_lines)

                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = SequenceMatcher(None, chunk_text, search_text).ratio()

                if similarity > max_similarity and similarity >= threshold:
                    max_similarity = similarity
                    best_match_start = i
                    best_match_end = i + length

        if best_match_start >= 0:
            logger.info(f"ğŸ¯ æ‰¾åˆ°æ¨¡ç³ŠåŒ¹é… (ç›¸ä¼¼åº¦: {max_similarity:.3f})")
            result_lines = (content_lines[:best_match_start] +
                           replace_lines +
                           content_lines[best_match_end:])
            return "".join(result_lines)

        return None

    def search_replace_operation(self,
                                source_dir: Path,
                                operation_json: str) -> Dict[str, Any]:
        """
        æ‰§è¡ŒåŸºäºJSONçš„æœç´¢æ›¿æ¢æ“ä½œ - ä»…æ”¯æŒç²¾ç¡®åŒ¹é…

        Args:
            source_dir: æºä»£ç ç›®å½•
            operation_json: JSONæ ¼å¼çš„æœç´¢æ›¿æ¢æ“ä½œæŒ‡ä»¤

        Returns:
            æ“ä½œç»“æœå­—å…¸

        Example JSON:
            {
                "operation": {
                    "type": "search_replace",
                    "file_path": "example.py",
                    "search": "def old_function():\n    pass",
                    "replace": "def new_function():\n    print('updated')",
                    "exact_match_only": true
                }
            }

        Note: ä¸ºç¡®ä¿ä»£ç ä¿®æ”¹çš„ç²¾ç¡®æ€§å’Œå®‰å…¨æ€§ï¼Œæ­¤æ–¹æ³•ä»…æ‰§è¡Œç²¾ç¡®åŒ¹é…ã€‚
              ä¸å†æ”¯æŒæ¨¡ç³ŠåŒ¹é…æˆ–ç©ºç™½å­—ç¬¦çµæ´»åŒ¹é…ã€‚
        """
        try:
            logger.debug('operation_json: ', operation_json)
            operation_data = json.loads(operation_json)
        except json.JSONDecodeError as e:
            return {"success": False, "error": f"æ— æ•ˆçš„JSONæ ¼å¼: {e}"}

        if "operation" not in operation_data:
            return {"success": False, "error": "JSONä¸­ç¼ºå°‘'operation'å­—æ®µ"}

        operation = operation_data["operation"]

        # éªŒè¯å¿…è¦å­—æ®µ
        required_fields = ["type", "file_path", "search", "replace"]
        for field in required_fields:
            if field not in operation:
                return {"success": False, "error": f"æ“ä½œç¼ºå°‘å¿…è¦å­—æ®µ: {field}"}

        if operation["type"] != "search_replace":
            return {"success": False, "error": f"ä¸æ”¯æŒçš„æ“ä½œç±»å‹: {operation['type']}"}

        # æ‰§è¡Œç²¾ç¡®æœç´¢æ›¿æ¢
        file_path = source_dir / operation["file_path"]
        search_text = operation["search"]
        replace_text = operation["replace"]

        # å¼ºåˆ¶ä½¿ç”¨ç²¾ç¡®åŒ¹é…æ¨¡å¼
        return self.search_replace_in_file(
            file_path, search_text, replace_text,
            fuzzy_match=False,  # ç¦ç”¨æ¨¡ç³ŠåŒ¹é…
            similarity_threshold=1.0  # ä»…ç²¾ç¡®åŒ¹é…
        )
