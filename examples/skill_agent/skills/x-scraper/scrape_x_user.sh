#!/usr/bin/env bash
# ============================================================
# scrape_x_user.sh â€” é€šè¿‡ agent-browser (CDP) æŠ“å– X ç”¨æˆ·å¸–å­
#
# ç”¨æ³•:
#   ./scrape_x_user.sh [-u <username>] [-k <keyword>] [-p <cdp_port>] [-n <max_scrolls>] [-o <output_file>] [-f <format>]
#
# å‚æ•°:
#   -u  X ç”¨æˆ·å (ä¸å¸¦@)ï¼Œé»˜è®¤ Alibaba_Qwen
#   -k  æœç´¢å…³é”®è¯ï¼Œå¯é€‰ï¼ˆä¸æŒ‡å®šåˆ™æŠ“å–ç”¨æˆ·æ‰€æœ‰æœ€æ–°å¸–å­ï¼‰
#   -p  CDP ç«¯å£å·ï¼Œé»˜è®¤ 9222
#   -n  æœ€å¤§æ»šåŠ¨æ¬¡æ•°ï¼Œé»˜è®¤ 10
#   -o  è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ stdout
#   -f  è¾“å‡ºæ ¼å¼: md (Markdown, é»˜è®¤) | rss (RSS XML) | json (åŸå§‹ JSON)
#
# ä¾èµ–:
#   - agent-browser (å·²é€šè¿‡ CDP è¿æ¥åˆ°è¿è¡Œä¸­çš„æµè§ˆå™¨)
#   - python3
#
# ç¤ºä¾‹:
#   ./scrape_x_user.sh                                    # æŠ“å– Alibaba_Qwen æ‰€æœ‰æœ€æ–°å¸–å­
#   ./scrape_x_user.sh -k qwen3                           # æŠ“å– Alibaba_Qwen å« qwen3 çš„å¸–å­
#   ./scrape_x_user.sh -u chenchengpro -k claw -f rss -o feed.xml
#   ./scrape_x_user.sh -u chenchengpro -f json -n 20 -o data.json
# ============================================================

set -euo pipefail

# ---------- é»˜è®¤å‚æ•° ----------
CDP_PORT=9222
MAX_SCROLLS=10
OUTPUT_FILE=""
USERNAME="Alibaba_Qwen"
KEYWORD=""
FORMAT="md"

# ---------- è§£æå‚æ•° ----------
while getopts "u:k:p:n:o:f:h" opt; do
  case $opt in
    u) USERNAME="$OPTARG" ;;
    k) KEYWORD="$OPTARG" ;;
    p) CDP_PORT="$OPTARG" ;;
    n) MAX_SCROLLS="$OPTARG" ;;
    o) OUTPUT_FILE="$OPTARG" ;;
    f) FORMAT="$OPTARG" ;;
    h)
      head -26 "$0" | tail -24
      exit 0
      ;;
    *)
      echo "ç”¨æ³•: $0 [-u <username>] [-k <keyword>] [-p <cdp_port>] [-n <max_scrolls>] [-o <output_file>] [-f md|rss|json]" >&2
      exit 1
      ;;
  esac
done

if [[ -z "$USERNAME" ]]; then
  echo "é”™è¯¯: å¿…é¡»æŒ‡å®š -u <username>" >&2
  exit 1
fi

if [[ "$FORMAT" != "md" && "$FORMAT" != "rss" && "$FORMAT" != "json" ]]; then
  echo "é”™è¯¯: æ ¼å¼å¿…é¡»ä¸º md, rss æˆ– json" >&2
  exit 1
fi

# ---------- å·¥å…·å‡½æ•° ----------
AB="agent-browser --cdp $CDP_PORT"
TMPDIR_SCRAPER=$(mktemp -d)
TWEETS_JSON="$TMPDIR_SCRAPER/tweets.json"

cleanup() {
  rm -rf "$TMPDIR_SCRAPER"
}
trap cleanup EXIT

log() {
  echo "[$(date '+%H:%M:%S')] $*" >&2
}

# ---------- ä¸»æµç¨‹ ----------

# 1. æ„å»ºç›®æ ‡ URL å¹¶å¯¼èˆª
if [[ -n "$KEYWORD" ]]; then
  ENCODED_QUERY=$(python3 -c "import urllib.parse; print(urllib.parse.quote('from:${USERNAME} ${KEYWORD}'))")
  TARGET_URL="https://x.com/search?q=${ENCODED_QUERY}&src=typed_query&f=live"
  log "æ­£åœ¨å¯¼èˆªåˆ°æœç´¢é¡µ: from:${USERNAME} ${KEYWORD}"
else
  TARGET_URL="https://x.com/${USERNAME}"
  log "æ­£åœ¨å¯¼èˆªåˆ°ç”¨æˆ·ä¸»é¡µ: @${USERNAME}"
fi
$AB open "$TARGET_URL" >/dev/null 2>&1
sleep 3

# 2. ç­‰å¾…é¡µé¢åŠ è½½
log "ç­‰å¾…é¡µé¢åŠ è½½..."
$AB wait --load networkidle >/dev/null 2>&1 || true
sleep 2

# 3. æ»šåŠ¨ + æå–å¸–å­å†…å®¹
PREV_COUNT=0
echo "[]" > "$TWEETS_JSON"

for ((i = 1; i <= MAX_SCROLLS; i++)); do
  log "ç¬¬ ${i}/${MAX_SCROLLS} è½®æŠ“å–..."

  # ç”¨ eval æå–å¸–å­ï¼Œè¿‡æ»¤å¹¿å‘Š
  EVAL_TMPFILE="$TMPDIR_SCRAPER/eval_${i}.json"
  $AB eval "
    JSON.stringify(
      Array.from(document.querySelectorAll('article[data-testid=\"tweet\"]'))
        .filter(el => !el.querySelector('[data-testid=\"placementTracking\"]'))
        .filter(el => {
          const nameEl = el.querySelector('[data-testid=\"User-Name\"]');
          return nameEl && nameEl.textContent.includes('$USERNAME');
        })
        .map(el => {
          const tweetText = el.querySelector('[data-testid=\"tweetText\"]');
          const time = el.querySelector('time');
          const nameEl = el.querySelector('[data-testid=\"User-Name\"]');
          const linkEl = el.querySelector('a[href*=\"/status/\"]');
          return JSON.stringify({
            author: nameEl ? nameEl.textContent.trim() : '',
            time: time ? time.getAttribute('datetime') : '',
            text: tweetText ? tweetText.innerText.trim() : '',
            link: linkEl ? 'https://x.com' + linkEl.getAttribute('href') : ''
          });
        })
        .map(s => JSON.parse(s))
    )
  " > "$EVAL_TMPFILE" 2>/dev/null || echo "[]" > "$EVAL_TMPFILE"

  # ç”¨ python åˆå¹¶å»é‡
  python3 - "$TWEETS_JSON" "$EVAL_TMPFILE" << 'PYEOF'
import json, sys

existing_file = sys.argv[1]
new_file = sys.argv[2]

try:
    with open(existing_file, "r") as f:
        existing = json.load(f)
except:
    existing = []

try:
    with open(new_file, "r") as f:
        raw = f.read().strip()
        if raw.startswith('"') and raw.endswith('"'):
            raw = json.loads(raw)
        new_tweets = json.loads(raw) if isinstance(raw, str) else raw
except:
    new_tweets = []

def get_key(item):
    if isinstance(item, dict):
        return item.get("text", "")[:150]
    return str(item)[:150]

seen = set()
for t in existing:
    seen.add(get_key(t))

merged = list(existing)
for t in (new_tweets if isinstance(new_tweets, list) else []):
    key = get_key(t)
    if key not in seen:
        seen.add(key)
        merged.append(t)

with open(existing_file, "w") as f:
    json.dump(merged, f, ensure_ascii=False)

print(len(merged))
PYEOF

  CURRENT_COUNT=$(python3 -c "import json; print(len(json.load(open('$TWEETS_JSON'))))" 2>/dev/null || echo "0")
  log "  å·²æ”¶é›† ${CURRENT_COUNT} æ¡å¸–å­ (æœ¬è½®æ–°å¢ $((CURRENT_COUNT - PREV_COUNT)))"

  if [[ "$CURRENT_COUNT" -eq "$PREV_COUNT" && "$i" -gt 1 ]]; then
    log "æ²¡æœ‰æ›´å¤šæ–°å¸–å­ï¼Œåœæ­¢æ»šåŠ¨"
    break
  fi
  PREV_COUNT=$CURRENT_COUNT

  $AB scroll down 1200 >/dev/null 2>&1
  sleep 2
done

# 4. æ ¼å¼åŒ–è¾“å‡º
log "æ­£åœ¨æ ¼å¼åŒ–è¾“å‡º (æ ¼å¼: $FORMAT)..."

FORMAT_OUTPUT=$(python3 - "$TWEETS_JSON" "$USERNAME" "$KEYWORD" "$FORMAT" << 'PYEOF'
import json, sys, html
from datetime import datetime, timezone
from email.utils import format_datetime

tweets_file = sys.argv[1]
username = sys.argv[2]
keyword = sys.argv[3]
fmt = sys.argv[4]

with open(tweets_file, "r") as f:
    tweets = json.load(f)

def parse_time(t):
    """è§£æ ISO æ—¶é—´å­—ç¬¦ä¸²"""
    if not t:
        return None
    try:
        return datetime.fromisoformat(t.replace("Z", "+00:00"))
    except:
        return None

def get_text(tweet):
    if isinstance(tweet, dict):
        return tweet.get("text", "")
    return str(tweet)

def get_time(tweet):
    if isinstance(tweet, dict):
        return parse_time(tweet.get("time", ""))
    return None

def get_link(tweet):
    if isinstance(tweet, dict):
        return tweet.get("link", "")
    return ""

# ==================== Markdown ====================
if fmt == "md":
    print("# X ç”¨æˆ·å¸–å­æŠ“å–ç»“æœ")
    print()
    print(f"- **ç”¨æˆ·**: @{username}")
    print(f"- **å…³é”®è¯**: {keyword if keyword else '(å…¨éƒ¨å¸–å­)'}")
    print(f"- **æŠ“å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print(f"- **å¸–å­æ•°é‡**: {len(tweets)}")
    print()
    print("---")
    print()
    for i, tweet in enumerate(tweets, 1):
        dt = get_time(tweet)
        text = get_text(tweet)
        link = get_link(tweet)
        time_str = dt.strftime("%Y-%m-%d %H:%M") if dt else ""
        print(f"## å¸–å­ {i}")
        if time_str:
            print(f"> ğŸ• {time_str}")
        if link:
            print(f"> ğŸ”— {link}")
        print()
        if text:
            for line in text.split("\n"):
                line = line.strip()
                if line:
                    print(line)
        print()
        print("---")
        print()

# ==================== RSS ====================
elif fmt == "rss":
    now_rfc822 = format_datetime(datetime.now(timezone.utc))
    feed_link = f"https://x.com/{username}"

    print('<?xml version="1.0" encoding="UTF-8"?>')
    print('<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">')
    print('  <channel>')
    kw_label = html.escape(keyword) if keyword else "å…¨éƒ¨å¸–å­"
    print(f'    <title>@{username} - {kw_label}</title>')
    print(f'    <link>{feed_link}</link>')
    print(f'    <description>X ç”¨æˆ· @{username} çš„{kw_label}</description>')
    print(f'    <language>zh-cn</language>')
    print(f'    <lastBuildDate>{now_rfc822}</lastBuildDate>')
    print(f'    <generator>scrape_x_user.sh</generator>')
    print()

    for tweet in tweets:
        dt = get_time(tweet)
        text = get_text(tweet)
        link = get_link(tweet) or feed_link
        # æ ‡é¢˜å–æ­£æ–‡å‰ 80 å­—ç¬¦
        title = text[:80].replace("\n", " ").strip()
        if len(text) > 80:
            title += "..."

        print('    <item>')
        print(f'      <title>{html.escape(title)}</title>')
        print(f'      <link>{html.escape(link)}</link>')
        print(f'      <guid isPermaLink="true">{html.escape(link)}</guid>')
        if dt:
            print(f'      <pubDate>{format_datetime(dt)}</pubDate>')
        # description ç”¨ CDATA åŒ…è£¹ä¿ç•™åŸå§‹æ ¼å¼
        desc_lines = []
        for line in text.split("\n"):
            line = line.strip()
            if line:
                desc_lines.append(f"<p>{html.escape(line)}</p>")
        print(f'      <description><![CDATA[{"".join(desc_lines)}]]></description>')
        print(f'      <author>@{username}</author>')
        print('    </item>')

    print('  </channel>')
    print('</rss>')

# ==================== JSON ====================
elif fmt == "json":
    output = {
        "meta": {
            "username": username,
            "keyword": keyword,
            "scraped_at": datetime.now().isoformat(),
            "count": len(tweets)
        },
        "tweets": tweets
    }
    print(json.dumps(output, ensure_ascii=False, indent=2))
PYEOF
)

# 5. è¾“å‡ºç»“æœ
if [[ -n "$OUTPUT_FILE" ]]; then
  echo "$FORMAT_OUTPUT" > "$OUTPUT_FILE"
  log "ç»“æœå·²å†™å…¥: $OUTPUT_FILE"
else
  echo "$FORMAT_OUTPUT"
fi

log "æŠ“å–å®Œæˆ!"
