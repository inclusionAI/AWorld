#!/usr/bin/env bash
# ============================================================
# scrape_x_home.sh â€” é€šè¿‡ agent-browser (CDP) æŠ“å– X é¦–é¡µæ¨èæµ
#
# ç”¨æ³•:
#   ./scrape_x_home.sh [-t <tab>] [-p <cdp_port>] [-n <max_scrolls>] [-o <output_file>] [-f <format>]
#
# å‚æ•°:
#   -t  æ¨è Tab: foryou (é»˜è®¤) | following
#   -p  CDP ç«¯å£å·ï¼Œé»˜è®¤ 9222
#   -n  æœ€å¤§æ»šåŠ¨æ¬¡æ•°ï¼Œé»˜è®¤ 5
#   -o  è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ stdout
#   -f  è¾“å‡ºæ ¼å¼: md (Markdown, é»˜è®¤) | rss (RSS XML) | json (åŸå§‹ JSON)
#
# ä¾èµ–:
#   - agent-browser (å·²é€šè¿‡ CDP è¿æ¥åˆ°è¿è¡Œä¸­çš„æµè§ˆå™¨ï¼Œä¸”å·²ç™»å½• X)
#   - python3
#
# ç¤ºä¾‹:
#   ./scrape_x_home.sh                           # æŠ“å– For you æ¨èæµ
#   ./scrape_x_home.sh -t following -n 10        # æŠ“å– Following æ—¶é—´çº¿
#   ./scrape_x_home.sh -f json -o feed.json      # JSON è¾“å‡ºåˆ°æ–‡ä»¶
#   ./scrape_x_home.sh -n 3 -f rss -o home.xml   # å°‘é‡æŠ“å–ï¼ŒRSS è¾“å‡º
# ============================================================

set -euo pipefail

# ---------- é»˜è®¤å‚æ•° ----------
CDP_PORT=9222
MAX_SCROLLS=5
OUTPUT_FILE=""
TAB="foryou"
FORMAT="md"

# ---------- è§£æå‚æ•° ----------
while getopts "t:p:n:o:f:h" opt; do
  case $opt in
    t) TAB="$OPTARG" ;;
    p) CDP_PORT="$OPTARG" ;;
    n) MAX_SCROLLS="$OPTARG" ;;
    o) OUTPUT_FILE="$OPTARG" ;;
    f) FORMAT="$OPTARG" ;;
    h)
      head -26 "$0" | tail -24
      exit 0
      ;;
    *)
      echo "ç”¨æ³•: $0 [-t foryou|following] [-p <cdp_port>] [-n <max_scrolls>] [-o <output_file>] [-f md|rss|json]" >&2
      exit 1
      ;;
  esac
done

if [[ "$TAB" != "foryou" && "$TAB" != "following" ]]; then
  echo "é”™è¯¯: -t å¿…é¡»ä¸º foryou æˆ– following" >&2
  exit 1
fi

if [[ "$FORMAT" != "md" && "$FORMAT" != "rss" && "$FORMAT" != "json" ]]; then
  echo "é”™è¯¯: æ ¼å¼å¿…é¡»ä¸º md, rss æˆ– json" >&2
  exit 1
fi

# ---------- å·¥å…·å‡½æ•° ----------
AB="agent-browser --cdp $CDP_PORT"
TMPDIR_SCRAPER=$(mktemp -d)
TWEETS_JSON="$TMPDIR_SCRAPER/tweets.json"

cleanup() {
  rm -rf "$TMPDIR_SCRAPER"
}
trap cleanup EXIT

log() {
  echo "[$(date '+%H:%M:%S')] $*" >&2
}

# ---------- ä¸»æµç¨‹ ----------

# 1. å¯¼èˆªåˆ° X é¦–é¡µ
log "æ­£åœ¨å¯¼èˆªåˆ° X é¦–é¡µ..."
$AB open "https://x.com/home" >/dev/null 2>&1
sleep 3

# 2. ç­‰å¾…é¡µé¢åŠ è½½
log "ç­‰å¾…é¡µé¢åŠ è½½..."
$AB wait --load networkidle >/dev/null 2>&1 || true
sleep 2

# 3. åˆ‡æ¢åˆ°ç›®æ ‡ Tab
if [[ "$TAB" == "foryou" ]]; then
  TAB_LABEL="For you"
else
  TAB_LABEL="Following"
fi

log "åˆ‡æ¢åˆ° Tab: ${TAB_LABEL}..."
$AB eval "
  (() => {
    const tabs = document.querySelectorAll('[role=\"tab\"]');
    for (const tab of tabs) {
      if (tab.textContent.trim() === '$TAB_LABEL') {
        const active = tab.getAttribute('aria-selected');
        if (active !== 'true') tab.click();
        return 'switched to $TAB_LABEL';
      }
    }
    return 'tab not found, staying on current';
  })()
" >/dev/null 2>&1
sleep 2

# 4. æ»šåŠ¨ + æå–å¸–å­å†…å®¹
PREV_COUNT=0
echo "[]" > "$TWEETS_JSON"

for ((i = 1; i <= MAX_SCROLLS; i++)); do
  log "ç¬¬ ${i}/${MAX_SCROLLS} è½®æŠ“å–..."

  # æå–å¸–å­ï¼Œè¿‡æ»¤å¹¿å‘Šï¼ˆplacementTrackingï¼‰
  EVAL_TMPFILE="$TMPDIR_SCRAPER/eval_${i}.json"
  $AB eval "
    JSON.stringify(
      Array.from(document.querySelectorAll('article[data-testid=\"tweet\"]'))
        .filter(el => !el.querySelector('[data-testid=\"placementTracking\"]'))
        .map(el => {
          const tweetText = el.querySelector('[data-testid=\"tweetText\"]');
          const time = el.querySelector('time');
          const nameEl = el.querySelector('[data-testid=\"User-Name\"]');
          const linkEl = el.querySelector('a[href*=\"/status/\"]');
          const imgEls = el.querySelectorAll('img[src*=\"pbs.twimg.com/media\"]');
          const retweetEl = el.querySelector('[data-testid=\"socialContext\"]');
          return {
            author: nameEl ? nameEl.textContent.trim().replace(/\\n/g, ' ') : '',
            time: time ? time.getAttribute('datetime') : '',
            text: tweetText ? tweetText.innerText.trim() : '',
            link: linkEl ? 'https://x.com' + linkEl.getAttribute('href') : '',
            hasMedia: imgEls.length > 0,
            retweet: retweetEl ? retweetEl.textContent.trim() : ''
          };
        })
        .filter(t => t.text)
    )
  " > "$EVAL_TMPFILE" 2>/dev/null || echo "[]" > "$EVAL_TMPFILE"

  # åˆå¹¶å»é‡
  python3 - "$TWEETS_JSON" "$EVAL_TMPFILE" << 'PYEOF'
import json, sys

existing_file = sys.argv[1]
new_file = sys.argv[2]

try:
    with open(existing_file, "r") as f:
        existing = json.load(f)
except:
    existing = []

try:
    with open(new_file, "r") as f:
        raw = f.read().strip()
        if raw.startswith('"') and raw.endswith('"'):
            raw = json.loads(raw)
        new_tweets = json.loads(raw) if isinstance(raw, str) else raw
except:
    new_tweets = []

def get_key(item):
    if isinstance(item, dict):
        return item.get("text", "")[:150]
    return str(item)[:150]

seen = set()
for t in existing:
    seen.add(get_key(t))

merged = list(existing)
for t in (new_tweets if isinstance(new_tweets, list) else []):
    key = get_key(t)
    if key and key not in seen:
        seen.add(key)
        merged.append(t)

with open(existing_file, "w") as f:
    json.dump(merged, f, ensure_ascii=False)

print(len(merged))
PYEOF

  CURRENT_COUNT=$(python3 -c "import json; print(len(json.load(open('$TWEETS_JSON'))))" 2>/dev/null || echo "0")
  log "  å·²æ”¶é›† ${CURRENT_COUNT} æ¡å¸–å­ (æœ¬è½®æ–°å¢ $((CURRENT_COUNT - PREV_COUNT)))"

  if [[ "$CURRENT_COUNT" -eq "$PREV_COUNT" && "$i" -gt 1 ]]; then
    log "æ²¡æœ‰æ›´å¤šæ–°å¸–å­ï¼Œåœæ­¢æ»šåŠ¨"
    break
  fi
  PREV_COUNT=$CURRENT_COUNT

  $AB scroll down 1500 >/dev/null 2>&1
  sleep 3
done

# 5. æ ¼å¼åŒ–è¾“å‡º
log "æ­£åœ¨æ ¼å¼åŒ–è¾“å‡º (æ ¼å¼: $FORMAT)..."

FORMAT_OUTPUT=$(python3 - "$TWEETS_JSON" "$TAB_LABEL" "$FORMAT" << 'PYEOF'
import json, sys, html, re
from datetime import datetime, timezone
from email.utils import format_datetime

tweets_file = sys.argv[1]
tab_label = sys.argv[2]
fmt = sys.argv[3]

with open(tweets_file, "r") as f:
    tweets = json.load(f)

def parse_time(t):
    if not t:
        return None
    try:
        return datetime.fromisoformat(t.replace("Z", "+00:00"))
    except:
        return None

def parse_author(raw):
    """è§£æ author å­—æ®µï¼Œæå– name å’Œ handle"""
    raw = raw.strip()
    # æ ¼å¼é€šå¸¸ä¸º: "Name@handle Â· 17h" æˆ– "Name @handle Â· 17h"
    m = re.match(r'^(.+?)@(\w+)', raw)
    if m:
        name = m.group(1).strip()
        handle = "@" + m.group(2)
        return name, handle
    return raw, ""

def get_val(tweet, key, default=""):
    return tweet.get(key, default) if isinstance(tweet, dict) else default

# ==================== Markdown ====================
if fmt == "md":
    print(f"# X æ¨èæµ - {tab_label}")
    print()
    print(f"- **æŠ“å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print(f"- **å¸–å­æ•°é‡**: {len(tweets)}")
    print()
    print("---")
    print()
    for i, tweet in enumerate(tweets, 1):
        text = get_val(tweet, "text")
        time_raw = get_val(tweet, "time")
        link = get_val(tweet, "link")
        author_raw = get_val(tweet, "author")
        retweet = get_val(tweet, "retweet")
        has_media = get_val(tweet, "hasMedia", False)

        name, handle = parse_author(author_raw)
        dt = parse_time(time_raw)
        time_str = dt.strftime("%Y-%m-%d %H:%M") if dt else ""

        print(f"## {i}. {name} {handle}")
        meta = []
        if time_str:
            meta.append(f"ğŸ• {time_str}")
        if has_media:
            meta.append("ğŸ–¼ï¸ å«å›¾ç‰‡/è§†é¢‘")
        if retweet:
            meta.append(f"ğŸ” {retweet}")
        if meta:
            print(f"> {' | '.join(meta)}")
        if link:
            print(f"> ğŸ”— {link}")
        print()
        if text:
            for line in text.split("\n"):
                line = line.strip()
                if line:
                    print(line)
        print()
        print("---")
        print()

# ==================== RSS ====================
elif fmt == "rss":
    now_rfc822 = format_datetime(datetime.now(timezone.utc))
    feed_link = "https://x.com/home"

    print('<?xml version="1.0" encoding="UTF-8"?>')
    print('<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">')
    print('  <channel>')
    print(f'    <title>X æ¨èæµ - {html.escape(tab_label)}</title>')
    print(f'    <link>{feed_link}</link>')
    print(f'    <description>X {html.escape(tab_label)} æ¨èå†…å®¹</description>')
    print(f'    <language>zh-cn</language>')
    print(f'    <lastBuildDate>{now_rfc822}</lastBuildDate>')
    print(f'    <generator>scrape_x_home.sh</generator>')
    print()

    for tweet in tweets:
        text = get_val(tweet, "text")
        link = get_val(tweet, "link") or feed_link
        author_raw = get_val(tweet, "author")
        time_raw = get_val(tweet, "time")
        dt = parse_time(time_raw)
        name, handle = parse_author(author_raw)

        title = f"[{name}] " + text[:70].replace("\n", " ").strip()
        if len(text) > 70:
            title += "..."

        desc_lines = []
        for line in text.split("\n"):
            line = line.strip()
            if line:
                desc_lines.append(f"<p>{html.escape(line)}</p>")

        print('    <item>')
        print(f'      <title>{html.escape(title)}</title>')
        print(f'      <link>{html.escape(link)}</link>')
        print(f'      <guid isPermaLink="true">{html.escape(link)}</guid>')
        if dt:
            print(f'      <pubDate>{format_datetime(dt)}</pubDate>')
        print(f'      <description><![CDATA[{"".join(desc_lines)}]]></description>')
        print(f'      <author>{html.escape(name)} {html.escape(handle)}</author>')
        print('    </item>')

    print('  </channel>')
    print('</rss>')

# ==================== JSON ====================
elif fmt == "json":
    # å¯¹ author åšç»“æ„åŒ–å¤„ç†
    processed = []
    for tweet in tweets:
        t = dict(tweet)
        name, handle = parse_author(t.get("author", ""))
        t["author_name"] = name
        t["author_handle"] = handle
        processed.append(t)

    output = {
        "meta": {
            "platform": "x",
            "tab": tab_label,
            "scraped_at": datetime.now().isoformat(),
            "count": len(processed)
        },
        "tweets": processed
    }
    print(json.dumps(output, ensure_ascii=False, indent=2))
PYEOF
)

# 6. è¾“å‡ºç»“æœ
if [[ -n "$OUTPUT_FILE" ]]; then
  echo "$FORMAT_OUTPUT" > "$OUTPUT_FILE"
  log "ç»“æœå·²å†™å…¥: $OUTPUT_FILE"
else
  echo "$FORMAT_OUTPUT"
fi

log "æŠ“å–å®Œæˆ!"
