#!/usr/bin/env bash
# ============================================================
# scrape_xhs.sh â€” é€šè¿‡ agent-browser (CDP) æŠ“å–å°çº¢ä¹¦æœç´¢ç»“æœ
#
# ç”¨æ³•:
#   ./scrape_xhs.sh -k <keyword> [-p <cdp_port>] [-n <max_scrolls>] [-d <detail_count>] [-o <output_file>] [-f <format>]
#
# å‚æ•°:
#   -k  æœç´¢å…³é”®è¯ï¼Œå¿…å¡«
#   -p  CDP ç«¯å£å·ï¼Œé»˜è®¤ 9222
#   -n  æœ€å¤§æ»šåŠ¨æ¬¡æ•°ï¼ˆåˆ—è¡¨é¡µï¼‰ï¼Œé»˜è®¤ 5
#   -d  è¿›å…¥è¯¦æƒ…é¡µè·å–æ­£æ–‡çš„å¸–å­æ•°é‡ï¼Œé»˜è®¤ 10ï¼ˆ0 = ä»…æŠ“åˆ—è¡¨ï¼‰
#   -o  è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ stdout
#   -f  è¾“å‡ºæ ¼å¼: md (Markdown, é»˜è®¤) | rss (RSS XML) | json (åŸå§‹ JSON)
#
# ä¾èµ–:
#   - agent-browser (å·²é€šè¿‡ CDP è¿æ¥åˆ°è¿è¡Œä¸­çš„æµè§ˆå™¨)
#   - python3
#
# ç¤ºä¾‹:
#   ./scrape_xhs.sh -k "Agentå¼€å‘å·¥ç¨‹å¸ˆ"
#   ./scrape_xhs.sh -k "AI Agentå²—ä½" -d 5 -f rss -o feed.xml
#   ./scrape_xhs.sh -k "å¤§æ¨¡å‹é¢ç»" -n 10 -d 20 -f json -o data.json
# ============================================================

set -euo pipefail

# ---------- é»˜è®¤å‚æ•° ----------
CDP_PORT=9222
MAX_SCROLLS=5
DETAIL_COUNT=10
OUTPUT_FILE=""
KEYWORD=""
FORMAT="md"

# ---------- è§£æå‚æ•° ----------
while getopts "k:p:n:d:o:f:h" opt; do
  case $opt in
    k) KEYWORD="$OPTARG" ;;
    p) CDP_PORT="$OPTARG" ;;
    n) MAX_SCROLLS="$OPTARG" ;;
    d) DETAIL_COUNT="$OPTARG" ;;
    o) OUTPUT_FILE="$OPTARG" ;;
    f) FORMAT="$OPTARG" ;;
    h)
      head -27 "$0" | tail -25
      exit 0
      ;;
    *)
      echo "ç”¨æ³•: $0 -k <keyword> [-p <cdp_port>] [-n <max_scrolls>] [-d <detail_count>] [-o <output_file>] [-f md|rss|json]" >&2
      exit 1
      ;;
  esac
done

if [[ -z "$KEYWORD" ]]; then
  echo "é”™è¯¯: å¿…é¡»æŒ‡å®š -k <keyword>" >&2
  exit 1
fi

if [[ "$FORMAT" != "md" && "$FORMAT" != "rss" && "$FORMAT" != "json" ]]; then
  echo "é”™è¯¯: æ ¼å¼å¿…é¡»ä¸º md, rss æˆ– json" >&2
  exit 1
fi

# ---------- å·¥å…·å‡½æ•° ----------
AB="agent-browser --cdp $CDP_PORT"
TMPDIR_SCRAPER=$(mktemp -d)
POSTS_JSON="$TMPDIR_SCRAPER/posts.json"

cleanup() {
  rm -rf "$TMPDIR_SCRAPER"
}
trap cleanup EXIT

log() {
  echo "[$(date '+%H:%M:%S')] $*" >&2
}

# ---------- ä¸»æµç¨‹ ----------

# 1. æ„å»ºæœç´¢ URL å¹¶å¯¼èˆª
ENCODED_KW=$(python3 -c "import urllib.parse; print(urllib.parse.quote('${KEYWORD}'))")
TARGET_URL="https://www.xiaohongshu.com/search_result?keyword=${ENCODED_KW}&source=web_search_result_notes"

log "æ­£åœ¨æœç´¢å°çº¢ä¹¦: ${KEYWORD}"
$AB open "$TARGET_URL" >/dev/null 2>&1
sleep 3

# 2. ç­‰å¾…é¡µé¢åŠ è½½
log "ç­‰å¾…é¡µé¢åŠ è½½..."
$AB wait --load networkidle >/dev/null 2>&1 || true
sleep 2

# 3. æ»šåŠ¨åˆ—è¡¨é¡µ + æå–å¸–å­å¡ç‰‡ä¿¡æ¯
PREV_COUNT=0
echo "[]" > "$POSTS_JSON"

for ((i = 1; i <= MAX_SCROLLS; i++)); do
  log "ç¬¬ ${i}/${MAX_SCROLLS} è½®æŠ“å–åˆ—è¡¨..."

  EVAL_TMPFILE="$TMPDIR_SCRAPER/eval_${i}.json"
  $AB eval "
    JSON.stringify(
      Array.from(document.querySelectorAll('section.note-item, div.note-item')).map(el => {
        const titleEl = el.querySelector('.title span') || el.querySelector('.title');
        const authorEl = el.querySelector('.author-wrapper .name, .name');
        const likesEl = el.querySelector('.like-wrapper .count, .count');
        const linkEl = el.querySelector('a[href*=\"/search_result/\"], a[href*=\"/explore/\"], a');
        const imgEl = el.querySelector('img');
        let href = '';
        if (linkEl) {
          href = linkEl.getAttribute('href') || '';
          if (href.startsWith('/')) href = 'https://www.xiaohongshu.com' + href;
        }
        return {
          title: titleEl ? titleEl.textContent.trim() : '',
          author: authorEl ? authorEl.textContent.trim() : '',
          likes: likesEl ? likesEl.textContent.trim() : '',
          link: href,
          cover: imgEl ? imgEl.getAttribute('src') || '' : ''
        };
      }).filter(p => p.title)
    )
  " > "$EVAL_TMPFILE" 2>/dev/null || echo "[]" > "$EVAL_TMPFILE"

  # åˆå¹¶å»é‡
  python3 - "$POSTS_JSON" "$EVAL_TMPFILE" << 'PYEOF'
import json, sys

existing_file = sys.argv[1]
new_file = sys.argv[2]

try:
    with open(existing_file, "r") as f:
        existing = json.load(f)
except:
    existing = []

try:
    with open(new_file, "r") as f:
        raw = f.read().strip()
        if raw.startswith('"') and raw.endswith('"'):
            raw = json.loads(raw)
        new_posts = json.loads(raw) if isinstance(raw, str) else raw
except:
    new_posts = []

def get_key(item):
    if isinstance(item, dict):
        return item.get("title", "")[:100]
    return str(item)[:100]

seen = set()
for t in existing:
    seen.add(get_key(t))

merged = list(existing)
for t in (new_posts if isinstance(new_posts, list) else []):
    key = get_key(t)
    if key and key not in seen:
        seen.add(key)
        merged.append(t)

with open(existing_file, "w") as f:
    json.dump(merged, f, ensure_ascii=False)

print(len(merged))
PYEOF

  CURRENT_COUNT=$(python3 -c "import json; print(len(json.load(open('$POSTS_JSON'))))" 2>/dev/null || echo "0")
  log "  å·²æ”¶é›† ${CURRENT_COUNT} æ¡å¸–å­ (æœ¬è½®æ–°å¢ $((CURRENT_COUNT - PREV_COUNT)))"

  if [[ "$CURRENT_COUNT" -eq "$PREV_COUNT" && "$i" -gt 1 ]]; then
    log "æ²¡æœ‰æ›´å¤šæ–°å¸–å­ï¼Œåœæ­¢æ»šåŠ¨"
    break
  fi
  PREV_COUNT=$CURRENT_COUNT

  $AB scroll down 1200 >/dev/null 2>&1
  sleep 2
done

log "åˆ—è¡¨æŠ“å–å®Œæˆï¼Œå…± ${CURRENT_COUNT} æ¡"

# 4. è¿›å…¥è¯¦æƒ…é¡µè·å–æ­£æ–‡ï¼ˆæŒ‰ likes æ’åºå– top Nï¼‰
if [[ "$DETAIL_COUNT" -gt 0 ]]; then
  log "å¼€å§‹è·å–å‰ ${DETAIL_COUNT} æ¡å¸–å­çš„è¯¦æƒ…æ­£æ–‡..."

  python3 - "$POSTS_JSON" "$DETAIL_COUNT" << 'PYEOF'
import json, sys

posts_file = sys.argv[1]
detail_count = int(sys.argv[2])

with open(posts_file, "r") as f:
    posts = json.load(f)

# æŒ‰ likes æ’åºï¼ˆæ•°å­—è¶Šå¤§è¶Šå‰ï¼‰
def parse_likes(s):
    s = str(s).strip()
    if not s:
        return 0
    # å¤„ç† "1.2ä¸‡" è¿™ç§æ ¼å¼
    if 'ä¸‡' in s:
        return int(float(s.replace('ä¸‡', '')) * 10000)
    try:
        return int(s)
    except:
        return 0

posts.sort(key=lambda p: parse_likes(p.get("likes", "0")), reverse=True)

# è¾“å‡ºéœ€è¦è·å–è¯¦æƒ…çš„å¸–å­ç´¢å¼•
indices = []
for i, p in enumerate(posts[:detail_count]):
    indices.append(i)

# é‡å†™æ’åºåçš„æ•°æ®
with open(posts_file, "w") as f:
    json.dump(posts, f, ensure_ascii=False)

# è¾“å‡ºéœ€è¦è¯¦æƒ…çš„æ•°é‡
print(min(detail_count, len(posts)))
PYEOF

  DETAIL_ACTUAL=$(python3 -c "import json; posts=json.load(open('$POSTS_JSON')); print(min($DETAIL_COUNT, len(posts)))")

  for ((j = 0; j < DETAIL_ACTUAL; j++)); do
    TITLE=$(python3 -c "
import json
posts = json.load(open('$POSTS_JSON'))
print(posts[$j].get('title', ''))
")

    if [[ -z "$TITLE" ]]; then
      continue
    fi

    log "  è·å–è¯¦æƒ… [$((j+1))/${DETAIL_ACTUAL}]: ${TITLE:0:40}..."

    # å°†æ ‡é¢˜å†™å…¥ä¸´æ—¶æ–‡ä»¶ï¼Œé¿å… bash å¼•å·è½¬ä¹‰é—®é¢˜
    TITLE_FILE="$TMPDIR_SCRAPER/title_${j}.txt"
    echo "$TITLE" > "$TITLE_FILE"
    TITLE_JSON=$(python3 -c "import json; print(json.dumps(open('$TITLE_FILE').read().strip()))")

    # å…ˆæ»šå›é¡¶éƒ¨
    $AB scroll up 50000 >/dev/null 2>&1
    sleep 1

    # åœ¨åˆ—è¡¨ä¸­æŸ¥æ‰¾å¹¶ç‚¹å‡» a.coverï¼ˆè§¦å‘å¼¹çª—ï¼Œä¸è¦ç”¨ a[href^="/explore"] ä¼šç›´è·³ 404ï¼‰
    CLICK_RESULT="not_found"
    for ((s = 0; s < 8; s++)); do
      CLICK_RESULT=$($AB eval "
        (() => {
          const target = $TITLE_JSON;
          const items = document.querySelectorAll('section.note-item, div.note-item');
          for (const item of items) {
            const titleEl = item.querySelector('.title span') || item.querySelector('.title');
            if (titleEl && titleEl.textContent.trim().includes(target.substring(0, 15))) {
              const cover = item.querySelector('a.cover') || item.querySelector('a[href*=\"search_result\"]');
              if (cover) { cover.click(); return 'clicked'; }
            }
          }
          return 'not_found';
        })()
      " 2>/dev/null || echo "not_found")

      if [[ "$CLICK_RESULT" == *"clicked"* ]]; then
        break
      fi
      $AB scroll down 600 >/dev/null 2>&1
      sleep 1
    done

    if [[ "$CLICK_RESULT" == *"not_found"* ]]; then
      log "    è·³è¿‡ï¼ˆæœªåœ¨é¡µé¢ä¸­æ‰¾åˆ°ï¼‰"
      continue
    fi

    # ç­‰å¾…å¼¹çª—åŠ è½½
    sleep 4

    # æå–æ­£æ–‡ï¼ˆå¤šé€‰æ‹©å™¨ fallbackï¼‰+ æ—¥æœŸ
    DETAIL_TMPFILE="$TMPDIR_SCRAPER/detail_${j}.json"
    $AB eval "
      (() => {
        const noteText = document.querySelector('#detail-desc')
          || document.querySelector('.note-content .desc .note-text')
          || document.querySelector('span.note-text');
        const title = document.querySelector('.note-content .title');
        const dateEl = document.querySelector('.note-content .bottom-container');
        // å¦‚æœ note-text ä¸ºç©ºï¼ˆå›¾æ–‡å¸–ï¼‰ï¼Œå°è¯•ä» scroller è·å–
        let text = noteText ? noteText.innerText.trim() : '';
        if (!text) {
          const scroller = document.querySelector('.note-scroller .content, .note-scroller');
          if (scroller) {
            // ä» scroller æå–ï¼Œå»æ‰è¯„è®ºåŒº
            const raw = scroller.innerText;
            const endIdx = raw.indexOf('æ¡è¯„è®º');
            text = endIdx > 0 ? raw.substring(raw.indexOf('\\n', endIdx) + 1).trim() : '';
          }
        }
        return JSON.stringify({
          text: text,
          title: title ? title.innerText.trim() : '',
          date: dateEl ? dateEl.innerText.trim() : ''
        });
      })()
    " > "$DETAIL_TMPFILE" 2>/dev/null || echo '{}' > "$DETAIL_TMPFILE"

    # å†™å› JSON
    python3 - "$POSTS_JSON" "$j" "$DETAIL_TMPFILE" << 'PYEOF'
import json, sys

posts_file = sys.argv[1]
idx = int(sys.argv[2])
detail_file = sys.argv[3]

with open(posts_file, "r") as f:
    posts = json.load(f)

try:
    with open(detail_file, "r") as f:
        raw = f.read().strip()
        if raw.startswith('"') and raw.endswith('"'):
            raw = json.loads(raw)
        detail = json.loads(raw) if isinstance(raw, str) else raw
except:
    detail = {}

text = detail.get("text", "") if isinstance(detail, dict) else ""
date = detail.get("date", "") if isinstance(detail, dict) else ""

if idx < len(posts):
    if text:
        posts[idx]["detail"] = text
    if date:
        posts[idx]["date"] = date

with open(posts_file, "w") as f:
    json.dump(posts, f, ensure_ascii=False)

print(f"OK len={len(text)}")
PYEOF

    DETAIL_LEN=$(python3 -c "import json; p=json.load(open('$POSTS_JSON'))[$j]; print(len(p.get('detail','')))")
    if [[ "$DETAIL_LEN" -gt 0 ]]; then
      log "    å·²è·å–æ­£æ–‡ (${DETAIL_LEN} å­—ç¬¦)"
    else
      log "    æœªè·å–åˆ°æ­£æ–‡ï¼ˆå¯èƒ½æ˜¯å›¾æ–‡å¸–ï¼‰"
    fi

    # å…³é—­å¼¹çª—
    $AB press Escape >/dev/null 2>&1
    sleep 1

    # ç¡®è®¤å›åˆ°æœç´¢é¡µï¼ˆé˜²æ­¢æ„å¤–è·³è½¬ï¼‰
    CURRENT_URL=$($AB get url 2>/dev/null || echo "")
    if [[ "$CURRENT_URL" != *"search_result"* ]]; then
      log "    æ£€æµ‹åˆ°é¡µé¢è·³è½¬ï¼Œè¿”å›æœç´¢é¡µ..."
      $AB back >/dev/null 2>&1
      sleep 2
    fi
  done
fi

# 5. æ ¼å¼åŒ–è¾“å‡º
log "æ­£åœ¨æ ¼å¼åŒ–è¾“å‡º (æ ¼å¼: $FORMAT)..."

FORMAT_OUTPUT=$(python3 - "$POSTS_JSON" "$KEYWORD" "$FORMAT" << 'PYEOF'
import json, sys, html
from datetime import datetime, timezone
from email.utils import format_datetime

posts_file = sys.argv[1]
keyword = sys.argv[2]
fmt = sys.argv[3]

with open(posts_file, "r") as f:
    posts = json.load(f)

def get_val(post, key, default=""):
    return post.get(key, default) if isinstance(post, dict) else default

# ==================== Markdown ====================
if fmt == "md":
    print("# å°çº¢ä¹¦æœç´¢ç»“æœ")
    print()
    print(f"- **å…³é”®è¯**: {keyword}")
    print(f"- **æŠ“å–æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M')}")
    print(f"- **å¸–å­æ•°é‡**: {len(posts)}")
    print()
    print("---")
    print()
    for i, post in enumerate(posts, 1):
        title = get_val(post, "title")
        author = get_val(post, "author")
        likes = get_val(post, "likes")
        link = get_val(post, "link")
        detail = get_val(post, "detail")
        date = get_val(post, "date")

        print(f"## {i}. {title}")
        meta_parts = []
        if author:
            meta_parts.append(f"ğŸ‘¤ {author}")
        if likes:
            meta_parts.append(f"â¤ï¸ {likes}")
        if date:
            meta_parts.append(f"ğŸ“… {date}")
        if meta_parts:
            print(f"> {' | '.join(meta_parts)}")
        if link:
            print(f"> ğŸ”— {link}")
        print()
        if detail:
            for line in detail.split("\n"):
                line = line.strip()
                if line:
                    print(line)
            print()
        print("---")
        print()

# ==================== RSS ====================
elif fmt == "rss":
    now_rfc822 = format_datetime(datetime.now(timezone.utc))
    feed_link = "https://www.xiaohongshu.com"

    print('<?xml version="1.0" encoding="UTF-8"?>')
    print('<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">')
    print('  <channel>')
    print(f'    <title>å°çº¢ä¹¦ - {html.escape(keyword)}</title>')
    print(f'    <link>{feed_link}</link>')
    print(f'    <description>å°çº¢ä¹¦æœç´¢ã€Œ{html.escape(keyword)}ã€çš„ç»“æœ</description>')
    print(f'    <language>zh-cn</language>')
    print(f'    <lastBuildDate>{now_rfc822}</lastBuildDate>')
    print(f'    <generator>scrape_xhs.sh</generator>')
    print()

    for post in posts:
        title = get_val(post, "title")
        author = get_val(post, "author")
        link = get_val(post, "link") or feed_link
        detail = get_val(post, "detail")
        likes = get_val(post, "likes")

        desc_text = detail if detail else title
        desc_lines = []
        for line in desc_text.split("\n"):
            line = line.strip()
            if line:
                desc_lines.append(f"<p>{html.escape(line)}</p>")
        if likes:
            desc_lines.append(f"<p>â¤ï¸ {html.escape(str(likes))} èµ</p>")

        print('    <item>')
        print(f'      <title>{html.escape(title)}</title>')
        print(f'      <link>{html.escape(link)}</link>')
        print(f'      <guid isPermaLink="false">{html.escape(title[:80])}</guid>')
        print(f'      <description><![CDATA[{"".join(desc_lines)}]]></description>')
        if author:
            print(f'      <author>{html.escape(author)}</author>')
        print('    </item>')

    print('  </channel>')
    print('</rss>')

# ==================== JSON ====================
elif fmt == "json":
    output = {
        "meta": {
            "platform": "xiaohongshu",
            "keyword": keyword,
            "scraped_at": datetime.now().isoformat(),
            "count": len(posts)
        },
        "posts": posts
    }
    print(json.dumps(output, ensure_ascii=False, indent=2))
PYEOF
)

# 6. è¾“å‡ºç»“æœ
if [[ -n "$OUTPUT_FILE" ]]; then
  echo "$FORMAT_OUTPUT" > "$OUTPUT_FILE"
  log "ç»“æœå·²å†™å…¥: $OUTPUT_FILE"
else
  echo "$FORMAT_OUTPUT"
fi

log "æŠ“å–å®Œæˆ!"
